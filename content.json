{"meta":{"title":"Gre的博客","subtitle":null,"description":null,"author":"Zhanggdong","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2018-06-19T09:56:34.000Z","updated":"2018-06-20T00:10:20.888Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"html 本文链接：&lt;%= post.title %&gt; 作者：Zhanggdong 出处：https://Zhanggdong.github.io/本文基于 知识共享署名-相同方式共享 4.0 国际许可协议发布，欢迎转载，演绎或用于商业目的，但是必须保留本文的署名张贵东及链接。本站总访问量 次, 访客数 人次, 本文总阅读量 次"},{"title":"文章分类","date":"2018-06-20T00:17:21.000Z","updated":"2018-06-20T00:19:59.907Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"elasticsearch源码分析(三)Discover模块","slug":"elasticsearch源码分析(三)--Discover模块","date":"2018-05-27T04:12:57.000Z","updated":"2018-06-26T02:47:56.044Z","comments":true,"path":"2018/05/27/elasticsearch源码分析(三)--Discover模块/","link":"","permalink":"http://yoursite.com/2018/05/27/elasticsearch源码分析(三)--Discover模块/","excerpt":"","text":"通过上一篇对Elasticsearch启动的分析，我们知道了ES启动的大致流程，还遗留下几个问题 master选举是在什么模块进行的 ES集群是如何进行Master选举的？ ES是如何维护这些节点的？ Discovery模块是如何运作的？ 要想进行Master选举，必然要有一套算法机制，以及节点之前的通信连接、判断节点存活状态等。 通过查阅官网资料，我们知道这些功能是在Elasticsearch的发现协议Discovery里面进行的，在官网上，Elasticsearch的Discovery Module有下面几种实现： Azure Classic Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-azure-classic.html EC2 Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-ec2.html#modules-discovery-ec2 Google Compute Engine Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-gce.html Zen Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-zen.html ​ 一、Zen Discovery模块介绍这里基本上是官网的翻译，建议还是查看官网文档，翻译不准。。。 Zen Discovery是内置在elasticsearch的默认发现模块。它提供单播发现，但可扩展到支持云环境和其他形式的发现。 禅发现集成了其它模块，例如，节点之间的所有通信是使用transport模块。 它被分离成多个子模块，其解释如下： 1.1 Ping这是一个节点使用发现机制来查找其他节点的过程。 1.2 Unicast单播发现需要一个主机列表，用于将作为GossipRouter。这些宿主可被指定为主机名或IP地址;指定主机名的主机每一轮Ping过程中解析为IP地址。请注意，如果您处于DNS解析度随时间变化的环境中，则可能需要调整JVM安全设置。 建议将单播主机列表维护为集群中符合主节点的节点列表。 单播发现提供以下设置和discovery.zen.ping.unicast前缀： 设置 描述 hosts 数组设置或逗号分隔的设置。每个值的形式应该是host:port或host（如果没有设置，port默认设置会transport.profiles.default.port 回落到transport.tcp.port）。请注意，IPv6主机必须放在括号内。默认为127.0.0.1, [::1] hosts.resolve_timeout 在每轮ping中等待DNS查找的时间量。指定为 时间单位。默认为5秒。 单播发现使用传输模块执行发现。 1.3 master选举作为Ping过程的一部分，集群的主节点要么当选要么加入假期。这是自动完成的。ping的默认超时为3秒 1discovery.zen.ping_timeout（默认为3s） 如果在超时后没有做出决定，则重新启动ping程序。在缓慢或拥塞的网络中，在作出选举决定之前，三秒可能不足以让节点意识到其环境中的其他节点。在这种情况下，应该谨慎地增加超时时间，因为这会减慢选举进程。一旦一个节点决定加入一个现有的已形成的集群，它将发送一个加入请求给主设备（discovery.zen.join_timeout）的超时默认值是ping超时的20倍。 当主节点停止或遇到问题时，群集节点会再次启动ping并选择新的主节点。这种ping测试也可以作为防止（部分）网络故障的保护，其中一个节点可能会不公正地认为主站发生故障。在这种情况下，节点将简单地从其他节点听到关于当前活动的主节点的信息。 如果discovery.zen.master_election.ignore_non_master_pings是true，没有参与资格（节点，其中节点坪node.master是false）的主选期间忽略; 默认值是 false。 可以通过设置node.master来排除节点成为主节点false。 该discovery.zen.minimum_master_nodes套需要加入新当选主为了选举完成并当选节点接受其主控权掌握合格节点的最小数量。相同的设置控制应该成为任何活动集群一部分的活动主节点合格节点的最小数量。如果不满足这个要求，活动的主节点将下台，新的主节点选举将开始。 此设置必须设置为您的主要合格节点的法定人数。建议避免只有两个主节点，因为两个法定人数是两个。因此，任何主节点的损失都将导致无法运行的群集。 1.4 故障检测有两个故障检测进程正在运行。第一种方法是通过主设备对群集中的所有其他节点进行ping操作，并验证它们是否处于活动状态。另一方面，每个节点都会主动确认它是否仍然存在或需要启动选举过程。 以下设置使用discovery.zen.fd前缀控制故障检测过程 ： 设置 描述 ping_interval 一个节点多久发作一次。默认为1s。 ping_timeout 等待ping响应需要多长时间，默认为 30s。 ping_retries 有多少ping故障/超时会导致节点被视为失败。默认为3。 1.5 群集状态更新Cluster state updates主节点是群集中，可以使改变集群状态的唯一节点。主节点一次处理一个集群状态更新，状态改变和发布更新的到集群中的所有其他节点。每个节点接收发布消息，确认它，但还没有立即应用它。如果主节点没有接收来自节点确认的数量至少为discovery.zen.minimum_master_nodes，在时间（由受控discovery.zen.commit_timeout设置，默认值为30秒）内。节点集群状态改变被拒绝。 一旦足够的节点已作出回应，集群状态改变被提交然后消息将被发送到所有结点。然后节点然后进行新的群集状态适用于他们的内部状态。主节点等待所有节点响应，在去队列处理下一个状态更新之前，直到超时，超时时间是在discovery.zen.publish_timeout默认情况下设置为30秒，时间从发布开始时测量h超时设置可以通过动态的改变集群更新设置API 1.6 无主块No master block要使群集完全可操作，它必须具有活动的主节点和一些有主资格的节点，并且主资格的节点必须满足的数目必须满足discovery.zen.minimum_master_nodes设置的值。如果设置 discovery.zen.no_master_block ，那么设置控制在没有活动的主设备时应拒绝哪些操作。 该discovery.zen.no_master_block设置有两个有效选项： all 节点上的所有操作（即读取和写入操作）都将被拒绝。这也适用于api集群状态读取或写入操作，如get索引设置，put映射和集群状态api。 write （默认）写入操作将被拒绝。基于最后一次已知的群集配置，读取操作将成功。这可能会导致部分读取过时的数据，因为此节点可能与群集的其余部分隔离。 该discovery.zen.no_master_block设置不适用于基于节点的apis（例如，群集统计信息，节点信息和节点统计信息apis）。对这些apis的请求不会被阻止，并且可以在任何可用的节点上运行。 1.7 Fault Delection用ping的方式来确定node是否在集群里面 二、Discovery源码分析2.1 Discovery类图 2.2 与Discovery相关的几个类ZenDiscovery.java 模块的主类，也是启动这个模块的入口，由Node.java调用并初始化，几乎涵盖了全部的发现协议的逻辑，是一个高度内聚了类，它有一些成员变量，需要明白他们的意思： pingTimeout：取自discovery.zen.ping_timeout（默认为3s）允许调整选举时间来处理网络慢或拥塞的情况（更高的值确保更少的失败机会） joinTimeout：取自discovery.zen.join_timeout（默认值为ping超时的20倍）。当一个新的node加入集群时，将会发个join的request到master，这个request的timeout即joinTimeout。 joinRetryAttempts：join重试的次数，默认为3次。 joinRetryDelay：重试的间隔，默认为100ms。 maxPingsFromAnotherMaster：容忍其他master发出的,在强制其他或是本地master rejoin之前的次数。 masterElectionIgnoreNonMasters：用来控制在主节点选举时候的ping响应，只有在极端情况下才会使用这个参数，平时一般不用配置，默认值为false 1234有人说，选举master时，node.master为false的节点的投票是不起作用的，这个说法不完全正确：如果discovery.zen.master_election.ignore_non_master_pings设置为true，那么以上说法正确，但是默认是false，也就是说，它们的投票是起作用的，只是它们不可能成为master。所以我觉得，集群机器数不大的话，除了负担特别重的机器，都设置为node.master为true比较妥当。设置需要加入新一轮master选举的“master”候选人的最小数量也就是说，集群中，该值是针对那些node.master=true的来设置的，建议&gt;=num(node.master=true)/2+1.并不是有的朋友解释的，集群机器数量的除以2再加1，当然默认情况下是，因为默认情况下，discovery.zen.master_election.ignore_non_master_pings为false masterElectionWaitForJoinsTimeout：master选举时等待join的timeout,默认是joinTimeout的一半。 其中joinRetryAttempts和maxPingsFromAnotherMaster是一定要大于等于1的。 UnicastZenPing.java 是一个ZenPing 实现类，主要是负责底层和其他Nodes建立并维护连接的任务 PublishClusterStateAction.java 在ZenDiscovery中的变量名是publishClusterState，之前讲过，这些**Action 都是对**Service的封装，因此它主要是用来处理发送事件和处理事件的接口，比如发送一个clusterStateChangeEvent 和处理这个event，都是通过这个类调用 MasterFaultDetection.java 构建完cluster后所有的node用来检测master存活状态的类 NodeFaultDetection.java 构建完cluster后master用来检测其他node存活状态的类 2.3 如何运行我们通过上一篇的分析知道，在ES启动的时候会去实例化Node，然后调用Node#start()方法启动各个module，Discovery是在实例化Node的时候通过guice进行注入的，在Node启动的时候去启动的，代码如下： Node的构造函数中实例化 12345...final DiscoveryModule discoveryModule = new DiscoveryModule(this.settings, threadPool, transportService, namedWriteableRegistry, networkService, clusterService, pluginsService.filterPlugins(DiscoveryPlugin.class));...b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery()); 2.3.1 ZenDiscover的初始化初始化的时候会加载我上段ZenDiscovery模块介绍提到的几个模块，我就不再重复了，值得注意的是Fault Delection的分为两个masterFD和nodesFD；其次还加载了一些对于discover的配置 2.3.2 ZenDiscovery运行其实ZenDiscover的运行就是几个子模块的运行；它是通过Node#start()方法启动的。 在Node#start()方法中：我们可以看到Discovery相关的代码 12345678910Discovery discovery = injector.getInstance(Discovery.class);clusterService.setDiscoverySettings(discovery.getDiscoverySettings());clusterService.addInitialStateBlock(discovery.getDiscoverySettings().getNoMasterBlock());clusterService.setClusterStatePublisher(discovery::publish);... // start after cluster service so the local disco is known discovery.start(); transportService.acceptIncomingRequests(); // 核心方法 discovery.startInitialJoin(); 在DiscoveryModule类中， 12345678910111213141516171819202122 Map&lt;String, Supplier&lt;Discovery&gt;&gt; discoveryTypes = new HashMap&lt;&gt;(); discoveryTypes.put(\"zen\", () -&gt; new ZenDiscovery(settings, threadPool, transportService, namedWriteableRegistry, clusterService, hostsProvider)); discoveryTypes.put(\"none\", () -&gt; new NoneDiscovery(settings, clusterService, clusterService.getClusterSettings())); discoveryTypes.put(\"single-node\", () -&gt; new SingleNodeDiscovery(settings, clusterService)); for (DiscoveryPlugin plugin : plugins) &#123; plugin.getDiscoveryTypes(threadPool, transportService, namedWriteableRegistry, clusterService, hostsProvider).entrySet().forEach(entry -&gt; &#123; if (discoveryTypes.put(entry.getKey(), entry.getValue()) != null) &#123; throw new IllegalArgumentException(\"Cannot register discovery type [\" + entry.getKey() + \"] twice\"); &#125; &#125;); &#125;String discoveryType = DISCOVERY_TYPE_SETTING.get(settings); // 这里是函数式编程的用法，详情请百度或者Google Supplier&lt;Discovery&gt; discoverySupplier = discoveryTypes.get(discoveryType); if (discoverySupplier == null) &#123; throw new IllegalArgumentException(\"Unknown discovery type [\" + discoveryType + \"]\"); &#125; Loggers.getLogger(getClass(), settings).info(\"using discovery type [&#123;&#125;]\", discoveryType); discovery = Objects.requireNonNull(discoverySupplier.get()); 由上面的代码可以看出，这里Discovery的实例是由DisdcoveryModule的suppiler 提供。 discovery.start()方法调用AbstractLifecycleComponent#start()方法进行监听，同时在该start()方法中调用ZenDiscovery#doStart()方法进行真正工作，这里用到的模板方法的设计模式，在Spring中很多地方都是这样使用的，首先在抽象类 123456789101112131415161718192021public void start() &#123; // ES的生命周期zhuangt:INITIALIZED -&amp;gt; STARTED, STOPPED, CLOSED // 如果不可以启动，直接返回 if (!lifecycle.canMoveToStarted()) &#123; return; &#125; // 启动之前循环监听 for (LifecycleListener listener : listeners) &#123; listener.beforeStart(); &#125; // 调用ZenDiscovery的doStart()方法对一些变量进行初始化工作 doStart(); // 处理状态 lifecycle.moveToStarted(); // 启动之后的监听 for (LifecycleListener listener : listeners) &#123; listener.afterStart(); &#125; &#125; // 模板方法：由具体的Discovery类来实现，比如ZenDiscovery类 protected abstract void doStart(); ZenDiscovery的start()方法： 1234567891011121314/** * 该方法其实就是做了一些初始化操作，不要被它的start()命名给误导 */ @Override protected void doStart() &#123; // 节点故障探测设置 nodesFD.setLocalNode(clusterService.localNode()); // 调用连接线程控制类进行初始化操作 joinThreadControl.start(); // 设置ping参数 zenPing.start(this); this.nodeJoinController = new NodeJoinController(clusterService, allocationService, electMaster, settings); this.nodeRemovalExecutor = new NodeRemovalClusterStateTaskExecutor(allocationService, electMaster, this::submitRejoin, logger); &#125; discovery.startInitialJoin()方法分析： 1234567891011121314151617181920@Override public void startInitialJoin() &#123; // start the join thread from a cluster state update. See &#123;@link JoinThreadControl&#125; for details. clusterService.submitStateUpdateTask(\"initial_join\", new LocalClusterUpdateTask() &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) throws Exception &#123; // do the join on a different thread, the DiscoveryService waits for 30s anyhow till it is discovered // 调用这个方法进行详细的处理 joinThreadControl.startNewThreadIfNotRunning(); // 返回LocalClusterUpdateTask return unchanged(); &#125; // 加入集群失败后的逻辑：这里只是打印日志 @Override public void onFailure(String source, @org.elasticsearch.common.Nullable Exception e) &#123; logger.warn(\"failed to start initial join process\", e); &#125; &#125;); &#125; 我们通过上面的源码分析知道，joinThreadControl.startNewThreadIfNotRunning()这个方法是其核心处理逻辑，我们来看看它做了什么工作 123456789101112131415161718192021222324/** starts a new joining thread if there is no currently active one and join thread controlling is started */ public void startNewThreadIfNotRunning() &#123; ClusterService.assertClusterStateThread(); // 如果join线程还存活，直接返回 if (joinThreadActive()) &#123; return; &#125; // 从ES中的线程池获取generic线程池，然后提交一个任务 threadPool.generic().execute(new Runnable() &#123; @Override public void run() &#123; Thread currentThread = Thread.currentThread(); // CAS操作： if (!currentJoinThread.compareAndSet(null, currentThread)) &#123; return; &#125; // 第一次启动，这里的running肯定是true，是在之前的doStart()中进行初始化的 while (running.get() &amp;&amp; joinThreadActive(currentThread)) &#123; try &#123; // 第一步：首先自己先加入集群 innerJoinCluster(); return; ..... &#125; 通过上面的分析我们知道，如果join线程还存活，则直接返回，否则从从ES中的线程池获取generic线程池，然后提交一个任务，在该任务中主要调用innerJoinCluster()方法加入集群。 我们来猜猜看看innerJoinCluster()方法做了什么？ 1我们要加入一个集群，肯定先要找到组织，熟悉Elasticsearch的配置都知道，我们的ES默认的集群名称是slasticsearch，如果配置了，则为我们配置的名称（有点废话了），肯定要进行master选举 innerJoinCluster()方法分析： 12345678private void innerJoinCluster() &#123; DiscoveryNode masterNode = null; final Thread currentThread = Thread.currentThread(); nodeJoinController.startElectionContext(); // 通过findMaster方法来进行Master选举: while (masterNode == null &amp;&amp; joinThreadControl.joinThreadActive(currentThread)) &#123; masterNode = findMaster(); &#125; 三、master选举源码分析阅读该部分源码，我们带着下面的一些问题去看。 分布式系统设计思想？ ES为什么要master选举？ 有哪些选举算法？ master选举主流程和详细流程？ 什么时候触发选举？ 为什么不用ZK来实现master选举？ 如何获取到最新的状态数据？ 3.1 分布式系统设计思想所有的分布式系统都会遇到各个节点数据同步问题（一致性）、因网络故障存在的延迟、脑裂等问题都需要用一套合理的解决方案。比如，在互联网中比较经典的CAP理论和BASE理论。在ES中同样需要面临这样的问题？ 3.2 ES为什么要master选举一种选择是分布式哈希表(DHT),可以支持每小时数千个节点的离开和加入,他可以在不了解底层网络拓扑的异构网络中工作,查询响应时间大约为4到10跳(中转次数)，但是在相对稳定的对等网络中,Master模式会更好 Elasticsearch的典型场景中的另一个简化是集群中没有那么多节点。 通常，节点的数量远远小于单个节点能够维护的连接数，并且网格环境不必经常处理节点加入和离开。 这就是为什么master的做法更适合Elasticsearch。 3.3 选举算法3.3.1 Bully算法Leader选举的基本算法之一。 它假定所有节点都有一个惟一的ID，该ID对节点进行排序。 任何时候的当前Leader都是参与集群的最高id节点。 该算法的优点是易于实现,但是,当拥有最大 id 的节点处于不稳定状态的场景下会有问题,例如 Master 负载过重而假死,集群拥有第二大id 的节点被选为 新主,这时原来的 Master 恢复,再次被选为新主,然后又假死… elasticsearch 通过推迟选举直到当前的 Master 失效来解决上述问题,但是容易产生脑裂,再通过 法定得票人数过半 解决脑裂 3.3.2 Paxos算法Paxos实现起来非常复杂,但非常强大，尤其在什么时机,以及如何进行选举方面的灵活性比简单的Bully算法有很大的优势，因为在现实生活中，存在比网络链接异常更多的故障模式。比较典型的是Zookeeper在该算法进行了改进，形成自己的一套选举算法。 3.4 选举流程只有一个 Leader将当前版本的全局集群状态推送到每个节点。 ZenDiscovery（默认）过程就是这样的: 每个节点计算最低的已知节点ID，并向该节点发送领导投票 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演领导者的角色，开始发布集群状态。 所有节点都会参数选举,并参与投票,但是,只有有资格成为 master 的节点的投票才有效. 有多少选票赢得选举的定义就是所谓的法定人数。 在弹性搜索中，法定大小是一个可配置的参数。 （一般配置成:可以成为master节点数n/2+1） 3.5 选举详细流程3.5.1 获取PingResponse列表节奏上一个Discovery模块的源码进行分析： 123456789101112private DiscoveryNode findMaster() &#123; logger.trace(\"starting to ping\"); // ping所有节点并获取PingResponse List&lt;ZenPing.PingResponse&gt; fullPingResponses = pingAndWait(pingTimeout).toList();&#125;private ZenPing.PingCollection pingAndWait(TimeValue timeout) &#123; final CompletableFuture&lt;ZenPing.PingCollection&gt; response = new CompletableFuture&lt;&gt;(); try &#123; // 步骤2：ping所有节点，调用UnicastZenPing的ping()方法 zenPing.ping(response::complete, timeout); &#125; 从 response::complete和response.get两句大致就能猜猜，这个方法里面会异步发起请求，主线程等待response。 UnicastZenPing#ping()方法 12345678910111213141516171819202122232425262728293031 public void ping(final Consumer&lt;PingCollection&gt; resultsConsumer, final TimeValue duration) &#123; ping(resultsConsumer, duration, duration); &#125;protected void ping(final Consumer&lt;PingCollection&gt; resultsConsumer, final TimeValue scheduleDuration, final TimeValue requestDuration) &#123; final List&lt;DiscoveryNode&gt; seedNodes; try &#123; seedNodes = resolveHostsLists( // 1、从配置的discovery.zen.ping.unicast.hosts列表中获取 unicastZenPingExecutorService, logger, configuredHosts, limitPortCounts, transportService, UNICAST_NODE_PREFIX, resolveTimeout); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; // 通过其他方式添加seedNodes：List&lt;DiscoveryNode&gt; seedNodes.addAll(hostsProvider.buildDynamicNodes()); // 本实例最近一次的clusterState的masterNode final DiscoveryNodes nodes = contextProvider.clusterState().nodes(); // add all possible master nodes that were active in the last known cluster configuration // 步骤3：添加有成为master资格的节点 for (ObjectCursor&lt;DiscoveryNode&gt; masterNode : nodes.getMasterNodes().values()) &#123; seedNodes.add(masterNode.value); &#125; ....&#125; 从上面的代码可以看出，在sendPing之前需要确定seedNodes（List）,它从三个地方获取，第一，从配置的discovery.zen.ping.unicast.hosts列表中获取，第二、hostsProvider.buildDynamicNodes()中获取，最后，从本实例最近一次的clusterState的masterNode中获取。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public static List&lt;DiscoveryNode&gt; resolveHostsLists()&#123;// create tasks to submit to the executor service; we will wait up to resolveTimeout for these tasks to complete final List&lt;Callable&lt;TransportAddress[]&gt;&gt; callables = hosts .stream() // 地址列表通过TransportService构造 .map(hn -&gt; (Callable&lt;TransportAddress[]&gt;) () -&gt; transportService.addressesFromString(hn, limitPortCounts)) .collect(Collectors.toList()); // 异步Future列表 final List&lt;Future&lt;TransportAddress[]&gt;&gt; futures = executorService.invokeAll(callables, resolveTimeout.nanos(), TimeUnit.NANOSECONDS); final List&lt;DiscoveryNode&gt; discoveryNodes = new ArrayList&lt;&gt;(); final Set&lt;TransportAddress&gt; localAddresses = new HashSet&lt;&gt;(); localAddresses.add(transportService.boundAddress().publishAddress()); localAddresses.addAll(Arrays.asList(transportService.boundAddress().boundAddresses())); // ExecutorService#invokeAll guarantees that the futures are returned in the iteration order of the tasks so we can associate the // hostname with the corresponding task by iterating together final Iterator&lt;String&gt; it = hosts.iterator(); // 循环获取异步结果：就是简单的IP获取，为啥会用异步方式？ // 猜猜是因为host中可能配置有域名，怕解析时间过长 for (final Future&lt;TransportAddress[]&gt; future : futures) &#123; final String hostname = it.next(); if (!future.isCancelled()) &#123; assert future.isDone(); try &#123; final TransportAddress[] addresses = future.get(); logger.trace(\"resolved host [&#123;&#125;] to &#123;&#125;\", hostname, addresses); for (int addressId = 0; addressId &lt; addresses.length; addressId++) &#123; final TransportAddress address = addresses[addressId]; // no point in pinging ourselves if (localAddresses.contains(address) == false) &#123; discoveryNodes.add( new DiscoveryNode( nodeId_prefix + hostname + \"_\" + addressId + \"#\", address, emptyMap(), emptySet(), Version.CURRENT.minimumCompatibilityVersion())); &#125; &#125; &#125; catch (final ExecutionException e) &#123; assert e.getCause() != null; final String message = \"failed to resolve host [\" + hostname + \"]\"; logger.warn(message, e.getCause()); &#125; &#125; else &#123; logger.warn(\"timed out after [&#123;&#125;] resolving host [&#123;&#125;]\", resolveTimeout, hostname); &#125; &#125; // 返回List&lt;DiscoveryNode&gt; return discoveryNodes;&#125; 拿到seedNodes之后就需要发起连接，这里会构造一个叫PingRound的类来统计，并且分别会在 scheduleDuration的0, 1/3, 2/3时刻发起一轮sendPing操作，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041// 刚开始启动，集群健康状态设置为RED final ConnectionProfile connectionProfile = ConnectionProfile.buildSingleChannelProfile(TransportRequestOptions.Type.REG, requestDuration, requestDuration); // 构造一个叫PingRound的类来统计：这个类有id，seedNodes、pingListener、localNode等 final PingingRound pingingRound = new PingingRound(pingingRoundIdGenerator.incrementAndGet(), seedNodes, resultsConsumer, nodes.getLocalNode(), connectionProfile); activePingingRounds.put(pingingRound.id(), pingingRound); // 构造ping发送对象pingSender final AbstractRunnable pingSender = new AbstractRunnable() &#123; @Override public void onFailure(Exception e) &#123; if (e instanceof AlreadyClosedException == false) &#123; logger.warn(\"unexpected error while pinging\", e); &#125; &#125; @Override protected void doRun() throws Exception &#123; // 发送sendPings sendPings(requestDuration, pingingRound); &#125; &#125;; // 提交线generic程池：ping的连接不像其他那样由transportService 来保持长连接，而是即建即销，的一条连接 // 0 threadPool.generic().execute(pingSender); // 1/3 threadPool.schedule(TimeValue.timeValueMillis(scheduleDuration.millis() / 3), ThreadPool.Names.GENERIC, pingSender); // 2/3 threadPool.schedule(TimeValue.timeValueMillis(scheduleDuration.millis() / 3 * 2), ThreadPool.Names.GENERIC, pingSender); threadPool.schedule(scheduleDuration, ThreadPool.Names.GENERIC, new AbstractRunnable() &#123; @Override protected void doRun() throws Exception &#123; // 关闭临时连接 finishPingingRound(pingingRound); &#125; @Override public void onFailure(Exception e) &#123; logger.warn(\"unexpected error while finishing pinging round\", e); &#125; &#125;); 这里注意一点就是ping的连接不像其他那样由transportService 来保持长连接，而是即建即销，的一条连接。最后finishPingRound时则把这些临时连接关闭。 3.5.2 选举master再次回到findMaster()方法，上面ping完之后，我们拿到了一个fullPingResponses列表，这里有一个filter操作，如果我们启用了discovery.zen.master_election.ignore_non_master_pings则就会把那些node.master = false 那些节点都忽略掉： 123// filter responses // 如果我们启用了discovery.zen.master_election.ignore_non_master_pings则就会把那些node.master = false 那些节点都忽略掉： final List&lt;ZenPing.PingResponse&gt; pingResponses = filterPingResponses(fullPingResponses, masterElectionIgnoreNonMasters, logger); 紧接着就要从这些pingResponse里面收集其他节点当前的master节点是谁，最后拿到一个activeMasters的候选的名单，并把自己给去掉，Discovery的策略是非直到最后一刻都不会选自己为master（猜猜可能是预防脑裂吧）。 123456789101112131415161718192021222324252627282930 //从pingResponse列表里面收集其他节点当前的master节点是谁，最后拿到一个activeMasters的候选的名单 List&lt;DiscoveryNode&gt; activeMasters = new ArrayList&lt;&gt;(); for (ZenPing.PingResponse pingResponse : pingResponses) &#123; // We can't include the local node in pingMasters list, otherwise we may up electing ourselves without // any check / verifications from other nodes in ZenDiscover#DiscoveryNode() if (pingResponse.master() != null &amp;&amp; !localNode.equals(pingResponse.master())) &#123; // 添加master到activeMasters名单 activeMasters.add(pingResponse.master()); &#125; &#125;// 如果可选名单为空，就是大家刚刚启动，则进入选举环节 if (activeMasters.isEmpty()) &#123; // 如果有足够的候选人参与:由discovery.zen.minimum_master_nodes参数指定 if (electMaster.hasEnoughCandidates(masterCandidates)) &#123; // 具体的选举方法：非常简单，就是找到id为最小的节点 final ElectMasterService.MasterCandidate winner = electMaster.electMaster(masterCandidates); logger.trace(\"candidate &#123;&#125; won election\", winner); return winner.getNode(); &#125; else &#123; // if we don't have enough master nodes, we bail, because there are not enough master to elect from logger.warn(\"not enough master nodes discovered during pinging (found [&#123;&#125;], but needed [&#123;&#125;]), pinging again\", masterCandidates, electMaster.minimumMasterNodes()); return null; &#125; &#125; else &#123; assert !activeMasters.contains(localNode) : \"local node should never be elected as master when other nodes indicate an active master\"; // lets tie break between discovered nodes return electMaster.tieBreakActiveMasters(activeMasters); &#125; 接着就对这个候选列表判断，最理想就是列表为1，就证明你当前加入一个健康的集群中去，如果是有多个（正常情况下肯定不会有多个，除非你没有配置那个discovery.zen.minimum_master_nodes导致很多分治子群了）则在列表里面简单的选一个id号最小的（意思是不参乱了）。如果列表为空，就是大家都是刚启动，则进入选举环节，选举环节还是选出那个id最小的。 现在这个masterNode是定下来了，如果这个master是别人，则就简单的发送个join请求过去就好了，如果选出的master是你自己，那就还有一件很重要的事要做，还记得那个discovery.zen.minimum_master_nodes参数吗，一般要求这个值需要配成你的集群的cluster节点数的一半+1，以预防有脑裂，当前如果你选举出自己是master，那么你还需要等待 minimumMasterNodes() - 1 这么多个人join过来并认同你是master，那你才是真正的master，选举才结束。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// 上面通过findMaster()方法找到masterNode，但是这时还不能算是真正的master节点 // 如果选出了自己 if (clusterService.localNode().equals(masterNode)) &#123; // 需要的节点数是discovery.zen.minimum_master_nodes-1（因为masterNode自己不需要表决了） final int requiredJoins = Math.max(0, electMaster.minimumMasterNodes() - 1); // we count as one logger.debug(\"elected as master, waiting for incoming joins ([&#123;&#125;] needed)\", requiredJoins); nodeJoinController.waitToBeElectedAsMaster(requiredJoins, masterElectionWaitForJoinsTimeout, new NodeJoinController.ElectionCallback() &#123; @Override public void onElectedAsMaster(ClusterState state) &#123; joinThreadControl.markThreadAsDone(currentThread); // we only starts nodesFD if we are master (it may be that we received a cluster state while pinging) nodesFD.updateNodesAndPing(state); // start the nodes FD &#125; @Override public void onFailure(Throwable t) &#123; logger.trace(\"failed while waiting for nodes to join, rejoining\", t); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); &#125; &#125; ); &#125; else &#123; // 如果这个master是别人，则就简单的发送个join请求过去就好了 // process any incoming joins (they will fail because we are not the master) nodeJoinController.stopElectionContext(masterNode + \" elected\"); // send join request final boolean success = joinElectedMaster(masterNode); // finalize join through the cluster state update thread final DiscoveryNode finalMasterNode = masterNode; clusterService.submitStateUpdateTask(\"finalize_join (\" + masterNode + \")\", new LocalClusterUpdateTask() &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) throws Exception &#123; if (!success) &#123; // failed to join. Try again... joinThreadControl.markThreadAsDoneAndStartNew(currentThread); return unchanged(); &#125; if (currentState.getNodes().getMasterNode() == null) &#123; // Post 1.3.0, the master should publish a new cluster state before acking our join request. we now should have // a valid master. logger.debug(\"no master node is set, despite of join request completing. retrying pings.\"); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); return unchanged(); &#125; if (!currentState.getNodes().getMasterNode().equals(finalMasterNode)) &#123; return joinThreadControl.stopRunningThreadAndRejoin(currentState, \"master_switched_while_finalizing_join\"); &#125; // Note: we do not have to start master fault detection here because it's set at &#123;@link #processNextPendingClusterState &#125; // when the first cluster state arrives. joinThreadControl.markThreadAsDone(currentThread); return unchanged(); &#125; @Override public void onFailure(String source, @Nullable Exception e) &#123; logger.error(\"unexpected error while trying to finalize cluster join\", e); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); &#125; &#125;); &#125; 这里会有一个等待join超时配置，超时后还没有满足数量的join请求，则选举失败，需要新一轮选举。 3.6 什么时候触发Master选举 集群启动 Master 失效 非 Master 节点运行的 MasterFaultDetection 检测到 Master 失效,在其注册的 listener 中执行 handleMasterGone,执行 rejoin 操作,重新选主.注意,即使一个节点认为 Master 失效也会进入选主流程 3.7 为什么不用ZK？个人猜测可能是当时ZK不是很流行，应用不广泛。Elasticsearch第一版发布于2010年，zookeeper发布于2008年，时间间隔不是很长。 3.8 如何获取到最新的数据现在 Master 已成功当选,但是他未必有最新的 clusterState 信息,这些信息如何得到?gateway 模块负责 clusterState 持久化和恢复,Master 节点在当选后,会通过下面的流程获取到集群最新 clusterState: 枚举集群中有资格成为 Master 的节点列表 通过listGatewayMetaState获取这些节点上存储的 clusterState 对比这些节点的 clusterState 版本号,选择最新的作为 clusterState 并应用. 这一块将在后续的gateway源码分析模块中进行分析。 3.9 总结master选举四、同步状态选举流程结束后两个重要的小task就开始工作了，分别是MasterFaultDetection和NodesFaultDetection，这两个task很简单，就拿一个master的来看，唯一不同就是node的里面保存的是cluster里面所有的nodes。 这两个是在ZenDiscovery初始化的时候就初始化好的Listener：MasterNodeFailureListener和NodeFaultDetectionListener分别是实现了MasterFaultDetection和NodesFaultDetection的内部类Listener。 如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 private class MasterNodeFailureListener implements MasterFaultDetection.Listener &#123; @Override public void onMasterFailure(DiscoveryNode masterNode, Throwable cause, String reason) &#123; handleMasterGone(masterNode, cause, reason); &#125; &#125;private class NodeFaultDetectionListener extends NodesFaultDetection.Listener &#123; private final AtomicInteger pingsWhileMaster = new AtomicInteger(0); @Override public void onNodeFailure(DiscoveryNode node, String reason) &#123; handleNodeFailure(node, reason); &#125; @Override public void onPingReceived(final NodesFaultDetection.PingRequest pingRequest) &#123; // if we are master, we don't expect any fault detection from another node. If we get it // means we potentially have two masters in the cluster. if (!localNodeMaster()) &#123; pingsWhileMaster.set(0); return; &#125; if (pingsWhileMaster.incrementAndGet() &lt; maxPingsFromAnotherMaster) &#123; logger.trace(\"got a ping from another master &#123;&#125;. current ping count: [&#123;&#125;]\", pingRequest.masterNode(), pingsWhileMaster.get()); return; &#125; logger.debug(\"got a ping from another master &#123;&#125;. resolving who should rejoin. current ping count: [&#123;&#125;]\", pingRequest.masterNode(), pingsWhileMaster.get()); clusterService.submitStateUpdateTask(\"ping from another master\", new LocalClusterUpdateTask(Priority.IMMEDIATE) &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) throws Exception &#123; if (currentState.nodes().isLocalNodeElectedMaster()) &#123; pingsWhileMaster.set(0); return handleAnotherMaster(currentState, pingRequest.masterNode(), pingRequest.clusterStateVersion(), \"node fd ping\"); &#125; else &#123; return unchanged(); &#125; &#125; @Override public void onFailure(String source, Exception e) &#123; logger.debug(\"unexpected error during cluster state update task after pings from another master\", e); &#125; &#125;); &#125; &#125; 对与这两个类，我们只需要看run方法就行了 MasterFaultDetection#run() 1234567...// check if the master node did not get switched on us..., if it did, we simply return with no reschedule if (masterToPing.equals(MasterFaultDetection.this.masterNode())) &#123; // we don't stop on disconnection from master, we keep pinging it threadPool.schedule(pingInterval, ThreadPool.Names.SAME, MasterPinger.this); &#125;... 和findMaster（）里面的不一样就是这里不再用temp连接而是在threadPool里面的长连接，这里对错误进行分类，如果是一些业务错误则不受尝试次数的限制，如请求的节点根本不是master节点，请求的master不是自己的cluster等等，会直接调用notifyMasterFailure回调，如果是常规错误，则记录尝试次数，当错误次数超过了阈值，则调用notifyMasterFailure回调。 接着看MasterNodeFailureListener的handleMasterGone()方法 123456789101112131415161718192021222324252627282930313233343536private void handleMasterGone(final DiscoveryNode masterNode, final Throwable cause, final String reason) &#123; // 状态不是已经开始，则直接返回 if (lifecycleState() != Lifecycle.State.STARTED) &#123; // not started, ignore a master failure return; &#125; // master是自己也直接返回 if (localNodeMaster()) &#123; // we might get this on both a master telling us shutting down, and then the disconnect failure return; &#125; logger.info((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"master_left [&#123;&#125;], reason [&#123;&#125;]\", masterNode, reason), cause); clusterService.submitStateUpdateTask(\"master_failed (\" + masterNode + \")\", new LocalClusterUpdateTask(Priority.IMMEDIATE) &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) &#123; if (!masterNode.equals(currentState.nodes().getMasterNode())) &#123; // master got switched on us, no need to send anything return unchanged(); &#125; // flush any pending cluster states from old master, so it will not be set as master again publishClusterState.pendingStatesQueue().failAllStatesAndClear(new ElasticsearchException(\"master left [&#123;&#125;]\", reason)); // 重新进入rejion流程 return rejoin(currentState, \"master left (reason = \" + reason + \")\"); &#125; @Override public void onFailure(String source, Exception e) &#123; logger.error((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"unexpected failure during [&#123;&#125;]\", source), e); &#125; &#125;); &#125; 在前面分析的Node初始化时，注册了clusterState的发布方法 1clusterService.setClusterStatePublisher(discovery::publish); 我们来看看publish()方法 12345678910111213141516171819202122public void publish(ClusterChangedEvent clusterChangedEvent, AckListener ackListener) &#123; if (!clusterChangedEvent.state().getNodes().isLocalNodeElectedMaster()) &#123; throw new IllegalStateException(\"Shouldn't publish state when not master\"); &#125; try &#123; // 调用PublishClusterStateAction的publish()方法 publishClusterState.publish(clusterChangedEvent, electMaster.minimumMasterNodes(), ackListener); &#125; catch (FailedToCommitClusterStateException t) &#123; // cluster service logs a WARN message logger.debug(\"failed to publish cluster state version [&#123;&#125;] (not enough nodes acknowledged, min master nodes [&#123;&#125;])\", clusterChangedEvent.state().version(), electMaster.minimumMasterNodes()); submitRejoin(\"zen-disco-failed-to-publish\"); throw t; &#125; // update the set of nodes to ping after the new cluster state has been published nodesFD.updateNodesAndPing(clusterChangedEvent.state()); // clean the pending cluster queue - we are currently master, so any pending cluster state should be failed // note that we also clean the queue on master failure (see handleMasterGone) but a delayed cluster state publish // from a stale master can still make it in the queue during the election (but not be committed) publishClusterState.pendingStatesQueue().failAllStatesAndClear(new ElasticsearchException(\"elected as master\")); &#125; 调用PublishClusterStateAction的publish()方法，在PublishClusterStateAction初始化的时候，会调用CommitClusterStateRequestHandler的构造方法，然后传递给TransportService的registerRequestHandler方法，最后会处理到一个pendingStatesQueue队列。 pendingStatesQueue会保存着每个待提交的state，并且也会提供最新的commit 的state给其他请求。 123456789101112131415161718192021222324252627282930313233343536private class CommitClusterStateRequestHandler implements TransportRequestHandler&lt;CommitClusterStateRequest&gt; &#123; @Override public void messageReceived(CommitClusterStateRequest request, final TransportChannel channel) throws Exception &#123; handleCommitRequest(request, channel); &#125; &#125; protected void handleCommitRequest(CommitClusterStateRequest request, final TransportChannel channel) &#123; final ClusterState state = pendingStatesQueue.markAsCommitted(request.stateUUID, new PendingClusterStatesQueue.StateProcessedListener() &#123; @Override public void onNewClusterStateProcessed() &#123; try &#123; // send a response to the master to indicate that this cluster state has been processed post committing it. channel.sendResponse(TransportResponse.Empty.INSTANCE); &#125; catch (Exception e) &#123; logger.debug(\"failed to send response on cluster state processed\", e); onNewClusterStateFailed(e); &#125; &#125; @Override public void onNewClusterStateFailed(Exception e) &#123; try &#123; channel.sendResponse(e); &#125; catch (Exception inner) &#123; inner.addSuppressed(e); logger.debug(\"failed to send response on cluster state processed\", inner); &#125; &#125; &#125;); if (state != null) &#123; newPendingClusterStatelistener.onNewClusterState(\"master \" + state.nodes().getMasterNode() + \" committed version [\" + state.version() + \"]\"); &#125; &#125; 而发布clusterChangedEvent则交给了PublishClusterStateAction主要逻辑在innerPublish方法 1234567891011121314151617181920212223242526272829303132333435363738394041private void innerPublish(final ClusterChangedEvent clusterChangedEvent, final Set&lt;DiscoveryNode&gt; nodesToPublishTo, final SendingController sendingController, final boolean sendFullVersion, final Map&lt;Version, BytesReference&gt; serializedStates, final Map&lt;Version, BytesReference&gt; serializedDiffs) &#123; final ClusterState clusterState = clusterChangedEvent.state(); final ClusterState previousState = clusterChangedEvent.previousState(); final TimeValue publishTimeout = discoverySettings.getPublishTimeout(); final long publishingStartInNanos = System.nanoTime(); for (final DiscoveryNode node : nodesToPublishTo) &#123; // try and serialize the cluster state once (or per version), so we don't serialize it // per node when we send it over the wire, compress it while we are at it... // we don't send full version if node didn't exist in the previous version of cluster state if (sendFullVersion || !previousState.nodes().nodeExists(node)) &#123; sendFullClusterState(clusterState, serializedStates, node, publishTimeout, sendingController); &#125; else &#123; // 是在ES2.x之后才有的，目的是为了减少网络带宽 sendClusterStateDiff(clusterState, serializedDiffs, serializedStates, node, publishTimeout, sendingController); &#125; &#125; sendingController.waitForCommit(discoverySettings.getCommitTimeout()); try &#123; long timeLeftInNanos = Math.max(0, publishTimeout.nanos() - (System.nanoTime() - publishingStartInNanos)); final BlockingClusterStatePublishResponseHandler publishResponseHandler = sendingController.getPublishResponseHandler(); sendingController.setPublishingTimedOut(!publishResponseHandler.awaitAllNodes(TimeValue.timeValueNanos(timeLeftInNanos))); if (sendingController.getPublishingTimedOut()) &#123; DiscoveryNode[] pendingNodes = publishResponseHandler.pendingNodes(); // everyone may have just responded if (pendingNodes.length &gt; 0) &#123; logger.warn(\"timed out waiting for all nodes to process published state [&#123;&#125;] (timeout [&#123;&#125;], pending nodes: &#123;&#125;)\", clusterState.version(), publishTimeout, pendingNodes); &#125; &#125; &#125; catch (InterruptedException e) &#123; // ignore &amp; restore interrupt Thread.currentThread().interrupt(); &#125; &#125; 在ES2.x 之后支持了发送临近版本的diff来同步状态，目的为了省网络带宽，点进去ClusterState类可以发现里面的状态信息量还是不少，不过diff 需要你的版本和目前的最新的版本只相差一个版本，如果你要从1跳到3需要发送full的状态。sendFullClusterState 和sendClusterStateDiff都会调用底层transportService来真正发送状态，而状态记录通过一个sendingController来维护，没接收到ack或者timeout都会让controller来check是否达到了minMasterNodes-1，达到则标记这次的状态推送commited，其余情况都会抛错。 这里一定需要注意，Publish状态分成两个阶段，首先是sendNotification 1234567891011121314151617181920212223242526272829private void sendClusterStateToNode(final ClusterState clusterState, BytesReference bytes, final DiscoveryNode node, final TimeValue publishTimeout, final SendingController sendingController, final boolean sendDiffs, final Map&lt;Version, BytesReference&gt; serializedStates) &#123; try &#123; // -&gt; no need to put a timeout on the options here, because we want the response to eventually be received // and not log an error if it arrives after the timeout // -&gt; no need to compress, we already compressed the bytes TransportRequestOptions options = TransportRequestOptions.builder() .withType(TransportRequestOptions.Type.STATE).withCompress(false).build(); transportService.sendRequest(node, SEND_ACTION_NAME, new BytesTransportRequest(bytes, node.getVersion()), options, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) &#123; @Override public void handleResponse(TransportResponse.Empty response) &#123; if (sendingController.getPublishingTimedOut()) &#123; logger.debug(\"node &#123;&#125; responded for cluster state [&#123;&#125;] (took longer than [&#123;&#125;])\", node, clusterState.version(), publishTimeout); &#125; sendingController.onNodeSendAck(node); &#125; .... &#125; &#125; 就是master先向所有节点发送这个状态，需要等minMasterNodes确认了这个通知，master节点才会把这个状态mark成commited，再sendCommitToNode() 告知所有节点把commited这个状态。 1234567891011121314public synchronized void onNodeSendAck(DiscoveryNode node) &#123; if (committed) &#123; assert sendAckedBeforeCommit.isEmpty(); sendCommitToNode(node, clusterState, this); &#125; else if (committedOrFailed()) &#123; logger.trace(\"ignoring ack from [&#123;&#125;] for cluster state version [&#123;&#125;]. already failed\", node, clusterState.version()); &#125; else &#123; // we're still waiting sendAckedBeforeCommit.add(node); if (node.isMasterNode()) &#123; checkForCommitOrFailIfNoPending(node); &#125; &#125; &#125; 其他Node消息处理也是在该类当中，大致流程就不再详细查看了，入口在 1234transportService.registerRequestHandler(SEND_ACTION_NAME, BytesTransportRequest::new, ThreadPool.Names.SAME, false, false, new SendClusterStateRequestHandler()); transportService.registerRequestHandler(COMMIT_ACTION_NAME, CommitClusterStateRequest::new, ThreadPool.Names.SAME, false, false, new CommitClusterStateRequestHandler()); 都会调用底层transportService来真正发送状态。 五、Discovery模块总结从上面的源码分析我们知道，Discovery模块是ES的核心模块，它对ES集群进行master选举、状态发布和更新。","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]},{"title":"Redis面试题整理","slug":"redis/redis","date":"2018-05-26T04:12:57.000Z","updated":"2018-06-20T01:13:19.831Z","comments":true,"path":"2018/05/26/redis/redis/","link":"","permalink":"http://yoursite.com/2018/05/26/redis/redis/","excerpt":"","text":"1、Redis为什么单线程的，能说说它的原理吗Redis使用了单线程架构和I/O多路复用模型来实现高性能的内存数据库服务。Redis使用了单线程架构，预防了多线程可能产生的竞争问题，但是也会引入另外的问题。Redis单线程架构导致无法充分利用CPU多核特性，通常的做法是在一台机器上部署多个Redis实例。 那么Redis使用单线程模型，为什么还那么快： 第一，纯内存访问，Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，这是Redis达到每秒万级别访问的重要基础。第二，非阻塞I/O，Redis使用epoll作为I/O多路复用技术的实现，再加上Redis自身的事件处理模型将epoll中的连接、读写、关闭都转换为事件，不在网络I/O上浪费过多的时间。 第三，单线程避免了线程切换和竞态产生的消耗。 2、mySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据Redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 3、缓存穿透可以介绍⼀一下么？你认为应该如何解决这个问题缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库中查询。 解决思路： 1，如果查询数据库也为空，直接设置一个默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。 2，根据缓存数据Key的规则。例如我们公司是做机顶盒的，缓存数据以Mac为Key，Mac是有规则，如果不符合规则就过滤掉，这样可以过滤一部分查询。在做缓存规划的时候，Key有一定规则的话，可以采取这种办法。这种办法只能缓解一部分的压力，过滤和系统无关的查询，但是无法根治。 3，采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的BitSet中，不存在的数据将会被拦截掉，从而避免了对底层存储系统的查询压力。关于布隆过滤器，详情查看：基于BitSet的布隆过滤器(Bloom Filter) 大并发的缓存穿透会导致缓存雪崩。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[]},{"title":"elasticsearch源码分析(二)--启动","slug":"elasticsearch源码分析(二)--启动","date":"2018-05-25T04:10:57.000Z","updated":"2018-06-25T00:07:41.602Z","comments":true,"path":"2018/05/25/elasticsearch源码分析(二)--启动/","link":"","permalink":"http://yoursite.com/2018/05/25/elasticsearch源码分析(二)--启动/","excerpt":"","text":"由于最近半年来一直在使用Elasticsearch来做全文检索和ELK统一日志工作，对于ES还是觉得需要细细研究，才能感受到它的魅力，才能有所提高。 我们先提出几个问题： 启动入口在哪个类？ 启动需要做哪些初始化工作？ 如何加载配置文件？ 一、怎么找启动入口在哪个类看源码最头疼的事情就是找入口，相信很多刚开始也是这样，面对那么多模块中的类，很难找到一个切入点，我刚开始看也是这样，对于这样的问题，其实还是自己的积累不够，多学习就是了。 我们先来看看启动的脚本elasticsearch.bat或者elasticsearch.sh 1234567@echo off忽略其他%JAVA% %ES_JAVA_OPTS% %ES_PARAMS% -cp \"%ES_CLASSPATH%\" \"org.elasticsearch.bootstrap.Elasticsearch\" !newparams!ENDLOCAL 看到了org.elasticsearch.bootstrap.Elasticsearch这个类，不用想就是它的启动类。 二、Elasticsearch类做了什么事情我们先来猜想一下，我们下载完Elasticsearch的安装包，一般有两种部署方式：单机部署和集群部署 2.1 单机部署一般我们会修改{Elasticsearch_home}\\config下的elasticsearch.yml文件和jvm.options 在elasticsearch.yml中配置集群名称、节点名称、日志存放路径、数据存放路径、网络IP、http端口（9200）、Netty端口（9300）等 同时还会去初始化一些module，如下图 2.2 集群部署我们会在单机部署的基础上，增加Discovery模块（集群发现）的配置、 有哪些节点参与到集群当中：discovery.zen.ping.unicast.hosts: [“host1”, “host2”] 需要有几个皇子在场才可以选举投票出master：discovery.zen.minimum_master_nodes: 3 2.3 启动流程猜想通过上述分析我们知道，ES集群启动会做一些初始化工作、加载配置文件，加载一下扩展插件，如果是集群启动，还会进行master选举，master选举需要有足够多的节点参与投票，这个参数是可以指定。 三、启动源码分析3.1 Elasticsearch类图 3.2 Elasticsearch#main()方法我们先来看看org.elasticsearch.bootstrap.Elasticsearch#main()方法 12345678910111213141516171819202122public static void main(final String[] args) throws Exception &#123; // we want the JVM to think there is a security manager installed so that if internal policy decisions that would be based on the // presence of a security manager or lack thereof act as if there is a security manager present (e.g., DNS cache policy) System.setSecurityManager(new SecurityManager() &#123; @Override public void checkPermission(Permission perm) &#123; // grant all permissions so that we can later set the security manager to the one that we want &#125; &#125;); LogConfigurator.registerErrorListener(); // 调用构造器 final Elasticsearch elasticsearch = new Elasticsearch(); // 调用main方法，执行完后返回一个状态 int status = main(args, elasticsearch, Terminal.DEFAULT); // 判断状态是否启动成功 if (status != ExitCodes.OK) &#123; exit(status); &#125; &#125; static int main(final String[] args, final Elasticsearch elasticsearch, final Terminal terminal) throws Exception &#123; return elasticsearch.main(args, terminal); &#125; 通过上面的类图关系，我们知道Elasticsearch是一个Command，就是一开始先设置了一个SecurityManager，做一些检查checkPermission(Permission perm)，因此主要还是增加一些启停的hook，配置日志输出，用意看注释吧，接着打印了一些基本参数后则进入init方法，在Command#execute(terminal, options)方法里会调用Bootstrap.init(!daemonize, pidFile, quiet, initialEnv); 12345678910111213141516171819202122public final int main(String[] args, Terminal terminal) throws Exception &#123; if (addShutdownHook()) &#123; shutdownHookThread.set(new Thread(() -&gt; &#123; try &#123; this.close(); &#125; catch (final IOException e) &#123; try ( StringWriter sw = new StringWriter(); PrintWriter pw = new PrintWriter(sw)) &#123; e.printStackTrace(pw); terminal.println(sw.toString()); &#125; catch (final IOException impossible) &#123; // StringWriter#close declares a checked IOException from the Closeable interface but the Javadocs for StringWriter // say that an exception here is impossible throw new AssertionError(impossible); &#125; &#125; &#125;)); // 当JVM关闭时，会执行系统中已经设置的所有通过方法addShutdownHook添加的钩子， // 当系统执行完这些钩子后，jvm才会关闭 Runtime.getRuntime().addShutdownHook(shutdownHookThread.get()); &#125; 配置日志输出Command#main()方法中 12345// 配置日志输出// initialize default for es.logger.level because we will not read the log4j2.propertiesfinal String loggerLevel = System.getProperty(&quot;es.logger.level&quot;, Level.INFO.name());final Settings settings = Settings.builder().put(&quot;logger.level&quot;, loggerLevel).build();LogConfigurator.configureWithoutConfig(settings); LogConfigurator#configureWithoutConfig()方法 123456public static void configureWithoutConfig(final Settings settings) &#123; Objects.requireNonNull(settings); // we initialize the status logger immediately otherwise Log4j will complain when we try to get the context configureStatusLogger(); configureLoggerLevels(settings); &#125; 在Command#mainWithoutErrorHandling(args, terminal)中执行Command，同时会抛出所有的异常给Command#main()方法，真正调用execute(terminal, options)方法执行操作，这是一个抽象方法，通过我们的类图,它的实现类应该是EnvironmentAwareCommand#execute() 123456789101112131415161718192021222324252627@Override protected void execute(Terminal terminal, OptionSet options) throws Exception &#123; // 将配置信息设置到HashMap中 final Map&lt;String, String&gt; settings = new HashMap&lt;&gt;(); for (final KeyValuePair kvp : settingOption.values(options)) &#123; if (kvp.value.isEmpty()) &#123; throw new UserException(ExitCodes.USAGE, \"setting [\" + kvp.key + \"] must not be empty\"); &#125; if (settings.containsKey(kvp.key)) &#123; final String message = String.format( Locale.ROOT, \"setting [%s] already set, saw [%s] and [%s]\", kvp.key, settings.get(kvp.key), kvp.value); throw new UserException(ExitCodes.USAGE, message); &#125; settings.put(kvp.key, kvp.value); &#125; // 检查了elasticsearch的三个环境参数： putSystemPropertyIfSettingIsMissing(settings, \"path.conf\", \"es.path.conf\"); putSystemPropertyIfSettingIsMissing(settings, \"path.data\", \"es.path.data\"); putSystemPropertyIfSettingIsMissing(settings, \"path.home\", \"es.path.home\"); putSystemPropertyIfSettingIsMissing(settings, \"path.logs\", \"es.path.logs\"); // 调用execute方法 execute(terminal, options, createEnv(terminal, settings)); &#125; 该方法也是一个抽象方法，它有很多实现类 在该方法中，会先调用createEnv(terminal, settings)设置环境参数，使用该方法来加载配置文件信息 1234/** Create an &#123;@link Environment&#125; for the command to use. Overrideable for tests. */ protected Environment createEnv(Terminal terminal, Map&lt;String, String&gt; settings) &#123; return InternalSettingsPreparer.prepareEnvironment(Settings.EMPTY, terminal, settings); &#125; 那么这些配置信息怎么跟节点信息关联呢？ 3.3 Elasticsearch#execute()方法直接来看Elasticsearch#execute()方法做了什么？ 12345678910111213141516171819202122232425262728protected void execute(Terminal terminal, OptionSet options, Environment env) throws UserException &#123; // 检查参数是否为空 if (options.nonOptionArguments().isEmpty() == false) &#123; throw new UserException(ExitCodes.USAGE, \"Positional arguments not allowed, found \" + options.nonOptionArguments()); &#125; if (options.has(versionOption)) &#123; if (options.has(daemonizeOption) || options.has(pidfileOption)) &#123; throw new UserException(ExitCodes.USAGE, \"Elasticsearch version option is mutually exclusive with any other option\"); &#125; terminal.println(\"Version: \" + org.elasticsearch.Version.CURRENT + \", Build: \" + Build.CURRENT.shortHash() + \"/\" + Build.CURRENT.date() + \", JVM: \" + JvmInfo.jvmInfo().version()); return; &#125; // 是否以守护线程启动（后台启动 -d） final boolean daemonize = options.has(daemonizeOption); // 进程文件 final Path pidFile = pidfileOption.value(options); // final boolean quiet = options.has(quietOption); try &#123; // 执行初始化方法 init(daemonize, pidFile, quiet, env); &#125; catch (NodeValidationException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125; &#125; 该方法主要是检查一些参数，然后调用Elasticsearch#init(daemonize, pidFile, quiet, env)方法，在方法里会调用Bootstrap.init(!daemonize, pidFile, quiet, initialEnv)，而这个方法才是Elasticsearch真正去启动ES。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * This method is invoked by &#123;@link Elasticsearch#main(String[])&#125; to startup elasticsearch. */ static void init( final boolean foreground, final Path pidFile, final boolean quiet, final Environment initialEnv) throws BootstrapException, NodeValidationException, UserException &#123; // Set the system property before anything has a chance to trigger its use initLoggerPrefix(); // force the class initializer for BootstrapInfo to run before // the security manager is installed BootstrapInfo.init(); INSTANCE = new Bootstrap(); final SecureSettings keystore = loadSecureSettings(initialEnv); Environment environment = createEnvironment(foreground, pidFile, keystore, initialEnv.settings()); try &#123; // 配置日志输出 LogConfigurator.configure(environment); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; // 检查自定义配置文件 checkForCustomConfFile(); // 检查是否配置错误 checkConfigExtension(environment.configExtension()); // 如果pidFile文件不为空，则创建pid文件，会在磁盘上持久化一个记录应用pid的文件 if (environment.pidFile() != null) &#123; try &#123; PidFile.create(environment.pidFile(), true); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; &#125; //通过参数foreground和quiet来控制日志输出 final boolean closeStandardStreams = (foreground == false) || quiet; try &#123; if (closeStandardStreams) &#123; final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; closeSystOut(); &#125; // fail if somebody replaced the lucene jars checkLucene(); // install the default uncaught exception handler; must be done before security is // initialized as we do not want to grant the runtime permission // setDefaultUncaughtExceptionHandler // 初始化节点信息 Thread.setDefaultUncaughtExceptionHandler( new ElasticsearchUncaughtExceptionHandler(() -&gt; Node.NODE_NAME_SETTING.get(environment.settings()))); // 调用Bootstrap的setup方法和start方法 INSTANCE.setup(true, environment); try &#123; // any secure settings must be read during node construction IOUtils.close(keystore); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; // 调用Bootstrap的start方法 INSTANCE.start(); if (closeStandardStreams) &#123; closeSysError(); &#125; ... 略 参数详解 foreground：标识elasticsearch是否是作为后台守护进程启动的， pidFile：通过parser解析args后得到，实际是解析了默认命令行参数（verbose，E,silent，version，help，quiet，daemonize，pidfile） quiet：同上 initialEnv：Environment实例化的环境参数对象，保存了一些类似于repoFile，configFile，pluginsFile，binFile，libFile等参数。 通过上述的源码阅读，我们发现在该方法中： 主要工作 首先会实例化一个Bootstrap对象 配置log输出器 创建pid文件，会在磁盘上持久化一个记录应用pid的文件 通过参数foreground和quiet来控制日志输出 调用Bootstrap的setup方法和start方法 3.5 Bootstrap#setup()方法1setup(boolean addShutdownHook, Environment environment)throws BootstrapException 该方法主要工作 通过environment生成本地插件控制器 12345678Settings settings = environment.settings(); try &#123; // Spawner类是一个Environment本地插件控制器 spawner.spawnNativePluginControllers(environment); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; 初始化本地资源 12345initializeNatives( environment.tmpFile(), BootstrapSettings.MEMORY_LOCK_SETTING.get(settings), BootstrapSettings.SYSTEM_CALL_FILTER_SETTING.get(settings), BootstrapSettings.CTRLHANDLER_SETTING.get(settings)); ​ 在安全管理器安装之前初始化探针 1initializeProbes(); ​ 添加关闭钩子 1234567891011121314if (addShutdownHook) &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; try &#123; IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to stop node\", ex); &#125; &#125; &#125;); &#125; ​ 检查jar重复 123456try &#123; // look for jar hell,检查jar重复 JarHell.checkJarHell(); &#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e); &#125; ​ 在安全管理器安装之前配置日志输出器 1234567// install SM after natives, shutdown hooks, etc. // 安装安全管理器 try &#123; Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); &#125; catch (IOException | NoSuchAlgorithmException e) &#123; throw new BootstrapException(e); &#125; ​ 安装安全管理器 1234567// install SM after natives, shutdown hooks, etc. // 安装安全管理器 try &#123; Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); &#125; catch (IOException | NoSuchAlgorithmException e) &#123; throw new BootstrapException(e); &#125; ​ 通过参数environment实例化Node 123456789// 通过参数environment实例化Node node = new Node(environment) &#123; @Override protected void validateNodeBeforeAcceptingRequests( final Settings settings, final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException &#123; BootstrapChecks.check(settings, boundTransportAddress, checks); &#125; &#125;; ​ 3.6 Bootstrap#start()方法1234private void start() throws NodeValidationException &#123; node.start(); keepAliveThread.start(); &#125; 主要工作 启动已经实例化的Node 启动keepAliveThread 线程，这个线程在Bootstrap初始化的时候就已经实例化了，该线程创建了一个计数为1的CountDownLatch，目的是在启动完成后能顺利添加关闭钩子，而这句： 1Runtime.getRuntime().addShutdownHook(new Thread()) 意思就是在jvm中增加一个关闭的钩子，当jvm关闭的时候，会执行系统中已经设置的所有通过方法addShutdownHook添加的钩子，当系统执行完这些钩子后，jvm才会关闭。所以这些钩子可以在jvm关闭的时候进行内存清理、对象销毁等操作。可以看到启动的重点在setup方法中，启动过后就是Node的事了。 keepAliveThhread线程 123456789101112131415161718192021222324private final CountDownLatch keepAliveLatch = new CountDownLatch(1);/** creates a new instance */ Bootstrap() &#123; // 在构造器中就创建keepAliveThread线程 keepAliveThread = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; keepAliveLatch.await(); &#125; catch (InterruptedException e) &#123; // bail out &#125; &#125; &#125;, \"elasticsearch[keepAlive/\" + Version.CURRENT + \"]\"); keepAliveThread.setDaemon(false); // keep this thread alive (non daemon thread) until we shutdown Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; // 这里的钩子执行完毕，才会执行完keepAliveThread线程的run()方法 keepAliveLatch.countDown(); &#125; &#125;); &#125; 3.4 Node类源码解读我们先不看源码，如果是你，会怎么去设计这个Node类？会怎么去加载配置文件信息？ 猜想，我们启动ES都是一个节点Node，如果是集群，会有多个Node，那么我们应该也是通过Node来加载配置文件，加载完配置文件构造一个Config对象，最后初始化一个Node对象。 继续猜想，Node应该是包含一些基本信息、全局环境配置Setting和Environment，节点环境NodeEnvironment、是否为master、是否可以参与投票等。 问题：这些信息设置完毕，如何启动、如何停止？如何加载插件？ 验证猜想，查看类的定义信息 3.4.1 Node初始化我们前面通过分析Bootstrap#setup()方法知道，Node的实例化是在该方法中调用 new Node(environment)进行的，节点的启动是在Bootstrap#start()方法中调用Node#start()方法进行启动的。 123456789// 通过参数environment实例化Node node = new Node(environment) &#123; @Override protected void validateNodeBeforeAcceptingRequests( final Settings settings, final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException &#123; BootstrapChecks.check(settings, boundTransportAddress, checks); &#125; &#125;; 使用google的注入框架Guice的Injector进行注入与获取实例。elasticsearch里面的组件都是用上面的方法进行模块化管理，elasticsearch对guice进行了封装，通过ModulesBuilder类构建elasticsearch的模块： 123456789101112131415161718ModulesBuilder modules = new ModulesBuilder(); // plugin modules must be added here, before others or we can get crazy injection errors... for (Module pluginModule : pluginsService.createGuiceModules()) &#123; modules.add(pluginModule); &#125; final MonitorService monitorService = new MonitorService(settings, nodeEnvironment, threadPool); modules.add(new NodeModule(this, monitorService)); ClusterModule clusterModule = new ClusterModule(settings, clusterService, pluginsService.filterPlugins(ClusterPlugin.class)); modules.add(clusterModule); IndicesModule indicesModule = new IndicesModule(pluginsService.filterPlugins(MapperPlugin.class)); modules.add(indicesModule); SearchModule searchModule = new SearchModule(settings, false, pluginsService.filterPlugins(SearchPlugin.class)); CircuitBreakerService circuitBreakerService = createCircuitBreakerService(settingsModule.getSettings(), settingsModule.getClusterSettings()); resourcesToClose.add(circuitBreakerService);... Node的实例化主要工作： 设置初始化信息：nodeEnvironment 123456789101112131415161718192021222324252627try &#123; Settings tmpSettings = Settings.builder().put(environment.settings()) .put(Client.CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE).build(); tmpSettings = TribeService.processSettings(tmpSettings); // create the node environment as soon as possible, to recover the node id and enable logging try &#123; nodeEnvironment = new NodeEnvironment(tmpSettings, environment); resourcesToClose.add(nodeEnvironment); &#125; catch (IOException ex) &#123; throw new IllegalStateException(\"Failed to create node environment\", ex); &#125; final boolean hadPredefinedNodeName = NODE_NAME_SETTING.exists(tmpSettings); Logger logger = Loggers.getLogger(Node.class, tmpSettings); final String nodeId = nodeEnvironment.nodeId(); tmpSettings = addNodeNameIfNeeded(tmpSettings, nodeId); if (DiscoveryNode.nodeRequiresLocalStorage(tmpSettings)) &#123; checkForIndexDataInDefaultPathData(tmpSettings, nodeEnvironment, logger); &#125; // this must be captured after the node name is possibly added to the settings final String nodeName = NODE_NAME_SETTING.get(tmpSettings); if (hadPredefinedNodeName == false) &#123; logger.info(\"node name [&#123;&#125;] derived from node ID [&#123;&#125;]; set [&#123;&#125;] to override\", nodeName, nodeId, NODE_NAME_SETTING.getKey()); &#125; else &#123; logger.info(\"node name [&#123;&#125;], node ID [&#123;&#125;]\", nodeName, nodeId); &#125; 打印JVM信息 ​ 初始化pluginsService类 1this.pluginsService = new PluginsService(tmpSettings, environment.modulesFile(), environment.pluginsFile(), classpathPlugins); environment(这里会加载配置文件) 12this.environment = new Environment(this.settings);Environment.assertEquivalent(environment, this.environment); Executors 和threadPool 1234567final List&lt;ExecutorBuilder&lt;?&gt;&gt; executorBuilders = pluginsService.getExecutorBuilders(settings);final ThreadPool threadPool = new ThreadPool(settings, executorBuilders.toArray(new ExecutorBuilder[0]));resourcesToClose.add(() -&gt; ThreadPool.terminate(threadPool, 10, TimeUnit.SECONDS));// adds the context to the DeprecationLogger so that it does not need to be injected everywhereDeprecationLogger.setThreadContext(threadPool.getThreadContext());resourcesToClose.add(() -&gt; DeprecationLogger.removeThreadContext(threadPool.getThreadContext())); 我们来看es线程池做了什么？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public ThreadPool(final Settings settings, final ExecutorBuilder&lt;?&gt;... customBuilders) &#123; super(settings); assert Node.NODE_NAME_SETTING.exists(settings); // 将构造好的线程池添加到HashMap中，key是线程池的名称，value是ExecutorBuilder // 每一个线程都是通过ExecutorBuilder来构造 final Map&lt;String, ExecutorBuilder&gt; builders = new HashMap&lt;&gt;(); final int availableProcessors = EsExecutors.boundedNumberOfProcessors(settings); final int halfProcMaxAt5 = halfNumberOfProcessorsMaxFive(availableProcessors); final int halfProcMaxAt10 = halfNumberOfProcessorsMaxTen(availableProcessors); final int genericThreadPoolMax = boundedBy(4 * availableProcessors, 128, 512); builders.put(Names.GENERIC, new ScalingExecutorBuilder(Names.GENERIC, 4, genericThreadPoolMax, TimeValue.timeValueSeconds(30))); builders.put(Names.INDEX, new FixedExecutorBuilder(settings, Names.INDEX, availableProcessors, 200)); builders.put(Names.BULK, new FixedExecutorBuilder(settings, Names.BULK, availableProcessors, 200)); // now that we reuse bulk for index/delete ops builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, availableProcessors, 1000)); builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(availableProcessors), 1000)); builders.put(Names.MANAGEMENT, new ScalingExecutorBuilder(Names.MANAGEMENT, 1, 5, TimeValue.timeValueMinutes(5))); // no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded // the assumption here is that the listeners should be very lightweight on the listeners side builders.put(Names.LISTENER, new FixedExecutorBuilder(settings, Names.LISTENER, halfProcMaxAt10, -1)); builders.put(Names.FLUSH, new ScalingExecutorBuilder(Names.FLUSH, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5))); builders.put(Names.REFRESH, new ScalingExecutorBuilder(Names.REFRESH, 1, halfProcMaxAt10, TimeValue.timeValueMinutes(5))); builders.put(Names.WARMER, new ScalingExecutorBuilder(Names.WARMER, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5))); builders.put(Names.SNAPSHOT, new ScalingExecutorBuilder(Names.SNAPSHOT, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5))); builders.put(Names.FETCH_SHARD_STARTED, new ScalingExecutorBuilder(Names.FETCH_SHARD_STARTED, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5))); builders.put(Names.FORCE_MERGE, new FixedExecutorBuilder(settings, Names.FORCE_MERGE, 1, -1)); builders.put(Names.FETCH_SHARD_STORE, new ScalingExecutorBuilder(Names.FETCH_SHARD_STORE, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5))); for (final ExecutorBuilder&lt;?&gt; builder : customBuilders) &#123; if (builders.containsKey(builder.name())) &#123; throw new IllegalArgumentException(\"builder with name [\" + builder.name() + \"] already exists\"); &#125; builders.put(builder.name(), builder); &#125; this.builders = Collections.unmodifiableMap(builders); threadContext = new ThreadContext(settings); final Map&lt;String, ExecutorHolder&gt; executors = new HashMap&lt;&gt;(); for (@SuppressWarnings(\"unchecked\") final Map.Entry&lt;String, ExecutorBuilder&gt; entry : builders.entrySet()) &#123; final ExecutorBuilder.ExecutorSettings executorSettings = entry.getValue().getSettings(settings); final ExecutorHolder executorHolder = entry.getValue().build(executorSettings, threadContext); if (executors.containsKey(executorHolder.info.getName())) &#123; throw new IllegalStateException(\"duplicate executors with name [\" + executorHolder.info.getName() + \"] registered\"); &#125; logger.debug(\"created thread pool: &#123;&#125;\", entry.getValue().formatInfo(executorHolder.info)); executors.put(entry.getKey(), executorHolder); &#125; executors.put(Names.SAME, new ExecutorHolder(DIRECT_EXECUTOR, new Info(Names.SAME, ThreadPoolType.DIRECT))); this.executors = unmodifiableMap(executors); // 最后创建一个1线程的scheduler来执行定时任务 this.scheduler = new ScheduledThreadPoolExecutor(1, EsExecutors.daemonThreadFactory(settings, \"scheduler\"), new EsAbortPolicy()); this.scheduler.setExecuteExistingDelayedTasksAfterShutdownPolicy(false); this.scheduler.setContinueExistingPeriodicTasksAfterShutdownPolicy(false); this.scheduler.setRemoveOnCancelPolicy(true); TimeValue estimatedTimeInterval = ESTIMATED_TIME_INTERVAL_SETTING.get(settings); // 最后创建一个执行timer的线程 this.cachedTimeThread = new CachedTimeThread(EsExecutors.threadName(settings, \"[timer]\"), estimatedTimeInterval.millis()); this.cachedTimeThread.start(); &#125; 原来在ES的threadPool中，根据不同的类型分别分配了不同线程数的一个线程池，而executor由一个executorBuilder来提供，所以submit task的时候也需要指定不同的Name。最后创建一个1线程的scheduler来执行定时任务。最后创建一个执行timer的线程。 再继续往下看Node的构造方法就会看到接下来会new 一堆的services和modules，这里就不一一过了，其共性就是都会绑定刚刚创建的threadPool，已经也会绑定必要的services，某些module本身具有后台线程的话，初始化完成需要调用.start()去启动这些后台线程。 初始化modules实例，通过Guice的Injector进行注入各个Module实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748ModulesBuilder modules = new ModulesBuilder();***modules.add(b -&gt; &#123; b.bind(NodeService.class).toInstance(nodeService); b.bind(NamedXContentRegistry.class).toInstance(xContentRegistry); b.bind(PluginsService.class).toInstance(pluginsService); b.bind(Client.class).toInstance(client); b.bind(NodeClient.class).toInstance(client); b.bind(Environment.class).toInstance(this.environment); b.bind(ThreadPool.class).toInstance(threadPool); b.bind(NodeEnvironment.class).toInstance(nodeEnvironment); b.bind(TribeService.class).toInstance(tribeService); b.bind(ResourceWatcherService.class).toInstance(resourceWatcherService); b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService); b.bind(BigArrays.class).toInstance(bigArrays); b.bind(ScriptService.class).toInstance(scriptModule.getScriptService()); b.bind(AnalysisRegistry.class).toInstance(analysisModule.getAnalysisRegistry()); b.bind(IngestService.class).toInstance(ingestService); b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry); b.bind(MetaDataUpgrader.class).toInstance(metaDataUpgrader); b.bind(MetaStateService.class).toInstance(metaStateService); b.bind(IndicesService.class).toInstance(indicesService); b.bind(SearchService.class).toInstance(newSearchService(clusterService, indicesService, threadPool, scriptModule.getScriptService(), bigArrays, searchModule.getFetchPhase())); b.bind(SearchTransportService.class).toInstance(searchTransportService); b.bind(SearchPhaseController.class).toInstance(new SearchPhaseController(settings, bigArrays, scriptModule.getScriptService())); b.bind(Transport.class).toInstance(transport); b.bind(TransportService.class).toInstance(transportService); b.bind(NetworkService.class).toInstance(networkService); b.bind(UpdateHelper.class).toInstance(new UpdateHelper(settings, scriptModule.getScriptService())); b.bind(MetaDataIndexUpgradeService.class).toInstance(new MetaDataIndexUpgradeService(settings, xContentRegistry, indicesModule.getMapperRegistry(), settingsModule.getIndexScopedSettings(), indexMetaDataUpgraders)); b.bind(ClusterInfoService.class).toInstance(clusterInfoService); b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery()); &#123; RecoverySettings recoverySettings = new RecoverySettings(settings, settingsModule.getClusterSettings()); processRecoverySettings(settingsModule.getClusterSettings(), recoverySettings); b.bind(PeerRecoverySourceService.class).toInstance(new PeerRecoverySourceService(settings, transportService, indicesService, recoverySettings, clusterService)); b.bind(PeerRecoveryTargetService.class).toInstance(new PeerRecoveryTargetService(settings, threadPool, transportService, recoverySettings, clusterService)); &#125; httpBind.accept(b); pluginComponents.stream().forEach(p -&gt; b.bind((Class) p.getClass()).toInstance(p)); &#125; ); injector = modules.createInjector(); 这里面会注入Discovery，ClusterService，Transport Service，还创建了NodeClient用来接收全部其他节点请求。这些都会在往后重点剖析。 3.4.2 启动Node通过在Bootstrap#start()方法中调用Node.start()来启动节点 我们知道，在Node的初始化方法中，Model组件会被添加到绑定的线程当中，那么启动这些只需要调用相应组件的.start()方法即可完成组件的加载 1234567891011121314151617181920public Node start() throws NodeValidationException &#123; if (!lifecycle.moveToStarted()) &#123; return this; &#125; Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings)); logger.info(\"starting ...\"); // hack around dependency injection problem (for now...) injector.getInstance(Discovery.class).setAllocationService(injector.getInstance(AllocationService.class)); pluginLifecycleComponents.forEach(LifecycleComponent::start); injector.getInstance(MappingUpdatedAction.class).setClient(client); injector.getInstance(IndicesService.class).start(); injector.getInstance(IndicesClusterStateService.class).start(); injector.getInstance(IndicesTTLService.class).start(); injector.getInstance(SnapshotsService.class).start(); injector.getInstance(SnapshotShardsService.class).start(); injector.getInstance(RoutingService.class).start(); injector.getInstance(SearchService.class).start(); injector.getInstance(MonitorService.class).start(); 3.4.3 Node节点停止 该方法跟node启动差不多，也是调用相关组件的stop方法即可，这里就不再分析了 3.4.4 加载配置文件信息 入口 通过Node的构造方法 123public Node(Settings preparedSettings) &#123; this(InternalSettingsPreparer.prepareEnvironment(preparedSettings, null));&#125; 这就是加载配置文件的入口，它有三个方法 12345678910public static Settings prepareSettings(Settings input) &#123; Settings.Builder output = Settings.builder(); initializeSettings(output, input, Collections.emptyMap()); finalizeSettings(output, null); return output.build(); &#125; public static Environment prepareEnvironment(Settings input, Terminal terminal) &#123; return prepareEnvironment(input, terminal, Collections.emptyMap()); &#125;public static Environment prepareEnvironment(Settings input, Terminal terminal, Map&lt;String, String&gt; properties) &#123;&#125; 在InternalSettingsPreparer类的prepareEnvironment(org.elasticsearch.common.settings.Settings, org.elasticsearch.cli.Terminal, java.util.Map&lt;java.lang.String,java.lang.String&gt;, java.nio.file.Path)方法中进行了配置文件的加载。 加载配置文件的方法 1234567891011121314151617181920212223242526272829303132333435363738394041public static Environment prepareEnvironment(Settings input, Terminal terminal, Map&lt;String, String&gt; properties) &#123; // just create enough settings to build the environment, to get the config dir Settings.Builder output = Settings.builder(); // 初始化输入输出流信息 initializeSettings(output, input, properties); // 构造Environment实例 Environment environment = new Environment(output.build()); // 这个很关键，保证elasticsearch.yml文件中配置的日志路径path.logs生效 output = Settings.builder(); // start with a fresh output boolean settingsFileFound = false; Set&lt;String&gt; foundSuffixes = new HashSet&lt;&gt;(); for (String allowedSuffix : ALLOWED_SUFFIXES) &#123; Path path = environment.configFile().resolve(\"elasticsearch\" + allowedSuffix); if (Files.exists(path)) &#123; if (!settingsFileFound) &#123; try &#123; output.loadFromPath(path); &#125; catch (IOException e) &#123; throw new SettingsException(\"Failed to load settings from \" + path.toString(), e); &#125; &#125; settingsFileFound = true; foundSuffixes.add(allowedSuffix); &#125; &#125; if (foundSuffixes.size() &gt; 1) &#123; throw new SettingsException(\"multiple settings files found with suffixes: \" + Strings.collectionToDelimitedString(foundSuffixes, \",\")); &#125; // re-initialize settings now that the config file has been loaded initializeSettings(output, input, properties); finalizeSettings(output, terminal); // 再次获取Environment实例 environment = new Environment(output.build()); // we put back the path.logs so we can use it in the logging configuration file output.put(Environment.PATH_LOGS_SETTING.getKey(), cleanPath(environment.logsFile().toAbsolutePath().toString())); String configExtension = foundSuffixes.isEmpty() ? null : foundSuffixes.iterator().next(); // 返回Environment实例 return new Environment(output.build(), configExtension); 构建一个默认的Settings的实例 然后用构造出来的新的Settings来加载给定或默认路径下的elasticsearch.yml 然后将方法接受的参数Settings实例也加载到这个新的Settings中。 最后才将日志文件的路径加载进Settings中，这样就保证了elasticsearch.yml文件中配置的日志路径path.logs生效（覆盖该方法参数中的配置）。 最后返回一个Environment的实例，使得Node开始构建 四、总结：通过上述的源码分析，我们知道Elasticsearch节点启动的入口是Elasticsearch#main()方法，在该方法中会进行一些安全管理的设置，去调用Command的main()方法，整个方法执行没有任何异常，则返回ok状态。 Command#main()：会去添加一些钩子、配置日志输出、调用mainWithoutErrorHandling()去执行EnvironmentAwareCommand#execute(terminal, options)方法。 EnvironmentAwareCommand#execute(terminal, options)方法：只是将配置信息设置到HashMap中，检查了elasticsearch的参数path.conf、path.data、path.home、path.logs，最后调用Elasticsearch#execute()方法，execute(terminal, options, createEnv(terminal, settings))会先调用EnvironmentAwareCommand# createEnv(terminal, settings) Elasticsearch#execute()方法：主要是处理参数，调用init(daemonize, pidFile, quiet, env)，真正执行启动的是Bootstrap.init(!daemonize, pidFile, quiet, initialEnv)方法。 Bootstrap.init(!daemonize, pidFile, quiet, initialEnv)：主要是调用setup()方法和start()方法，在setup()方法中主要通过environment生成本地插件控制器spawner、添加钩子、添加安全管理器、检查jar包、创建Node节点。而start()通过启动初始化好的Node和keepAliveThread线程，这个keepAliveThread使用了CountdownLatch计数器为1来保证钩子一定能够关闭。 Node类的初始化：通过设置好的environment来初始化节点，设置nodEnvironment、Environment、设置Node_name、设置线程池（其实是一个HashMap&lt;String,ExecutorBuilder&gt;） ，根据不同的类型分别分配了不同线程数的一个线程池。将创建好的module绑定到创建的ThreadPool。 大致的时序图如下： 现在还遗留着几个问题： master选举是在什么模块进行的 怎么进行master选举 怎么进行节点监控、维护的 留到下一篇再进行分析。。。","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]},{"title":"elasticsearch源码分析(一)--整体架构","slug":"elasticsearch源码分析(一)--整体架构","date":"2018-05-24T02:12:57.000Z","updated":"2018-06-25T00:07:21.049Z","comments":true,"path":"2018/05/24/elasticsearch源码分析(一)--整体架构/","link":"","permalink":"http://yoursite.com/2018/05/24/elasticsearch源码分析(一)--整体架构/","excerpt":"","text":"一、源码主要模块我下载的Elasticsearch的源码版本为5.6.4 从上图来看：Elasticsearch主要包含以下几个模块 distribution：elasticsearch的打包发行相关，将elasticsearch打成各种发行包（zip，deb，rpm，tar）的模块。具体用法如是，在相应的发行版本模块下执行publishToMavenLocal这个Task，如果执行成功的话就会在路径build/distributions下生成对应的发行包，这种打好的包就能在生产服务器上运行。如果自己修改了源码，打包时就需要用到该模块了。 core：核心包，elasticsearch的源码主要在这个里面，Elasticsearch索引管理、集群管理、服务发现、查询、对Lucene操作的封装等都位于该模块 buildSrc：elasticsearch的构建相关的代码，gradle相关依赖配置都在改模块下 client：作为连接elasticsearch的客户端相关代码，它提供了Rest方式（基于Http）、transport （Java Netty内部的通信方式）等方式。 modules：作为elasticsearch除核心外的必备模块相关代码,比如对Netty的封装、父子类查询、重建索引 plugins：作为elasticsearch必备的插件的相关代码，丰富ES的相关功能，比如IK分词器插件、mapper-attachments/ingest-attachment文件处理插件。 二、Elasticsearch整体架构图 服务发现以及选主 ZenDiscovery 恢复以及容灾 搜索引擎 Search ClusterState 网络层 Rest 和 RPC 线程池","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]},{"title":"Elasticsearch源码阅读环境搭建","slug":"elasticsearch源码环境搭建","date":"2018-05-23T04:12:57.000Z","updated":"2018-06-25T00:07:07.275Z","comments":true,"path":"2018/05/23/elasticsearch源码环境搭建/","link":"","permalink":"http://yoursite.com/2018/05/23/elasticsearch源码环境搭建/","excerpt":"","text":"","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]}]}