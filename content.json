{"meta":{"title":"Gre的博客","subtitle":null,"description":null,"author":"Zhanggdong","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2018-06-19T09:56:34.000Z","updated":"2018-06-20T00:10:20.888Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"html 本文链接：&lt;%= post.title %&gt; 作者：Zhanggdong 出处：https://Zhanggdong.github.io/本文基于 知识共享署名-相同方式共享 4.0 国际许可协议发布，欢迎转载，演绎或用于商业目的，但是必须保留本文的署名张贵东及链接。本站总访问量 次, 访客数 人次, 本文总阅读量 次"},{"title":"文章分类","date":"2018-06-20T00:17:21.000Z","updated":"2018-06-20T00:19:59.907Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"elasticsearch源码分析(四)--transport模块","slug":"Elasticsearch/elasticsearch源码分析(四)--transport模块","date":"2018-05-30T02:12:57.000Z","updated":"2018-06-27T10:07:31.125Z","comments":true,"path":"2018/05/30/Elasticsearch/elasticsearch源码分析(四)--transport模块/","link":"","permalink":"http://yoursite.com/2018/05/30/Elasticsearch/elasticsearch源码分析(四)--transport模块/","excerpt":"","text":"通过上一篇对Elasticsearch Discovery的分析，我们知道了ES Discovery的主要是用来做发现协议的，它包含了master选举、集群状态更新等功能，而在这一过程当中也依赖了Transport模块，但是我们一直没有谈论，本篇着重来分析Transport模块是做什么的，我们先来提出几个问题？ Transport模块是用来干什么的？ ES有哪两种Client？ Client如何初始化？ ES有哪两种Client是如何与Nodes通信工作的？ Transport的工作原理？ 一、Transport模块介绍transport模块是es通信的基础模块，在elasticsearch中用的很广泛，比如集群node之间的通信、数据的传输、transport client方式的数据发送等等,只要数和通信、数据传输相关的都离不开transport模块的作用。 先来看官网初始化Client例子：TransportClient 123456789101112TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"host1\"), 9300)) .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"host2\"), 9300));Settings settings = Settings.builder() .put(\"cluster.name\", \"myClusterName\").build();TransportClient client = new PreBuiltTransportClient(settings);Settings settings = Settings.builder() .put(\"client.transport.sniff\", true).build();TransportClient client = new PreBuiltTransportClient(settings); RestClient 123RestClient restClient = RestClient.builder( new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9201, \"http\")).build(); 从官网上的相关资料可以看出，ES的Client有两种，TransportClient和RestClient。 创建一个Client需要以下参数： 需要连接到集群的IP地址（最好多填写几个，如果填写一个，同时启用了嗅探，但是填写的这个IP主机挂了，还是不能启动成功） 连接的端口 是否启用嗅探（自动发现集群中的机器） 二、源码分析上面就是源码的入口：我们先来分析TransportClient类的初始化： ####2.1 TransportClient类的初始化 2.1.1 PreBuiltTransportClient类图关系 2.1.2 PreBuiltTransportClient源码分析通过从上面的类图看出，PreBuiltTransportClient类继承了TransportClient类，我们先来看看它最终调用的构造方法： 1234567public PreBuiltTransportClient( Settings settings, Collection&lt;Class&lt;? extends Plugin&gt;&gt; plugins, HostFailureListener hostFailureListener) &#123; // 调用父类的构造方法：TransportClient super(settings, Settings.EMPTY, addPlugins(plugins, PRE_INSTALLED_PLUGINS), hostFailureListener); &#125; 同时可以看到，这个类除了构造方法和close()方法外，没有对外提供额外的其他方法，调用时，都是去调用父类的构造方法，为什么要多用这么一个类呢？猜猜是为了扩展。 我们来看TransportClient的构造方法做了什么？ 2.1.3 TransportClient类分析：2.1.3.1 参数先来看这个类的几个参数： 123456789101112 public static final Setting&lt;TimeValue&gt; CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL = Setting.positiveTimeSetting(\"client.transport.nodes_sampler_interval\", timeValueSeconds(5), Setting.Property.NodeScope);// ping超时时间，默认5秒public static final Setting&lt;TimeValue&gt; CLIENT_TRANSPORT_PING_TIMEOUT = Setting.positiveTimeSetting(\"client.transport.ping_timeout\", timeValueSeconds(5), Setting.Property.NodeScope);// 是否忽略集群名称，默认不忽略public static final Setting&lt;Boolean&gt; CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME = Setting.boolSetting(\"client.transport.ignore_cluster_name\", false, Setting.Property.NodeScope);// 是否启用嗅探，默认不启用public static final Setting&lt;Boolean&gt; CLIENT_TRANSPORT_SNIFF = Setting.boolSetting(\"client.transport.sniff\", false, Setting.Property.NodeScope); 2.1.3.2 构造方法PreBuiltTransportClient调用TransportClient的构造方法： 1234567891011121314protected TransportClient(Settings settings, Settings defaultSettings, Collection&lt;Class&lt;? extends Plugin&gt;&gt; plugins,HostFailureListener hostFailureListener) &#123; // 先调用buildTemplate方法创建ClientTemplate模块实例，然后再调用TransportClient构造方法实例化对象 this(buildTemplate(settings, defaultSettings, plugins, hostFailureListener)); &#125; private TransportClient(ClientTemplate template) &#123; // 初始化父类信息 super(template.getSettings(), template.getThreadPool()); this.injector = template.injector; this.pluginLifecycleComponents = Collections.unmodifiableList(template.pluginLifecycleComponents); this.nodesService = template.nodesService; this.proxy = template.proxy; this.namedWriteableRegistry = template.namedWriteableRegistry; &#125; 我们可以看到，在TransportClient的构造方法里，先调用buildTemplate方法创建ClientTemplate模块实例，然后再调用TransportClient构造方法实例化对象，那么TransportClient初始化核心逻辑就应该在buildTemplate()方法里面： 我们先来看看buildTemplate()方法干了什么： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101private static ClientTemplate buildTemplate(Settings providedSettings, Settings defaultSettings, Collection&lt;Class&lt;? extends Plugin&gt;&gt; plugins, HostFailureListener failureListner) &#123; // node.name:没有配置，则配置\"_client_\" if (Node.NODE_NAME_SETTING.exists(providedSettings) == false) &#123; providedSettings = Settings.builder().put(providedSettings).put(Node.NODE_NAME_SETTING.getKey(), \"_client_\").build(); &#125; // 初始化插件 final PluginsService pluginsService = newPluginService(providedSettings, plugins); final Settings settings = Settings.builder().put(defaultSettings).put(pluginsService.updatedSettings()).build(); final List&lt;Closeable&gt; resourcesToClose = new ArrayList&lt;&gt;(); // 创建线程池：核心 final ThreadPool threadPool = new ThreadPool(settings); resourcesToClose.add(() -&gt; ThreadPool.terminate(threadPool, 10, TimeUnit.SECONDS)); // 初始化网络设置 final NetworkService networkService = new NetworkService(settings, Collections.emptyList()); try &#123; final List&lt;Setting&lt;?&gt;&gt; additionalSettings = new ArrayList&lt;&gt;(pluginsService.getPluginSettings()); final List&lt;String&gt; additionalSettingsFilter = new ArrayList&lt;&gt;(pluginsService.getPluginSettingsFilter()); for (final ExecutorBuilder&lt;?&gt; builder : threadPool.builders()) &#123; additionalSettings.addAll(builder.getRegisteredSettings()); &#125; SettingsModule settingsModule = new SettingsModule(settings, additionalSettings, additionalSettingsFilter); SearchModule searchModule = new SearchModule(settings, true, pluginsService.filterPlugins(SearchPlugin.class)); List&lt;NamedWriteableRegistry.Entry&gt; entries = new ArrayList&lt;&gt;(); entries.addAll(NetworkModule.getNamedWriteables()); entries.addAll(searchModule.getNamedWriteables()); entries.addAll(ClusterModule.getNamedWriteables()); entries.addAll(pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedWriteables().stream()) .collect(Collectors.toList())); NamedWriteableRegistry namedWriteableRegistry = new NamedWriteableRegistry(entries); NamedXContentRegistry xContentRegistry = new NamedXContentRegistry(Stream.of( searchModule.getNamedXContents().stream(), pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedXContent().stream()) ).flatMap(Function.identity()).collect(toList())); // 模块构建 ModulesBuilder modules = new ModulesBuilder(); // plugin modules must be added here, before others or we can get crazy injection errors... for (Module pluginModule : pluginsService.createGuiceModules()) &#123; modules.add(pluginModule); &#125; modules.add(b -&gt; b.bind(ThreadPool.class).toInstance(threadPool)); ActionModule actionModule = new ActionModule(true, settings, null, settingsModule.getIndexScopedSettings(), settingsModule.getClusterSettings(), settingsModule.getSettingsFilter(), threadPool, pluginsService.filterPlugins(ActionPlugin.class), null, null); modules.add(actionModule); CircuitBreakerService circuitBreakerService = Node.createCircuitBreakerService(settingsModule.getSettings(), settingsModule.getClusterSettings()); resourcesToClose.add(circuitBreakerService); BigArrays bigArrays = new BigArrays(settings, circuitBreakerService); resourcesToClose.add(bigArrays); modules.add(settingsModule); // 初始化网络模块 NetworkModule networkModule = new NetworkModule(settings, true, pluginsService.filterPlugins(NetworkPlugin.class), threadPool, bigArrays, circuitBreakerService, namedWriteableRegistry, xContentRegistry, networkService, null); final Transport transport = networkModule.getTransportSupplier().get(); final TransportAddress address; try &#123; address = transport.addressesFromString(\"0.0.0.0:0\", 1)[0]; // this is just a dummy transport address &#125; catch (UnknownHostException e) &#123; throw new RuntimeException(e); &#125; // 初始化TransportService final TransportService transportService = new TransportService(settings, transport, threadPool, networkModule.getTransportInterceptor(), boundTransportAddress -&gt; DiscoveryNode.createLocal(settings, address, UUIDs.randomBase64UUID()), null); // 使用GUICE注入到 ModuleBuild modules.add((b -&gt; &#123; b.bind(BigArrays.class).toInstance(bigArrays); b.bind(PluginsService.class).toInstance(pluginsService); b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService); b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry); b.bind(Transport.class).toInstance(transport); b.bind(TransportService.class).toInstance(transportService); b.bind(NetworkService.class).toInstance(networkService); &#125;)); Injector injector = modules.createInjector(); final TransportClientNodesService nodesService = new TransportClientNodesService(settings, transportService, threadPool, failureListner == null ? (t, e) -&gt; &#123;&#125; : failureListner); final TransportProxyClient proxy = new TransportProxyClient(settings, transportService, nodesService, actionModule.getActions().values().stream().map(x -&gt; x.getAction()).collect(Collectors.toList())); List&lt;LifecycleComponent&gt; pluginLifecycleComponents = new ArrayList&lt;&gt;(pluginsService.getGuiceServiceClasses().stream() .map(injector::getInstance).collect(Collectors.toList())); resourcesToClose.addAll(pluginLifecycleComponents); // 开始 transportService.start(); transportService.acceptIncomingRequests(); // 创建ClientTemplate实例 ClientTemplate transportClient = new ClientTemplate(injector, pluginLifecycleComponents, nodesService, proxy, namedWriteableRegistry); resourcesToClose.clear(); // 返回ClientTemplate实例 return transportClient; &#125; finally &#123; IOUtils.closeWhileHandlingException(resourcesToClose); &#125; &#125; 通过上面的源码可以知道，构造函数的初始化和Node的初始化过程是非常相似的，Client的初始化只是不会初始化那么多模块信息，但是常规的ThreadPool、PluginsService、NetworkService、NetworkModule、transportService等，最终会创建一个ClientTemplate对象，然后返回，我们不会分析每一个模块的初始化过程，这里只会分析ThreadPool和transportService的过程，在后面会介绍。 2.1.3.3 ClientTemplate 类分析我们来看看ClientTemplate 类： 12345678910111213141516171819202122232425262728private static final class ClientTemplate &#123; // 依赖注入的对象,负责管理各种注入的Bean final Injector injector; // 插件管理：启动、停止等 private final List&lt;LifecycleComponent&gt; pluginLifecycleComponents; private final TransportClientNodesService nodesService; // Transport代理 private final TransportProxyClient proxy; // 暂时不清楚 private final NamedWriteableRegistry namedWriteableRegistry; private ClientTemplate(Injector injector, List&lt;LifecycleComponent&gt; pluginLifecycleComponents, TransportClientNodesService nodesService, TransportProxyClient proxy, NamedWriteableRegistry namedWriteableRegistry) &#123; this.injector = injector; this.pluginLifecycleComponents = pluginLifecycleComponents; this.nodesService = nodesService; this.proxy = proxy; this.namedWriteableRegistry = namedWriteableRegistry; &#125; Settings getSettings() &#123; return injector.getInstance(Settings.class); &#125; ThreadPool getThreadPool() &#123; return injector.getInstance(ThreadPool.class); &#125; &#125; 这个类封装了依赖注入管理类injector、插件pluginLifecycleComponents、Transport代理proxy、namedWriteableRegistry、nodesService等5个成员变量。 2.1.3.4 nodesService初始化nodesService是TransportClientNodesService： 12345678910111213141516171819202122232425TransportClientNodesService(Settings settings, TransportService transportService, ThreadPool threadPool, TransportClient.HostFailureListener hostFailureListener) &#123; super(settings); this.clusterName = ClusterName.CLUSTER_NAME_SETTING.get(settings); this.transportService = transportService; this.threadPool = threadPool; this.minCompatibilityVersion = Version.CURRENT.minimumCompatibilityVersion(); // CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL参数默认5秒，在TransportClient中定义的 this.nodesSamplerInterval = TransportClient.CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL.get(this.settings); this.pingTimeout = TransportClient.CLIENT_TRANSPORT_PING_TIMEOUT.get(this.settings).millis(); this.ignoreClusterName = TransportClient.CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME.get(this.settings); if (logger.isDebugEnabled()) &#123; logger.debug(\"node_sampler_interval[&#123;&#125;]\", nodesSamplerInterval); &#125; // 这里判断是否启用了嗅探：这个Sniff参数就在这里使用 if (TransportClient.CLIENT_TRANSPORT_SNIFF.get(this.settings)) &#123; this.nodesSampler = new SniffNodesSampler(); &#125; else &#123; this.nodesSampler = new SimpleNodeSampler(); &#125; this.hostFailureListener = hostFailureListener; // 创建定时任务：通过GENERIC类型的线程池处理 this.nodesSamplerFuture = threadPool.schedule(nodesSamplerInterval, ThreadPool.Names.GENERIC, new ScheduledNodeSampler()); &#125; 这个类的构造方法很多参数都是从TransportClient中带过来的，比如是否启用嗅探参数Sniff，根据这个参数选择不同的策略SniffNodesSampler和SimpleNodeSampler。最后会有定时任务的时间间隔，线程池类型是GENERIC，默认是每隔5秒去ping其他节点。对于这两种策略：代码如下 SimpleNodeSampler类分析如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class SimpleNodeSampler extends NodeSampler &#123; @Override protected void doSample() &#123; HashSet&lt;DiscoveryNode&gt; newNodes = new HashSet&lt;&gt;(); HashSet&lt;DiscoveryNode&gt; newFilteredNodes = new HashSet&lt;&gt;(); // for (DiscoveryNode listedNode : listedNodes) &#123; // 通过Netty4获取connection try (Transport.Connection connection = transportService.openConnection(listedNode, LISTED_NODES_PROFILE))&#123; final PlainTransportFuture&lt;LivenessResponse&gt; handler = new PlainTransportFuture&lt;&gt;( new FutureTransportResponseHandler&lt;LivenessResponse&gt;() &#123; @Override public LivenessResponse newInstance() &#123; return new LivenessResponse(); &#125; &#125;); // 发送STATE的类型的Request：最终是调用了一个异步发送的方式 transportService.sendRequest(connection, TransportLivenessAction.NAME, new LivenessRequest(), TransportRequestOptions.builder().withType(TransportRequestOptions.Type.STATE).withTimeout(pingTimeout).build(), handler); final LivenessResponse livenessResponse = handler.txGet(); if (!ignoreClusterName &amp;&amp; !clusterName.equals(livenessResponse.getClusterName())) &#123; logger.warn(\"node &#123;&#125; not part of the cluster &#123;&#125;, ignoring...\", listedNode, clusterName); newFilteredNodes.add(listedNode); &#125; else &#123; // use discovered information but do keep the original transport address, // so people can control which address is exactly used. DiscoveryNode nodeWithInfo = livenessResponse.getDiscoveryNode(); newNodes.add(new DiscoveryNode(nodeWithInfo.getName(), nodeWithInfo.getId(), nodeWithInfo.getEphemeralId(), nodeWithInfo.getHostName(), nodeWithInfo.getHostAddress(), listedNode.getAddress(), nodeWithInfo.getAttributes(), nodeWithInfo.getRoles(), nodeWithInfo.getVersion())); &#125; &#125; catch (ConnectTransportException e) &#123; logger.debug( (Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"failed to connect to node [&#123;&#125;], ignoring...\", listedNode), e); hostFailureListener.onNodeDisconnected(listedNode, e); &#125; catch (Exception e) &#123; logger.info( (Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"failed to get node info for &#123;&#125;, disconnecting...\", listedNode), e); &#125; &#125; // 验证节点 nodes = validateNewNodes(newNodes); filteredNodes = Collections.unmodifiableList(new ArrayList&lt;&gt;(newFilteredNodes)); &#125; &#125;//该方法主要是为每一个节点建立的连接进行检验 protected List&lt;DiscoveryNode&gt; validateNewNodes(Set&lt;DiscoveryNode&gt; nodes) &#123; for (Iterator&lt;DiscoveryNode&gt; it = nodes.iterator(); it.hasNext(); ) &#123; DiscoveryNode node = it.next(); // 循环的检验与每个可用节点建立的连接是否已完成 if (!transportService.nodeConnected(node)) &#123; try &#123; logger.trace(\"connecting to node [&#123;&#125;]\", node); // 确定已经建立连接 transportService.connectToNode(node); &#125; catch (Exception e) &#123; it.remove(); logger.debug((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"failed to connect to discovered node [&#123;&#125;]\", node), e); &#125; &#125; &#125; // 返回列表 return Collections.unmodifiableList(new ArrayList&lt;&gt;(nodes)); &#125; 这个类的工作还是比较清晰的，就是获取配置的listedNodes然后去请求一个STATE类型的Request，这里的Connection是通过Netty4来拿到的，具体在后续会分析到，而线程是用GENERIC的线程池。把成功建立连接的所有的Nodes保存起来，而与每个Node也只保持1条连接。 我们再来看看SniffNodesSampler是什么情况： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133class SniffNodesSampler extends NodeSampler &#123; @Override protected void doSample() &#123; // the nodes we are going to ping include the core listed nodes that were added // and the last round of discovered nodes // 获取待ping的nodesToPing：从listedNodes和nodes获取 Set&lt;DiscoveryNode&gt; nodesToPing = new HashSet&lt;&gt;(); for (DiscoveryNode node : listedNodes) &#123; nodesToPing.add(node); &#125; for (DiscoveryNode node : nodes) &#123; nodesToPing.add(node); &#125; // 设置一个计数器：大小为待ping的Node的列表nodesToPing大小 final CountDownLatch latch = new CountDownLatch(nodesToPing.size()); // 将cluster响应状态存储到ConcurrentMap中：clusterStateResponses final ConcurrentMap&lt;DiscoveryNode, ClusterStateResponse&gt; clusterStateResponses = ConcurrentCollections.newConcurrentMap(); try &#123; // 首先也是先向所有的listedNode都ping一遍，注意这里用的是MANAGEMENT的threadPool for (final DiscoveryNode nodeToPing : nodesToPing) &#123; // 通过MANAGEMENT类型的线程处理: threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(new AbstractRunnable() &#123; /** * we try to reuse existing connections but if needed we will open a temporary connection * that will be closed at the end of the execution. */ Transport.Connection connectionToClose = null; // 关闭异常 void onDone() &#123; try &#123; IOUtils.closeWhileHandlingException(connectionToClose); &#125; finally &#123; latch.countDown(); &#125; &#125; // 如果失败：调用 onDone()方法，同时抛出异常信息 @Override public void onFailure(Exception e) &#123; onDone(); if (e instanceof ConnectTransportException) &#123; logger.debug((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"failed to connect to node [&#123;&#125;], ignoring...\", nodeToPing), e); hostFailureListener.onNodeDisconnected(nodeToPing, e); &#125; else &#123; logger.info( (Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage( \"failed to get local cluster state info for &#123;&#125;, disconnecting...\", nodeToPing), e); &#125; &#125; @Override protected void doRun() throws Exception &#123; Transport.Connection pingConnection = null; if (nodes.contains(nodeToPing)) &#123; try &#123; // 调用TransportService发起连接：获取ping的连接 pingConnection = transportService.getConnection(nodeToPing); &#125; catch (NodeNotConnectedException e) &#123; // will use a temp connection &#125; &#125; if (pingConnection == null) &#123; logger.trace(\"connecting to cluster node [&#123;&#125;]\", nodeToPing); // 调用TransportService发起连接：这里会建立了一堆连接 // 这是因为这个列表添加了两次 connectionToClose = transportService.openConnection(nodeToPing, LISTED_NODES_PROFILE); pingConnection = connectionToClose; &#125; transportService.sendRequest(pingConnection, ClusterStateAction.NAME, Requests.clusterStateRequest().clear().nodes(true).local(true), // 在pingTimeout时间内写入STATE TransportRequestOptions.builder().withType(TransportRequestOptions.Type.STATE) .withTimeout(pingTimeout).build(), new TransportResponseHandler&lt;ClusterStateResponse&gt;() &#123; @Override public ClusterStateResponse newInstance() &#123; return new ClusterStateResponse(); &#125; @Override public String executor() &#123; return ThreadPool.Names.SAME; &#125; @Override public void handleResponse(ClusterStateResponse response) &#123; // 将nodeToPing拿到的clusterState添加到clusterStateResponses当中 clusterStateResponses.put(nodeToPing, response); onDone(); &#125; @Override public void handleException(TransportException e) &#123; logger.info( (Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage( \"failed to get local cluster state for &#123;&#125;, disconnecting...\", nodeToPing), e); try &#123; hostFailureListener.onNodeDisconnected(nodeToPing, e); &#125; finally &#123; onDone(); &#125; &#125; &#125;); &#125; &#125;); &#125; latch.await(); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); return; &#125; HashSet&lt;DiscoveryNode&gt; newNodes = new HashSet&lt;&gt;(); HashSet&lt;DiscoveryNode&gt; newFilteredNodes = new HashSet&lt;&gt;(); for (Map.Entry&lt;DiscoveryNode, ClusterStateResponse&gt; entry : clusterStateResponses.entrySet()) &#123; if (!ignoreClusterName &amp;&amp; !clusterName.equals(entry.getValue().getClusterName())) &#123; logger.warn(\"node &#123;&#125; not part of the cluster &#123;&#125;, ignoring...\", entry.getValue().getState().nodes().getLocalNode(), clusterName); newFilteredNodes.add(entry.getKey()); continue; &#125; for (ObjectCursor&lt;DiscoveryNode&gt; cursor : entry.getValue().getState().nodes().getDataNodes().values()) &#123; newNodes.add(cursor.value); &#125; &#125; // 同样需要检验连接 nodes = validateNewNodes(newNodes); filteredNodes = Collections.unmodifiableList(new ArrayList&lt;&gt;(newFilteredNodes)); &#125; &#125; 首先也是先向所有的listedNode都ping一遍，注意这里用的是MANAGEMENT的threadPool，也是调用TransportService发起连接，这里要特别注意，这种方式下其实是建立了一堆连接connectionToClose = transportService.openConnection(nodeToPing, LISTED_NODES_PROFILE);如每个类型多少条连接这样，所以这种模式一个Client会和一个Node保持一堆连接。回调函数都很简单，成功和失败的都归类，同时拿到了每个送回来的cluster的state保存下来`clusterStateResponses.put(nodeToPing, response)。 从哪里看出来是一个节点对应一堆连接呢？请思考 最后汇总再确认一遍所有的nodes，校验完后维护，其实这里的nodes就是整个集群的所有的nodes了，剩下的就交给那个调度器去每间隔时间去ping了。 至此，我们已经建立好了连接了，以后如何有什么请求，就是Client向一个node去获取一个连接或者一个类型的连接池threadpool，然后就可以发起请求了。这个过程其实就是proxy的事情。 2.1.3.5 proxy初始化在ClientTemplate类初始化时也需要初始化proxy，它只保存两个变量nodesService和proxies，nodesService是上一步初始化好的建立了连接的对象，proxies是各种操作action的proxy的集合列表：比如index、update、search、bulk等。如果我们要去执行一个操作，最终会将proxy.execute()方法拿给nodesService.execute()执行， nodesService会随机选取一个建立好连接的Node来执行操作。 123456789101112131415161718192021222324252627final TransportProxyClient proxy = new TransportProxyClient(settings, transportService, nodesService,actionModule.getActions().values().stream().map(x -&gt; x.getAction()).collect(Collectors.toList()));final class TransportProxyClient &#123; // 保存nodesService private final TransportClientNodesService nodesService; // 各种操作action的proxy变量：比如index、update、search、bulk等 private final Map&lt;Action, TransportActionNodeProxy&gt; proxies; TransportProxyClient(Settings settings, TransportService transportService, TransportClientNodesService nodesService, List&lt;GenericAction&gt; actions) &#123; this.nodesService = nodesService; Map&lt;Action, TransportActionNodeProxy&gt; proxies = new HashMap&lt;&gt;(); // 循环的添加action的proxy到HashMap中 for (GenericAction action : actions) &#123; if (action instanceof Action) &#123; proxies.put((Action) action, new TransportActionNodeProxy(settings, action, transportService)); &#125; &#125; this.proxies = unmodifiableMap(proxies); &#125; // 这个代理其实调用nodesService的execute()方法 public &lt;Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder&lt;Request, Response, RequestBuilder&gt;&gt; void execute(final Action&lt;Request, Response, RequestBuilder&gt; action,final Request request, ActionListener&lt;Response&gt; listener) &#123; final TransportActionNodeProxy&lt;Request, Response&gt; proxy = proxies.get(action); nodesService.execute((n, l) -&gt; proxy.execute(n, request, l), listener); &#125;&#125; nodesService.execute()方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142public &lt;Response&gt; void execute(NodeListenerCallback&lt;Response&gt; callback, ActionListener&lt;Response&gt; listener) &#123; // we first read nodes before checking the closed state; this // is because otherwise we could be subject to a race where we // read the state as not being closed, and then the client is // closed and the nodes list is cleared, and then a // NoNodeAvailableException is thrown // it is important that the order of first setting the state of // closed and then clearing the list of nodes is maintained in // the close method final List&lt;DiscoveryNode&gt; nodes = this.nodes; if (closed) &#123; throw new IllegalStateException(\"transport client is closed\"); &#125; // 确保Nodes可用 ensureNodesAreAvailable(nodes); // 随机获取 int index = getNodeNumber(); RetryListener&lt;Response&gt; retryListener = new RetryListener&lt;&gt;(callback, listener, nodes, index, hostFailureListener); DiscoveryNode node = retryListener.getNode(0); try &#123; // 回调doWithNode callback.doWithNode(node, retryListener); &#125; catch (Exception e) &#123; try &#123; //this exception can't come from the TransportService as it doesn't throw exception at all listener.onFailure(e); &#125; finally &#123; retryListener.maybeNodeFailed(node, e); &#125; &#125; &#125;// 随机选取一个Node private int getNodeNumber() &#123; int index = randomNodeGenerator.incrementAndGet(); if (index &lt; 0) &#123; index = 0; randomNodeGenerator.set(0); &#125; return index; &#125; 2.2 RestClient初始化这个类初始化非常简单，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class RestClient implements Closeable &#123; private static final Log logger = LogFactory.getLog(RestClient.class); private final CloseableHttpAsyncClient client; //we don't rely on default headers supported by HttpAsyncClient as those cannot be replaced private final Header[] defaultHeaders; private final long maxRetryTimeoutMillis; private final String pathPrefix; private final AtomicInteger lastHostIndex = new AtomicInteger(0); private volatile HostTuple&lt;Set&lt;HttpHost&gt;&gt; hostTuple; private final ConcurrentMap&lt;HttpHost, DeadHostState&gt; blacklist = new ConcurrentHashMap&lt;&gt;(); private final FailureListener failureListener; RestClient(CloseableHttpAsyncClient client, long maxRetryTimeoutMillis, Header[] defaultHeaders, HttpHost[] hosts, String pathPrefix, FailureListener failureListener) &#123; this.client = client; this.maxRetryTimeoutMillis = maxRetryTimeoutMillis; this.defaultHeaders = defaultHeaders; this.failureListener = failureListener; this.pathPrefix = pathPrefix; setHosts(hosts); &#125; /** * Returns a new &#123;@link RestClientBuilder&#125; to help with &#123;@link RestClient&#125; creation. */ public static RestClientBuilder builder(HttpHost... hosts) &#123; return new RestClientBuilder(hosts); &#125; /** * Replaces the hosts that the client communicates with. * @see HttpHost */ public synchronized void setHosts(HttpHost... hosts) &#123; if (hosts == null || hosts.length == 0) &#123; throw new IllegalArgumentException(\"hosts must not be null nor empty\"); &#125; Set&lt;HttpHost&gt; httpHosts = new HashSet&lt;&gt;(); AuthCache authCache = new BasicAuthCache(); for (HttpHost host : hosts) &#123; Objects.requireNonNull(host, \"host cannot be null\"); httpHosts.add(host); authCache.put(host, new BasicScheme()); &#125; this.hostTuple = new HostTuple&lt;&gt;(Collections.unmodifiableSet(httpHosts), authCache); this.blacklist.clear(); &#125; 这里不像TransportClient可以启用嗅探功能，仅仅只是会维护你明确配置的所有host，如果有必要，则把认证信息authCache缓存起来而已，剩下的事情就是交给httpClient调用。 对比这两种客户端的初始化，TransportClient稍微复杂一些，RestClient相对简单，它是最后是将所有的调用交给Apache的HttpClient客户端去完成。 至此，我们Client的源码流程大致分析完成，但是TransportClient还有一个问题，底层的通信是怎么样的？我们紧接着分析Transport源码来解答这一疑问。 buildTemplate()方法中有这么一段代码： 123456final Transport transport = networkModule.getTransportSupplier().get(); // 初始化TransportService final TransportService transportService = new TransportService(settings, transport, threadPool, networkModule.getTransportInterceptor(), boundTransportAddress -&gt; DiscoveryNode.createLocal(settings, address, UUIDs.randomBase64UUID()), null); 这段代码就说明，节点间通信都是需要构造Transport实例，然后传递到TransportService当中来进行的。 2.3 Transport源码分析我们知道这个Transport是用来进行节点之间通信的核心类。无论是无论是集群状态信息，还是搜索索引请求信息，都是通过transport传送。transport实现一般有LocalTransport和NettyTransport两种，在NetworkModule中注册中可以通过node是local还是network的来判别使用哪一种transport，可以通过配置transport.type来决定。 2.3.1 TransportClient调用流程通过transportService.start()方法，最终会进入到doStart()方法中，这个流程可以参考源码启动章节 我们来看看doStart()方法： 12345678910111213141516171819202122232425262728protected void doStart() &#123; adapter.rxMetric.clear(); adapter.txMetric.clear(); transport.transportServiceAdapter(adapter); // 调用父类的start()方法 transport.start(); if (transport.boundAddress() != null &amp;&amp; logger.isInfoEnabled()) &#123; logger.info(\"&#123;&#125;\", transport.boundAddress()); for (Map.Entry&lt;String, BoundTransportAddress&gt; entry : transport.profileBoundAddresses().entrySet()) &#123; logger.info(\"profile [&#123;&#125;]: &#123;&#125;\", entry.getKey(), entry.getValue()); &#125; &#125; // 获取节点 localNode = localNodeFactory.apply(transport.boundAddress()); // 注册到injection中：这里的线程类型是SAME registerRequestHandler( HANDSHAKE_ACTION_NAME, () -&gt; HandshakeRequest.INSTANCE, ThreadPool.Names.SAME, false, false, (request, channel) -&gt; channel.sendResponse( new HandshakeResponse(localNode, clusterName, localNode.getVersion()))); if (connectToRemoteCluster) &#123; // here we start to connect to the remote clusters remoteClusterService.initializeRemoteClusters(); &#125; &#125; NettyTransport.doStart()方法：这个方法创建了Netty的一个客户端ClientBootstrap和一个服务器ServerBootstrap，因为节点之间要进行通信，所以这个节点既是客户端也是服务端。默认情况下客户端和服务端都会启动。 12345678910111213141516171819202122232425262728protected void doStart() &#123; boolean success = false; try &#123; // 创建一个clientBootstrap bootstrap = createBootstrap(); if (NetworkService.NETWORK_SERVER.get(settings)) &#123; final Netty4OpenChannelsHandler openChannels = new Netty4OpenChannelsHandler(logger); this.serverOpenChannels = openChannels; // loop through all profiles and start them up, special handling for default one for (Map.Entry&lt;String, Settings&gt; entry : buildProfileSettings().entrySet()) &#123; // merge fallback settings with default settings with profile settings so we have complete settings with default values final Settings settings = Settings.builder() .put(createFallbackSettings()) .put(entry.getValue()).build(); // 创建一个ServerBootstrap createServerBootstrap(entry.getKey(), settings); bindServer(entry.getKey(), settings); &#125; &#125; // 调用TCPTransport的doStart()方法创建一个GENERIC类型的线程池 super.doStart(); success = true; &#125; finally &#123; if (success == false) &#123; doStop(); &#125; &#125; &#125; 在transportService.dosStart()方法当中，会调用transport.start()，最终会调用NettyTransport.doStart()方法。 通过上面的分析，我们就可以知道，最终transport service中的transport实例落到了transport 模块上。transport模块分为local transport 和 netty transport。这里我们只看netty transport。 其他的流程跟这个大致相同，就不再分析了。 我们来分析一下这个NettyTransport类： 2.3.2 Netty4Transport分析这个类，我们要带着几个问题去看： 怎么启动一个服务？ 怎么去发送数据？ 怎么接收数据？ 2.3.2.1 类图 我们可以看到Netty4Transport继承了TcpTransport。 2.3.2.2 配置信息12345678910111213141516171819202122232425// workerCount表示transport的总共的worker数目，由transport.netty.worker_count来配置 // 默认值是32和Runtime.getRuntime().availableProcessors()中的最小值，也就是不能超过32 public static final Setting&lt;Integer&gt; WORKER_COUNT = new Setting&lt;&gt;(\"transport.netty.worker_count\", (s) -&gt; Integer.toString(EsExecutors.boundedNumberOfProcessors(s) * 2), (s) -&gt; Setting.parseInt(s, 1, \"transport.netty.worker_count\"), Property.NodeScope, Property.Shared); // 最大 public static final Setting&lt;ByteSizeValue&gt; NETTY_MAX_CUMULATION_BUFFER_CAPACITY = Setting.byteSizeSetting( \"transport.netty.max_cumulation_buffer_capacity\", new ByteSizeValue(-1), Property.NodeScope, Property.Shared); public static final Setting&lt;Integer&gt; NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS = Setting.intSetting(\"transport.netty.max_composite_buffer_components\", -1, -1, Property.NodeScope, Property.Shared); public static final Setting&lt;ByteSizeValue&gt; NETTY_RECEIVE_PREDICTOR_SIZE = Setting.byteSizeSetting( \"transport.netty.receive_predictor_size\", new ByteSizeValue(64, ByteSizeUnit.KB), Property.NodeScope, Property.Shared); public static final Setting&lt;ByteSizeValue&gt; NETTY_RECEIVE_PREDICTOR_MIN = byteSizeSetting(\"transport.netty.receive_predictor_min\", NETTY_RECEIVE_PREDICTOR_SIZE, Property.NodeScope, Property.Shared); public static final Setting&lt;ByteSizeValue&gt; NETTY_RECEIVE_PREDICTOR_MAX = byteSizeSetting(\"transport.netty.receive_predictor_max\", NETTY_RECEIVE_PREDICTOR_SIZE, Property.NodeScope, Property.Shared); // Netty的boss线程池大小 public static final Setting&lt;Integer&gt; NETTY_BOSS_COUNT = intSetting(\"transport.netty.boss_count\", 1, 1, Property.NodeScope, Property.Shared); 2.3.2.3 启动服务其实在TransportClient的调用流程中已经说明了怎么去启动一个Netty服务。主要是通过下面的这句话来启动： 1transportService.start(); 最终会调用到Netty4Transport的doStart()方法，在改方法中创建clientBootstrap和serverBootstrap. 客户端代码： 1234567891011121314151617181920212223242526272829303132333435private Bootstrap createBootstrap() &#123; final Bootstrap bootstrap = new Bootstrap(); if (TCP_BLOCKING_CLIENT.get(settings)) &#123; bootstrap.group(new OioEventLoopGroup(1, daemonThreadFactory(settings, TRANSPORT_CLIENT_WORKER_THREAD_NAME_PREFIX))); bootstrap.channel(OioSocketChannel.class); &#125; else &#123; bootstrap.group(new NioEventLoopGroup(workerCount, daemonThreadFactory(settings, TRANSPORT_CLIENT_BOSS_THREAD_NAME_PREFIX))); bootstrap.channel(NioSocketChannel.class); &#125; bootstrap.handler(getClientChannelInitializer()); bootstrap.option(ChannelOption.CONNECT_TIMEOUT_MILLIS, Math.toIntExact(defaultConnectionProfile.getConnectTimeout().millis())); bootstrap.option(ChannelOption.TCP_NODELAY, TCP_NO_DELAY.get(settings)); bootstrap.option(ChannelOption.SO_KEEPALIVE, TCP_KEEP_ALIVE.get(settings)); final ByteSizeValue tcpSendBufferSize = TCP_SEND_BUFFER_SIZE.get(settings); if (tcpSendBufferSize.getBytes() &gt; 0) &#123; bootstrap.option(ChannelOption.SO_SNDBUF, Math.toIntExact(tcpSendBufferSize.getBytes())); &#125; final ByteSizeValue tcpReceiveBufferSize = TCP_RECEIVE_BUFFER_SIZE.get(settings); if (tcpReceiveBufferSize.getBytes() &gt; 0) &#123; bootstrap.option(ChannelOption.SO_RCVBUF, Math.toIntExact(tcpReceiveBufferSize.getBytes())); &#125; bootstrap.option(ChannelOption.RCVBUF_ALLOCATOR, recvByteBufAllocator); final boolean reuseAddress = TCP_REUSE_ADDRESS.get(settings); bootstrap.option(ChannelOption.SO_REUSEADDR, reuseAddress); bootstrap.validate(); return bootstrap; &#125; 服务端代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private void createServerBootstrap(String name, Settings settings) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"using profile[&#123;&#125;], worker_count[&#123;&#125;], port[&#123;&#125;], bind_host[&#123;&#125;], publish_host[&#123;&#125;], compress[&#123;&#125;], \" + \"connect_timeout[&#123;&#125;], connections_per_node[&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;], receive_predictor[&#123;&#125;-&gt;&#123;&#125;]\", name, workerCount, settings.get(\"port\"), settings.get(\"bind_host\"), settings.get(\"publish_host\"), compress, defaultConnectionProfile.getConnectTimeout(), defaultConnectionProfile.getNumConnectionsPerType(TransportRequestOptions.Type.RECOVERY), defaultConnectionProfile.getNumConnectionsPerType(TransportRequestOptions.Type.BULK), defaultConnectionProfile.getNumConnectionsPerType(TransportRequestOptions.Type.REG), defaultConnectionProfile.getNumConnectionsPerType(TransportRequestOptions.Type.STATE), defaultConnectionProfile.getNumConnectionsPerType(TransportRequestOptions.Type.PING), receivePredictorMin, receivePredictorMax); &#125; final ThreadFactory workerFactory = daemonThreadFactory(this.settings, TRANSPORT_SERVER_WORKER_THREAD_NAME_PREFIX, name); final ServerBootstrap serverBootstrap = new ServerBootstrap(); if (TCP_BLOCKING_SERVER.get(settings)) &#123; serverBootstrap.group(new OioEventLoopGroup(workerCount, workerFactory)); serverBootstrap.channel(OioServerSocketChannel.class); &#125; else &#123; serverBootstrap.group(new NioEventLoopGroup(workerCount, workerFactory)); serverBootstrap.channel(NioServerSocketChannel.class); &#125; serverBootstrap.childHandler(getServerChannelInitializer(name, settings)); serverBootstrap.childOption(ChannelOption.TCP_NODELAY, TCP_NO_DELAY.get(settings)); serverBootstrap.childOption(ChannelOption.SO_KEEPALIVE, TCP_KEEP_ALIVE.get(settings)); final ByteSizeValue tcpSendBufferSize = TCP_SEND_BUFFER_SIZE.getDefault(settings); if (tcpSendBufferSize != null &amp;&amp; tcpSendBufferSize.getBytes() &gt; 0) &#123; serverBootstrap.childOption(ChannelOption.SO_SNDBUF, Math.toIntExact(tcpSendBufferSize.getBytes())); &#125; final ByteSizeValue tcpReceiveBufferSize = TCP_RECEIVE_BUFFER_SIZE.getDefault(settings); if (tcpReceiveBufferSize != null &amp;&amp; tcpReceiveBufferSize.getBytes() &gt; 0) &#123; serverBootstrap.childOption(ChannelOption.SO_RCVBUF, Math.toIntExact(tcpReceiveBufferSize.bytesAsInt())); &#125; serverBootstrap.option(ChannelOption.RCVBUF_ALLOCATOR, recvByteBufAllocator); serverBootstrap.childOption(ChannelOption.RCVBUF_ALLOCATOR, recvByteBufAllocator); final boolean reuseAddress = TCP_REUSE_ADDRESS.get(settings); serverBootstrap.option(ChannelOption.SO_REUSEADDR, reuseAddress); serverBootstrap.childOption(ChannelOption.SO_REUSEADDR, reuseAddress); serverBootstrap.validate(); serverBootstraps.put(name, serverBootstrap); &#125; 2.3.2.4 连接服务我们以UnicastZenPing的连接为例： 在Discovery启动以后，会发现新的节点，如果发现有新节点加入，则会在UnicastZenPing调用transportService.openConnection(node, connectionProfile)方法 1234567public Transport.Connection openConnection(final DiscoveryNode node, ConnectionProfile profile) throws IOException &#123; if (isLocalNode(node)) &#123; return localNodeConnection; &#125; else &#123; return transport.openConnection(node, profile); &#125; &#125; 最终调用TcpTransport的openConnection方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public final NodeChannels openConnection(DiscoveryNode node, ConnectionProfile connectionProfile) throws IOException &#123; if (node == null) &#123; throw new ConnectTransportException(null, \"can't open connection to a null node\"); &#125; boolean success = false; NodeChannels nodeChannels = null; connectionProfile = resolveConnectionProfile(connectionProfile, defaultConnectionProfile); globalLock.readLock().lock(); // ensure we don't open connections while we are closing try &#123; ensureOpen(); try &#123; AtomicBoolean runOnce = new AtomicBoolean(false); Consumer&lt;Channel&gt; onClose = c -&gt; &#123; try &#123; // we can't assert that the channel is closed here since netty3 has a different behavior that doesn't // consider a channel closed while it's close future is running. onChannelClosed(c); &#125; finally &#123; // we only need to disconnect from the nodes once since all other channels // will also try to run this we protect it from running multiple times. if (runOnce.compareAndSet(false, true)) &#123; disconnectFromNodeChannel(c, \"channel closed\"); &#125; &#125; &#125;; // 获取连接的Channel nodeChannels = connectToChannels(node, connectionProfile, onClose); final Channel channel = nodeChannels.getChannels().get(0); // one channel is guaranteed by the connection profile final TimeValue connectTimeout = connectionProfile.getConnectTimeout() == null ? defaultConnectionProfile.getConnectTimeout() : connectionProfile.getConnectTimeout(); final TimeValue handshakeTimeout = connectionProfile.getHandshakeTimeout() == null ? connectTimeout : connectionProfile.getHandshakeTimeout(); // 处理数据 final Version version = executeHandshake(node, channel, handshakeTimeout); if (version != null) &#123; // if we are talking to a pre 5.2 node we won't be able to retrieve the version since it doesn't implement the handshake // we do since 5.2 - in this case we just go with the version provided by the node. nodeChannels = new NodeChannels(nodeChannels, version); // clone the channels - we now have the correct version &#125; transportServiceAdapter.onConnectionOpened(nodeChannels); openConnections.add(nodeChannels); success = true; return nodeChannels; &#125; catch (ConnectTransportException e) &#123; throw e; &#125; catch (Exception e) &#123; // ConnectTransportExceptions are handled specifically on the caller end - we wrap the actual exception to ensure // only relevant exceptions are logged on the caller end.. this is the same as in connectToNode throw new ConnectTransportException(node, \"general node connection failure\", e); &#125; finally &#123; if (success == false) &#123; IOUtils.closeWhileHandlingException(nodeChannels); &#125; &#125; &#125; finally &#123; globalLock.readLock().unlock(); &#125; &#125; 实质就是创建Netty中的Channel，一个连接就是第一个Channel，根据之前所说的会有多个类型的连接会创。 2.3.2.5 发送数据先了解发送类型： 12345RECOVERY：做数据恢复recovery，默认个数2个；BULK：做数据恢复recovery，默认个数2个；REG：典型的搜索和单doc索引，默认个数6个STATE：如集群state的发送等，默认个数1个PING：是node之间的ping。默认个数1个 怎么获取Channel？在TransportService的sendRequest方法会传入一个参数options，是TransportRequestOptions的实例，包含三个属性：timeout(超时时间)、compress(是否压缩)、type(发送的类型，即上面说的五个之一)，根据需要发送数据的节点和发送的类型（上面五大类型）获得到对应的channel，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 public final &lt;T extends TransportResponse&gt; void sendRequest(final Transport.Connection connection, final String action,final TransportRequest request,final TransportRequestOptions options,TransportResponseHandler&lt;T&gt; handler) &#123; asyncSender.sendRequest(connection, action, request, options, handler); &#125;this.asyncSender = interceptor.interceptSender(this::sendRequestInternal);private &lt;T extends TransportResponse&gt; void sendRequestInternal(final Transport.Connection connection, final String action,final TransportRequest request,final TransportRequestOptions options,TransportResponseHandler&lt;T&gt; handler) &#123; if (connection == null) &#123; throw new IllegalStateException(\"can't send request to a null connection\"); &#125; DiscoveryNode node = connection.getNode(); final long requestId = transport.newRequestId(); final TimeoutHandler timeoutHandler; try &#123; if (options.timeout() == null) &#123; timeoutHandler = null; &#125; else &#123; timeoutHandler = new TimeoutHandler(requestId); &#125; Supplier&lt;ThreadContext.StoredContext&gt; storedContextSupplier = threadPool.getThreadContext().newRestorableContext(true); TransportResponseHandler&lt;T&gt; responseHandler = new ContextRestoreResponseHandler&lt;&gt;(storedContextSupplier, handler); clientHandlers.put(requestId, new RequestHolder&lt;&gt;(responseHandler, connection, action, timeoutHandler)); if (lifecycle.stoppedOrClosed()) &#123; // if we are not started the exception handling will remove the RequestHolder again and calls the handler to notify // the caller. It will only notify if the toStop code hasn't done the work yet. throw new TransportException(\"TransportService is closed stopped can't send request\"); &#125; if (timeoutHandler != null) &#123; assert options.timeout() != null; timeoutHandler.future = threadPool.schedule(options.timeout(), ThreadPool.Names.GENERIC, timeoutHandler); &#125; connection.sendRequest(requestId, action, request, options); // local node optimization happens upstream &#125; catch (final Exception e) &#123; // usually happen either because we failed to connect to the node // or because we failed serializing the message final RequestHolder holderToNotify = clientHandlers.remove(requestId); // If holderToNotify == null then handler has already been taken care of. if (holderToNotify != null) &#123; holderToNotify.cancelTimeout(); // callback that an exception happened, but on a different thread since we don't // want handlers to worry about stack overflows final SendRequestTransportException sendRequestException = new SendRequestTransportException(node, action, e); threadPool.executor(ThreadPool.Names.GENERIC).execute(new AbstractRunnable() &#123; @Override public void onRejection(Exception e) &#123; // if we get rejected during node shutdown we don't wanna bubble it up logger.debug( (Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage( \"failed to notify response handler on rejection, action: &#123;&#125;\", holderToNotify.action()), e); &#125; @Override public void onFailure(Exception e) &#123; logger.warn( (Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage( \"failed to notify response handler on exception, action: &#123;&#125;\", holderToNotify.action()), e); &#125; @Override protected void doRun() throws Exception &#123; holderToNotify.handler().handleException(sendRequestException); &#125; &#125;); &#125; else &#123; logger.debug(\"Exception while sending request, handler likely already notified due to timeout\", e); &#125; &#125; &#125; 2.3.2.6 数据写入之后数据该压缩的压缩（压缩方法在CompressorFactory中实现），并写入version和action； 在openConnection方法中，通过调用executeHandshake()方法处理数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152final Version version = executeHandshake(node, channel, handshakeTimeout);protected Version executeHandshake(DiscoveryNode node, Channel channel, TimeValue timeout) throws IOException, InterruptedException &#123; numHandshakes.inc(); final long requestId = newRequestId(); final HandshakeResponseHandler handler = new HandshakeResponseHandler(channel); AtomicReference&lt;Version&gt; versionRef = handler.versionRef; AtomicReference&lt;Exception&gt; exceptionRef = handler.exceptionRef; pendingHandshakes.put(requestId, handler); boolean success = false; try &#123; if (isOpen(channel) == false) &#123; // we have to protect us here since sendRequestToChannel won't barf if the channel is closed. // it's weird but to change it will cause a lot of impact on the exception handling code all over the codebase. // yet, if we don't check the state here we might have registered a pending handshake handler but the close // listener calling #onChannelClosed might have already run and we are waiting on the latch below unitl we time out. throw new IllegalStateException(\"handshake failed, channel already closed\"); &#125; // for the request we use the minCompatVersion since we don't know what's the version of the node we talk to // we also have no payload on the request but the response will contain the actual version of the node we talk // to as the payload. final Version minCompatVersion = getCurrentVersion().minimumCompatibilityVersion(); // 调用该方法发送数据 sendRequestToChannel(node, channel, requestId, HANDSHAKE_ACTION_NAME, TransportRequest.Empty.INSTANCE, TransportRequestOptions.EMPTY, minCompatVersion, TransportStatus.setHandshake((byte)0)); if (handler.latch.await(timeout.millis(), TimeUnit.MILLISECONDS) == false) &#123; throw new ConnectTransportException(node, \"handshake_timeout[\" + timeout + \"]\"); &#125; success = true; if (handler.handshakeNotSupported.get()) &#123; // this is a BWC layer, if we talk to a pre 5.2 node then the handshake is not supported // this will go away in master once it's all ported to 5.2 but for now we keep this to make // the backport straight forward return null; &#125; if (exceptionRef.get() != null) &#123; throw new IllegalStateException(\"handshake failed\", exceptionRef.get()); &#125; else &#123; Version version = versionRef.get(); if (getCurrentVersion().isCompatible(version) == false) &#123; throw new IllegalStateException(\"Received message from unsupported version: [\" + version + \"] minimal compatible version is: [\" + getCurrentVersion().minimumCompatibilityVersion() + \"]\"); &#125; return version; &#125; &#125; finally &#123; final TransportResponseHandler&lt;?&gt; removedHandler = pendingHandshakes.remove(requestId); // in the case of a timeout or an exception on the send part the handshake has not been removed yet. // but the timeout is tricky since it's basically a race condition so we only assert on the success case. assert success &amp;&amp; removedHandler == null || success == false : \"handler for requestId [\" + requestId + \"] is not been removed\"; &#125; &#125; 我们来看sendRequestToChannel方法：写入request信息，通过ChannelBuffers创建出buffer 123456789101112131415161718192021222324252627282930313233343536373839404142434445private void sendRequestToChannel(DiscoveryNode node, final Channel targetChannel, final long requestId, final String action,final TransportRequest request, TransportRequestOptions options, Version channelVersion,byte status) throws IOException, TransportException &#123; if (compress) &#123; options = TransportRequestOptions.builder(options).withCompress(true).build(); &#125; status = TransportStatus.setRequest(status); ReleasableBytesStreamOutput bStream = new ReleasableBytesStreamOutput(bigArrays); boolean addedReleaseListener = false; StreamOutput stream = Streams.flushOnCloseStream(bStream); try &#123; // only compress if asked, and, the request is not bytes, since then only // the header part is compressed, and the \"body\" can't be extracted as compressed if (options.compress() &amp;&amp; canCompress(request)) &#123; status = TransportStatus.setCompress(status); // 对数据做压缩 stream = CompressorFactory.COMPRESSOR.streamOutput(stream); &#125; // we pick the smallest of the 2, to support both backward and forward compatibility // note, this is the only place we need to do this, since from here on, we use the serialized version // as the version to use also when the node receiving this request will send the response with // 写入Version信息 Version version = Version.min(getCurrentVersion(), channelVersion); stream.setVersion(version); threadPool.getThreadContext().writeTo(stream); stream.writeString(action); BytesReference message = buildMessage(requestId, status, node.getVersion(), request, stream, bStream); final TransportRequestOptions finalOptions = options; final StreamOutput finalStream = stream; Runnable onRequestSent = () -&gt; &#123; // this might be called in a different thread try &#123; IOUtils.closeWhileHandlingException(finalStream, bStream); &#125; finally &#123; transportServiceAdapter.onRequestSent(node, requestId, action, request, finalOptions); &#125; &#125;; addedReleaseListener = internalSendMessage(targetChannel, message, onRequestSent); &#125; finally &#123; IOUtils.close(stream); if (!addedReleaseListener) &#123; IOUtils.close(stream, bStream); &#125; &#125; &#125; 2.3.2.7 数据接收启动服务中的channelPipeline注册，统一的handler为MessageChannelHandler，负责消息接受及处理逻辑，在其他模块中会对不同的消息（Action）注册对应的处理程序（handler）。 在Netty4Transport的createBootstrap()方法中： 1234567891011121314151617181920212223bootstrap.handler(getClientChannelInitializer());。。。protected ChannelHandler getClientChannelInitializer() &#123; return new ClientChannelInitializer(); &#125; protected class ClientChannelInitializer extends ChannelInitializer&lt;Channel&gt; &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(\"size\", new Netty4SizeHeaderFrameDecoder()); // using a dot as a prefix means this cannot come from any settings parsed ch.pipeline().addLast(\"dispatcher\", new Netty4MessageChannelHandler(Netty4Transport.this, \".client\")); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; Netty4Utils.maybeDie(cause); super.exceptionCaught(ctx, cause); &#125; &#125; 在Netty4Transport的createServerBootstrap()方法中： 1234567891011121314151617serverBootstrap.childHandler(getServerChannelInitializer(name, settings));protected ChannelHandler getServerChannelInitializer(String name, Settings settings) &#123; return new ServerChannelInitializer(name, settings); &#125; protected ServerChannelInitializer(String name, Settings settings) &#123; this.name = name; this.settings = settings; &#125; @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(\"open_channels\", Netty4Transport.this.serverOpenChannels); ch.pipeline().addLast(\"size\", new Netty4SizeHeaderFrameDecoder()); ch.pipeline().addLast(\"dispatcher\", new Netty4MessageChannelHandler(Netty4Transport.this, name)); &#125; 可以看到在ClientBootStrap和ServerBootStrap中都注册了消息处理的类 MessageChannelHandler： 1pipeline.addLast(\"dispatcher\", new MessageChannelHandler(NettyTransport.this, logger)); MessageChannelHandler类负责消息接受及处理逻辑，在其他模块中会对不同的消息（Action）注册对应的处理程序（handler）。在对收到的内容进行解析的过程中获取到action，找到对应的handler进行处理，这个过程会交给threadpool中的线程来操作。 比如SearchAction的注册代码： 1transportService.registerHandler(SearchAction.NAME, new TransportHandler()); 在TcpTransport中的messageReceived方法则是负责消息的接收。 至此，Transport模块我们分析完成了。 三、总结","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]},{"title":"elasticsearch源码分析(三)Discover模块","slug":"Elasticsearch/elasticsearch源码分析(三)--Discover模块","date":"2018-05-27T04:12:57.000Z","updated":"2018-06-26T02:50:51.038Z","comments":true,"path":"2018/05/27/Elasticsearch/elasticsearch源码分析(三)--Discover模块/","link":"","permalink":"http://yoursite.com/2018/05/27/Elasticsearch/elasticsearch源码分析(三)--Discover模块/","excerpt":"","text":"通过上一篇对Elasticsearch启动的分析，我们知道了ES启动的大致流程，还遗留下几个问题 master选举是在什么模块进行的 ES集群是如何进行Master选举的？ ES是如何维护这些节点的？ Discovery模块是如何运作的？ 要想进行Master选举，必然要有一套算法机制，以及节点之前的通信连接、判断节点存活状态等。 通过查阅官网资料，我们知道这些功能是在Elasticsearch的发现协议Discovery里面进行的，在官网上，Elasticsearch的Discovery Module有下面几种实现： Azure Classic Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-azure-classic.html EC2 Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-ec2.html#modules-discovery-ec2 Google Compute Engine Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-gce.html Zen Discovery：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-discovery-zen.html ​ 一、Zen Discovery模块介绍这里基本上是官网的翻译，建议还是查看官网文档，翻译不准。。。 Zen Discovery是内置在elasticsearch的默认发现模块。它提供单播发现，但可扩展到支持云环境和其他形式的发现。 禅发现集成了其它模块，例如，节点之间的所有通信是使用transport模块。 它被分离成多个子模块，其解释如下： 1.1 Ping这是一个节点使用发现机制来查找其他节点的过程。 1.2 Unicast单播发现需要一个主机列表，用于将作为GossipRouter。这些宿主可被指定为主机名或IP地址;指定主机名的主机每一轮Ping过程中解析为IP地址。请注意，如果您处于DNS解析度随时间变化的环境中，则可能需要调整JVM安全设置。 建议将单播主机列表维护为集群中符合主节点的节点列表。 单播发现提供以下设置和discovery.zen.ping.unicast前缀： 设置 描述 hosts 数组设置或逗号分隔的设置。每个值的形式应该是host:port或host（如果没有设置，port默认设置会transport.profiles.default.port 回落到transport.tcp.port）。请注意，IPv6主机必须放在括号内。默认为127.0.0.1, [::1] hosts.resolve_timeout 在每轮ping中等待DNS查找的时间量。指定为 时间单位。默认为5秒。 单播发现使用传输模块执行发现。 1.3 master选举作为Ping过程的一部分，集群的主节点要么当选要么加入假期。这是自动完成的。ping的默认超时为3秒 1discovery.zen.ping_timeout（默认为3s） 如果在超时后没有做出决定，则重新启动ping程序。在缓慢或拥塞的网络中，在作出选举决定之前，三秒可能不足以让节点意识到其环境中的其他节点。在这种情况下，应该谨慎地增加超时时间，因为这会减慢选举进程。一旦一个节点决定加入一个现有的已形成的集群，它将发送一个加入请求给主设备（discovery.zen.join_timeout）的超时默认值是ping超时的20倍。 当主节点停止或遇到问题时，群集节点会再次启动ping并选择新的主节点。这种ping测试也可以作为防止（部分）网络故障的保护，其中一个节点可能会不公正地认为主站发生故障。在这种情况下，节点将简单地从其他节点听到关于当前活动的主节点的信息。 如果discovery.zen.master_election.ignore_non_master_pings是true，没有参与资格（节点，其中节点坪node.master是false）的主选期间忽略; 默认值是 false。 可以通过设置node.master来排除节点成为主节点false。 该discovery.zen.minimum_master_nodes套需要加入新当选主为了选举完成并当选节点接受其主控权掌握合格节点的最小数量。相同的设置控制应该成为任何活动集群一部分的活动主节点合格节点的最小数量。如果不满足这个要求，活动的主节点将下台，新的主节点选举将开始。 此设置必须设置为您的主要合格节点的法定人数。建议避免只有两个主节点，因为两个法定人数是两个。因此，任何主节点的损失都将导致无法运行的群集。 1.4 故障检测有两个故障检测进程正在运行。第一种方法是通过主设备对群集中的所有其他节点进行ping操作，并验证它们是否处于活动状态。另一方面，每个节点都会主动确认它是否仍然存在或需要启动选举过程。 以下设置使用discovery.zen.fd前缀控制故障检测过程 ： 设置 描述 ping_interval 一个节点多久发作一次。默认为1s。 ping_timeout 等待ping响应需要多长时间，默认为 30s。 ping_retries 有多少ping故障/超时会导致节点被视为失败。默认为3。 1.5 群集状态更新Cluster state updates主节点是群集中，可以使改变集群状态的唯一节点。主节点一次处理一个集群状态更新，状态改变和发布更新的到集群中的所有其他节点。每个节点接收发布消息，确认它，但还没有立即应用它。如果主节点没有接收来自节点确认的数量至少为discovery.zen.minimum_master_nodes，在时间（由受控discovery.zen.commit_timeout设置，默认值为30秒）内。节点集群状态改变被拒绝。 一旦足够的节点已作出回应，集群状态改变被提交然后消息将被发送到所有结点。然后节点然后进行新的群集状态适用于他们的内部状态。主节点等待所有节点响应，在去队列处理下一个状态更新之前，直到超时，超时时间是在discovery.zen.publish_timeout默认情况下设置为30秒，时间从发布开始时测量h超时设置可以通过动态的改变集群更新设置API 1.6 无主块No master block要使群集完全可操作，它必须具有活动的主节点和一些有主资格的节点，并且主资格的节点必须满足的数目必须满足discovery.zen.minimum_master_nodes设置的值。如果设置 discovery.zen.no_master_block ，那么设置控制在没有活动的主设备时应拒绝哪些操作。 该discovery.zen.no_master_block设置有两个有效选项： all 节点上的所有操作（即读取和写入操作）都将被拒绝。这也适用于api集群状态读取或写入操作，如get索引设置，put映射和集群状态api。 write （默认）写入操作将被拒绝。基于最后一次已知的群集配置，读取操作将成功。这可能会导致部分读取过时的数据，因为此节点可能与群集的其余部分隔离。 该discovery.zen.no_master_block设置不适用于基于节点的apis（例如，群集统计信息，节点信息和节点统计信息apis）。对这些apis的请求不会被阻止，并且可以在任何可用的节点上运行。 1.7 Fault Delection用ping的方式来确定node是否在集群里面 二、Discovery源码分析2.1 Discovery类图 2.2 与Discovery相关的几个类ZenDiscovery.java 模块的主类，也是启动这个模块的入口，由Node.java调用并初始化，几乎涵盖了全部的发现协议的逻辑，是一个高度内聚了类，它有一些成员变量，需要明白他们的意思： pingTimeout：取自discovery.zen.ping_timeout（默认为3s）允许调整选举时间来处理网络慢或拥塞的情况（更高的值确保更少的失败机会） joinTimeout：取自discovery.zen.join_timeout（默认值为ping超时的20倍）。当一个新的node加入集群时，将会发个join的request到master，这个request的timeout即joinTimeout。 joinRetryAttempts：join重试的次数，默认为3次。 joinRetryDelay：重试的间隔，默认为100ms。 maxPingsFromAnotherMaster：容忍其他master发出的,在强制其他或是本地master rejoin之前的次数。 masterElectionIgnoreNonMasters：用来控制在主节点选举时候的ping响应，只有在极端情况下才会使用这个参数，平时一般不用配置，默认值为false 1234有人说，选举master时，node.master为false的节点的投票是不起作用的，这个说法不完全正确：如果discovery.zen.master_election.ignore_non_master_pings设置为true，那么以上说法正确，但是默认是false，也就是说，它们的投票是起作用的，只是它们不可能成为master。所以我觉得，集群机器数不大的话，除了负担特别重的机器，都设置为node.master为true比较妥当。设置需要加入新一轮master选举的“master”候选人的最小数量也就是说，集群中，该值是针对那些node.master=true的来设置的，建议&gt;=num(node.master=true)/2+1.并不是有的朋友解释的，集群机器数量的除以2再加1，当然默认情况下是，因为默认情况下，discovery.zen.master_election.ignore_non_master_pings为false masterElectionWaitForJoinsTimeout：master选举时等待join的timeout,默认是joinTimeout的一半。 其中joinRetryAttempts和maxPingsFromAnotherMaster是一定要大于等于1的。 UnicastZenPing.java 是一个ZenPing 实现类，主要是负责底层和其他Nodes建立并维护连接的任务 PublishClusterStateAction.java 在ZenDiscovery中的变量名是publishClusterState，之前讲过，这些**Action 都是对**Service的封装，因此它主要是用来处理发送事件和处理事件的接口，比如发送一个clusterStateChangeEvent 和处理这个event，都是通过这个类调用 MasterFaultDetection.java 构建完cluster后所有的node用来检测master存活状态的类 NodeFaultDetection.java 构建完cluster后master用来检测其他node存活状态的类 2.3 如何运行我们通过上一篇的分析知道，在ES启动的时候会去实例化Node，然后调用Node#start()方法启动各个module，Discovery是在实例化Node的时候通过guice进行注入的，在Node启动的时候去启动的，代码如下： Node的构造函数中实例化 12345...final DiscoveryModule discoveryModule = new DiscoveryModule(this.settings, threadPool, transportService, namedWriteableRegistry, networkService, clusterService, pluginsService.filterPlugins(DiscoveryPlugin.class));...b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery()); 2.3.1 ZenDiscover的初始化初始化的时候会加载我上段ZenDiscovery模块介绍提到的几个模块，我就不再重复了，值得注意的是Fault Delection的分为两个masterFD和nodesFD；其次还加载了一些对于discover的配置 2.3.2 ZenDiscovery运行其实ZenDiscover的运行就是几个子模块的运行；它是通过Node#start()方法启动的。 在Node#start()方法中：我们可以看到Discovery相关的代码 12345678910Discovery discovery = injector.getInstance(Discovery.class);clusterService.setDiscoverySettings(discovery.getDiscoverySettings());clusterService.addInitialStateBlock(discovery.getDiscoverySettings().getNoMasterBlock());clusterService.setClusterStatePublisher(discovery::publish);... // start after cluster service so the local disco is known discovery.start(); transportService.acceptIncomingRequests(); // 核心方法 discovery.startInitialJoin(); 在DiscoveryModule类中， 12345678910111213141516171819202122 Map&lt;String, Supplier&lt;Discovery&gt;&gt; discoveryTypes = new HashMap&lt;&gt;(); discoveryTypes.put(\"zen\", () -&gt; new ZenDiscovery(settings, threadPool, transportService, namedWriteableRegistry, clusterService, hostsProvider)); discoveryTypes.put(\"none\", () -&gt; new NoneDiscovery(settings, clusterService, clusterService.getClusterSettings())); discoveryTypes.put(\"single-node\", () -&gt; new SingleNodeDiscovery(settings, clusterService)); for (DiscoveryPlugin plugin : plugins) &#123; plugin.getDiscoveryTypes(threadPool, transportService, namedWriteableRegistry, clusterService, hostsProvider).entrySet().forEach(entry -&gt; &#123; if (discoveryTypes.put(entry.getKey(), entry.getValue()) != null) &#123; throw new IllegalArgumentException(\"Cannot register discovery type [\" + entry.getKey() + \"] twice\"); &#125; &#125;); &#125;String discoveryType = DISCOVERY_TYPE_SETTING.get(settings); // 这里是函数式编程的用法，详情请百度或者Google Supplier&lt;Discovery&gt; discoverySupplier = discoveryTypes.get(discoveryType); if (discoverySupplier == null) &#123; throw new IllegalArgumentException(\"Unknown discovery type [\" + discoveryType + \"]\"); &#125; Loggers.getLogger(getClass(), settings).info(\"using discovery type [&#123;&#125;]\", discoveryType); discovery = Objects.requireNonNull(discoverySupplier.get()); 由上面的代码可以看出，这里Discovery的实例是由DisdcoveryModule的suppiler 提供。 discovery.start()方法调用AbstractLifecycleComponent#start()方法进行监听，同时在该start()方法中调用ZenDiscovery#doStart()方法进行真正工作，这里用到的模板方法的设计模式，在Spring中很多地方都是这样使用的，首先在抽象类 123456789101112131415161718192021public void start() &#123; // ES的生命周期zhuangt:INITIALIZED -&amp;gt; STARTED, STOPPED, CLOSED // 如果不可以启动，直接返回 if (!lifecycle.canMoveToStarted()) &#123; return; &#125; // 启动之前循环监听 for (LifecycleListener listener : listeners) &#123; listener.beforeStart(); &#125; // 调用ZenDiscovery的doStart()方法对一些变量进行初始化工作 doStart(); // 处理状态 lifecycle.moveToStarted(); // 启动之后的监听 for (LifecycleListener listener : listeners) &#123; listener.afterStart(); &#125; &#125; // 模板方法：由具体的Discovery类来实现，比如ZenDiscovery类 protected abstract void doStart(); ZenDiscovery的start()方法： 1234567891011121314/** * 该方法其实就是做了一些初始化操作，不要被它的start()命名给误导 */ @Override protected void doStart() &#123; // 节点故障探测设置 nodesFD.setLocalNode(clusterService.localNode()); // 调用连接线程控制类进行初始化操作 joinThreadControl.start(); // 设置ping参数 zenPing.start(this); this.nodeJoinController = new NodeJoinController(clusterService, allocationService, electMaster, settings); this.nodeRemovalExecutor = new NodeRemovalClusterStateTaskExecutor(allocationService, electMaster, this::submitRejoin, logger); &#125; discovery.startInitialJoin()方法分析： 1234567891011121314151617181920@Override public void startInitialJoin() &#123; // start the join thread from a cluster state update. See &#123;@link JoinThreadControl&#125; for details. clusterService.submitStateUpdateTask(\"initial_join\", new LocalClusterUpdateTask() &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) throws Exception &#123; // do the join on a different thread, the DiscoveryService waits for 30s anyhow till it is discovered // 调用这个方法进行详细的处理 joinThreadControl.startNewThreadIfNotRunning(); // 返回LocalClusterUpdateTask return unchanged(); &#125; // 加入集群失败后的逻辑：这里只是打印日志 @Override public void onFailure(String source, @org.elasticsearch.common.Nullable Exception e) &#123; logger.warn(\"failed to start initial join process\", e); &#125; &#125;); &#125; 我们通过上面的源码分析知道，joinThreadControl.startNewThreadIfNotRunning()这个方法是其核心处理逻辑，我们来看看它做了什么工作 123456789101112131415161718192021222324/** starts a new joining thread if there is no currently active one and join thread controlling is started */ public void startNewThreadIfNotRunning() &#123; ClusterService.assertClusterStateThread(); // 如果join线程还存活，直接返回 if (joinThreadActive()) &#123; return; &#125; // 从ES中的线程池获取generic线程池，然后提交一个任务 threadPool.generic().execute(new Runnable() &#123; @Override public void run() &#123; Thread currentThread = Thread.currentThread(); // CAS操作： if (!currentJoinThread.compareAndSet(null, currentThread)) &#123; return; &#125; // 第一次启动，这里的running肯定是true，是在之前的doStart()中进行初始化的 while (running.get() &amp;&amp; joinThreadActive(currentThread)) &#123; try &#123; // 第一步：首先自己先加入集群 innerJoinCluster(); return; ..... &#125; 通过上面的分析我们知道，如果join线程还存活，则直接返回，否则从从ES中的线程池获取generic线程池，然后提交一个任务，在该任务中主要调用innerJoinCluster()方法加入集群。 我们来猜猜看看innerJoinCluster()方法做了什么？ 1我们要加入一个集群，肯定先要找到组织，熟悉Elasticsearch的配置都知道，我们的ES默认的集群名称是slasticsearch，如果配置了，则为我们配置的名称（有点废话了），肯定要进行master选举 innerJoinCluster()方法分析： 12345678private void innerJoinCluster() &#123; DiscoveryNode masterNode = null; final Thread currentThread = Thread.currentThread(); nodeJoinController.startElectionContext(); // 通过findMaster方法来进行Master选举: while (masterNode == null &amp;&amp; joinThreadControl.joinThreadActive(currentThread)) &#123; masterNode = findMaster(); &#125; 三、master选举源码分析阅读该部分源码，我们带着下面的一些问题去看。 分布式系统设计思想？ ES为什么要master选举？ 有哪些选举算法？ master选举主流程和详细流程？ 什么时候触发选举？ 为什么不用ZK来实现master选举？ 如何获取到最新的状态数据？ 3.1 分布式系统设计思想所有的分布式系统都会遇到各个节点数据同步问题（一致性）、因网络故障存在的延迟、脑裂等问题都需要用一套合理的解决方案。比如，在互联网中比较经典的CAP理论和BASE理论。在ES中同样需要面临这样的问题？ 3.2 ES为什么要master选举一种选择是分布式哈希表(DHT),可以支持每小时数千个节点的离开和加入,他可以在不了解底层网络拓扑的异构网络中工作,查询响应时间大约为4到10跳(中转次数)，但是在相对稳定的对等网络中,Master模式会更好 Elasticsearch的典型场景中的另一个简化是集群中没有那么多节点。 通常，节点的数量远远小于单个节点能够维护的连接数，并且网格环境不必经常处理节点加入和离开。 这就是为什么master的做法更适合Elasticsearch。 3.3 选举算法3.3.1 Bully算法Leader选举的基本算法之一。 它假定所有节点都有一个惟一的ID，该ID对节点进行排序。 任何时候的当前Leader都是参与集群的最高id节点。 该算法的优点是易于实现,但是,当拥有最大 id 的节点处于不稳定状态的场景下会有问题,例如 Master 负载过重而假死,集群拥有第二大id 的节点被选为 新主,这时原来的 Master 恢复,再次被选为新主,然后又假死… elasticsearch 通过推迟选举直到当前的 Master 失效来解决上述问题,但是容易产生脑裂,再通过 法定得票人数过半 解决脑裂 3.3.2 Paxos算法Paxos实现起来非常复杂,但非常强大，尤其在什么时机,以及如何进行选举方面的灵活性比简单的Bully算法有很大的优势，因为在现实生活中，存在比网络链接异常更多的故障模式。比较典型的是Zookeeper在该算法进行了改进，形成自己的一套选举算法。 3.4 选举流程只有一个 Leader将当前版本的全局集群状态推送到每个节点。 ZenDiscovery（默认）过程就是这样的: 每个节点计算最低的已知节点ID，并向该节点发送领导投票 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演领导者的角色，开始发布集群状态。 所有节点都会参数选举,并参与投票,但是,只有有资格成为 master 的节点的投票才有效. 有多少选票赢得选举的定义就是所谓的法定人数。 在弹性搜索中，法定大小是一个可配置的参数。 （一般配置成:可以成为master节点数n/2+1） 3.5 选举详细流程3.5.1 获取PingResponse列表节奏上一个Discovery模块的源码进行分析： 123456789101112private DiscoveryNode findMaster() &#123; logger.trace(\"starting to ping\"); // ping所有节点并获取PingResponse List&lt;ZenPing.PingResponse&gt; fullPingResponses = pingAndWait(pingTimeout).toList();&#125;private ZenPing.PingCollection pingAndWait(TimeValue timeout) &#123; final CompletableFuture&lt;ZenPing.PingCollection&gt; response = new CompletableFuture&lt;&gt;(); try &#123; // 步骤2：ping所有节点，调用UnicastZenPing的ping()方法 zenPing.ping(response::complete, timeout); &#125; 从 response::complete和response.get两句大致就能猜猜，这个方法里面会异步发起请求，主线程等待response。 UnicastZenPing#ping()方法 12345678910111213141516171819202122232425262728293031 public void ping(final Consumer&lt;PingCollection&gt; resultsConsumer, final TimeValue duration) &#123; ping(resultsConsumer, duration, duration); &#125;protected void ping(final Consumer&lt;PingCollection&gt; resultsConsumer, final TimeValue scheduleDuration, final TimeValue requestDuration) &#123; final List&lt;DiscoveryNode&gt; seedNodes; try &#123; seedNodes = resolveHostsLists( // 1、从配置的discovery.zen.ping.unicast.hosts列表中获取 unicastZenPingExecutorService, logger, configuredHosts, limitPortCounts, transportService, UNICAST_NODE_PREFIX, resolveTimeout); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; // 通过其他方式添加seedNodes：List&lt;DiscoveryNode&gt; seedNodes.addAll(hostsProvider.buildDynamicNodes()); // 本实例最近一次的clusterState的masterNode final DiscoveryNodes nodes = contextProvider.clusterState().nodes(); // add all possible master nodes that were active in the last known cluster configuration // 步骤3：添加有成为master资格的节点 for (ObjectCursor&lt;DiscoveryNode&gt; masterNode : nodes.getMasterNodes().values()) &#123; seedNodes.add(masterNode.value); &#125; ....&#125; 从上面的代码可以看出，在sendPing之前需要确定seedNodes（List）,它从三个地方获取，第一，从配置的discovery.zen.ping.unicast.hosts列表中获取，第二、hostsProvider.buildDynamicNodes()中获取，最后，从本实例最近一次的clusterState的masterNode中获取。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public static List&lt;DiscoveryNode&gt; resolveHostsLists()&#123;// create tasks to submit to the executor service; we will wait up to resolveTimeout for these tasks to complete final List&lt;Callable&lt;TransportAddress[]&gt;&gt; callables = hosts .stream() // 地址列表通过TransportService构造 .map(hn -&gt; (Callable&lt;TransportAddress[]&gt;) () -&gt; transportService.addressesFromString(hn, limitPortCounts)) .collect(Collectors.toList()); // 异步Future列表 final List&lt;Future&lt;TransportAddress[]&gt;&gt; futures = executorService.invokeAll(callables, resolveTimeout.nanos(), TimeUnit.NANOSECONDS); final List&lt;DiscoveryNode&gt; discoveryNodes = new ArrayList&lt;&gt;(); final Set&lt;TransportAddress&gt; localAddresses = new HashSet&lt;&gt;(); localAddresses.add(transportService.boundAddress().publishAddress()); localAddresses.addAll(Arrays.asList(transportService.boundAddress().boundAddresses())); // ExecutorService#invokeAll guarantees that the futures are returned in the iteration order of the tasks so we can associate the // hostname with the corresponding task by iterating together final Iterator&lt;String&gt; it = hosts.iterator(); // 循环获取异步结果：就是简单的IP获取，为啥会用异步方式？ // 猜猜是因为host中可能配置有域名，怕解析时间过长 for (final Future&lt;TransportAddress[]&gt; future : futures) &#123; final String hostname = it.next(); if (!future.isCancelled()) &#123; assert future.isDone(); try &#123; final TransportAddress[] addresses = future.get(); logger.trace(\"resolved host [&#123;&#125;] to &#123;&#125;\", hostname, addresses); for (int addressId = 0; addressId &lt; addresses.length; addressId++) &#123; final TransportAddress address = addresses[addressId]; // no point in pinging ourselves if (localAddresses.contains(address) == false) &#123; discoveryNodes.add( new DiscoveryNode( nodeId_prefix + hostname + \"_\" + addressId + \"#\", address, emptyMap(), emptySet(), Version.CURRENT.minimumCompatibilityVersion())); &#125; &#125; &#125; catch (final ExecutionException e) &#123; assert e.getCause() != null; final String message = \"failed to resolve host [\" + hostname + \"]\"; logger.warn(message, e.getCause()); &#125; &#125; else &#123; logger.warn(\"timed out after [&#123;&#125;] resolving host [&#123;&#125;]\", resolveTimeout, hostname); &#125; &#125; // 返回List&lt;DiscoveryNode&gt; return discoveryNodes;&#125; 拿到seedNodes之后就需要发起连接，这里会构造一个叫PingRound的类来统计，并且分别会在 scheduleDuration的0, 1/3, 2/3时刻发起一轮sendPing操作，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041// 刚开始启动，集群健康状态设置为RED final ConnectionProfile connectionProfile = ConnectionProfile.buildSingleChannelProfile(TransportRequestOptions.Type.REG, requestDuration, requestDuration); // 构造一个叫PingRound的类来统计：这个类有id，seedNodes、pingListener、localNode等 final PingingRound pingingRound = new PingingRound(pingingRoundIdGenerator.incrementAndGet(), seedNodes, resultsConsumer, nodes.getLocalNode(), connectionProfile); activePingingRounds.put(pingingRound.id(), pingingRound); // 构造ping发送对象pingSender final AbstractRunnable pingSender = new AbstractRunnable() &#123; @Override public void onFailure(Exception e) &#123; if (e instanceof AlreadyClosedException == false) &#123; logger.warn(\"unexpected error while pinging\", e); &#125; &#125; @Override protected void doRun() throws Exception &#123; // 发送sendPings sendPings(requestDuration, pingingRound); &#125; &#125;; // 提交线generic程池：ping的连接不像其他那样由transportService 来保持长连接，而是即建即销，的一条连接 // 0 threadPool.generic().execute(pingSender); // 1/3 threadPool.schedule(TimeValue.timeValueMillis(scheduleDuration.millis() / 3), ThreadPool.Names.GENERIC, pingSender); // 2/3 threadPool.schedule(TimeValue.timeValueMillis(scheduleDuration.millis() / 3 * 2), ThreadPool.Names.GENERIC, pingSender); threadPool.schedule(scheduleDuration, ThreadPool.Names.GENERIC, new AbstractRunnable() &#123; @Override protected void doRun() throws Exception &#123; // 关闭临时连接 finishPingingRound(pingingRound); &#125; @Override public void onFailure(Exception e) &#123; logger.warn(\"unexpected error while finishing pinging round\", e); &#125; &#125;); 这里注意一点就是ping的连接不像其他那样由transportService 来保持长连接，而是即建即销，的一条连接。最后finishPingRound时则把这些临时连接关闭。 3.5.2 选举master再次回到findMaster()方法，上面ping完之后，我们拿到了一个fullPingResponses列表，这里有一个filter操作，如果我们启用了discovery.zen.master_election.ignore_non_master_pings则就会把那些node.master = false 那些节点都忽略掉： 123// filter responses // 如果我们启用了discovery.zen.master_election.ignore_non_master_pings则就会把那些node.master = false 那些节点都忽略掉： final List&lt;ZenPing.PingResponse&gt; pingResponses = filterPingResponses(fullPingResponses, masterElectionIgnoreNonMasters, logger); 紧接着就要从这些pingResponse里面收集其他节点当前的master节点是谁，最后拿到一个activeMasters的候选的名单，并把自己给去掉，Discovery的策略是非直到最后一刻都不会选自己为master（猜猜可能是预防脑裂吧）。 123456789101112131415161718192021222324252627282930 //从pingResponse列表里面收集其他节点当前的master节点是谁，最后拿到一个activeMasters的候选的名单 List&lt;DiscoveryNode&gt; activeMasters = new ArrayList&lt;&gt;(); for (ZenPing.PingResponse pingResponse : pingResponses) &#123; // We can't include the local node in pingMasters list, otherwise we may up electing ourselves without // any check / verifications from other nodes in ZenDiscover#DiscoveryNode() if (pingResponse.master() != null &amp;&amp; !localNode.equals(pingResponse.master())) &#123; // 添加master到activeMasters名单 activeMasters.add(pingResponse.master()); &#125; &#125;// 如果可选名单为空，就是大家刚刚启动，则进入选举环节 if (activeMasters.isEmpty()) &#123; // 如果有足够的候选人参与:由discovery.zen.minimum_master_nodes参数指定 if (electMaster.hasEnoughCandidates(masterCandidates)) &#123; // 具体的选举方法：非常简单，就是找到id为最小的节点 final ElectMasterService.MasterCandidate winner = electMaster.electMaster(masterCandidates); logger.trace(\"candidate &#123;&#125; won election\", winner); return winner.getNode(); &#125; else &#123; // if we don't have enough master nodes, we bail, because there are not enough master to elect from logger.warn(\"not enough master nodes discovered during pinging (found [&#123;&#125;], but needed [&#123;&#125;]), pinging again\", masterCandidates, electMaster.minimumMasterNodes()); return null; &#125; &#125; else &#123; assert !activeMasters.contains(localNode) : \"local node should never be elected as master when other nodes indicate an active master\"; // lets tie break between discovered nodes return electMaster.tieBreakActiveMasters(activeMasters); &#125; 接着就对这个候选列表判断，最理想就是列表为1，就证明你当前加入一个健康的集群中去，如果是有多个（正常情况下肯定不会有多个，除非你没有配置那个discovery.zen.minimum_master_nodes导致很多分治子群了）则在列表里面简单的选一个id号最小的（意思是不参乱了）。如果列表为空，就是大家都是刚启动，则进入选举环节，选举环节还是选出那个id最小的。 现在这个masterNode是定下来了，如果这个master是别人，则就简单的发送个join请求过去就好了，如果选出的master是你自己，那就还有一件很重要的事要做，还记得那个discovery.zen.minimum_master_nodes参数吗，一般要求这个值需要配成你的集群的cluster节点数的一半+1，以预防有脑裂，当前如果你选举出自己是master，那么你还需要等待 minimumMasterNodes() - 1 这么多个人join过来并认同你是master，那你才是真正的master，选举才结束。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// 上面通过findMaster()方法找到masterNode，但是这时还不能算是真正的master节点 // 如果选出了自己 if (clusterService.localNode().equals(masterNode)) &#123; // 需要的节点数是discovery.zen.minimum_master_nodes-1（因为masterNode自己不需要表决了） final int requiredJoins = Math.max(0, electMaster.minimumMasterNodes() - 1); // we count as one logger.debug(\"elected as master, waiting for incoming joins ([&#123;&#125;] needed)\", requiredJoins); nodeJoinController.waitToBeElectedAsMaster(requiredJoins, masterElectionWaitForJoinsTimeout, new NodeJoinController.ElectionCallback() &#123; @Override public void onElectedAsMaster(ClusterState state) &#123; joinThreadControl.markThreadAsDone(currentThread); // we only starts nodesFD if we are master (it may be that we received a cluster state while pinging) nodesFD.updateNodesAndPing(state); // start the nodes FD &#125; @Override public void onFailure(Throwable t) &#123; logger.trace(\"failed while waiting for nodes to join, rejoining\", t); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); &#125; &#125; ); &#125; else &#123; // 如果这个master是别人，则就简单的发送个join请求过去就好了 // process any incoming joins (they will fail because we are not the master) nodeJoinController.stopElectionContext(masterNode + \" elected\"); // send join request final boolean success = joinElectedMaster(masterNode); // finalize join through the cluster state update thread final DiscoveryNode finalMasterNode = masterNode; clusterService.submitStateUpdateTask(\"finalize_join (\" + masterNode + \")\", new LocalClusterUpdateTask() &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) throws Exception &#123; if (!success) &#123; // failed to join. Try again... joinThreadControl.markThreadAsDoneAndStartNew(currentThread); return unchanged(); &#125; if (currentState.getNodes().getMasterNode() == null) &#123; // Post 1.3.0, the master should publish a new cluster state before acking our join request. we now should have // a valid master. logger.debug(\"no master node is set, despite of join request completing. retrying pings.\"); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); return unchanged(); &#125; if (!currentState.getNodes().getMasterNode().equals(finalMasterNode)) &#123; return joinThreadControl.stopRunningThreadAndRejoin(currentState, \"master_switched_while_finalizing_join\"); &#125; // Note: we do not have to start master fault detection here because it's set at &#123;@link #processNextPendingClusterState &#125; // when the first cluster state arrives. joinThreadControl.markThreadAsDone(currentThread); return unchanged(); &#125; @Override public void onFailure(String source, @Nullable Exception e) &#123; logger.error(\"unexpected error while trying to finalize cluster join\", e); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); &#125; &#125;); &#125; 这里会有一个等待join超时配置，超时后还没有满足数量的join请求，则选举失败，需要新一轮选举。 3.6 什么时候触发Master选举 集群启动 Master 失效 非 Master 节点运行的 MasterFaultDetection 检测到 Master 失效,在其注册的 listener 中执行 handleMasterGone,执行 rejoin 操作,重新选主.注意,即使一个节点认为 Master 失效也会进入选主流程 3.7 为什么不用ZK？个人猜测可能是当时ZK不是很流行，应用不广泛。Elasticsearch第一版发布于2010年，zookeeper发布于2008年，时间间隔不是很长。 3.8 如何获取到最新的数据现在 Master 已成功当选,但是他未必有最新的 clusterState 信息,这些信息如何得到?gateway 模块负责 clusterState 持久化和恢复,Master 节点在当选后,会通过下面的流程获取到集群最新 clusterState: 枚举集群中有资格成为 Master 的节点列表 通过listGatewayMetaState获取这些节点上存储的 clusterState 对比这些节点的 clusterState 版本号,选择最新的作为 clusterState 并应用. 这一块将在后续的gateway源码分析模块中进行分析。 3.9 总结master选举四、同步状态选举流程结束后两个重要的小task就开始工作了，分别是MasterFaultDetection和NodesFaultDetection，这两个task很简单，就拿一个master的来看，唯一不同就是node的里面保存的是cluster里面所有的nodes。 这两个是在ZenDiscovery初始化的时候就初始化好的Listener：MasterNodeFailureListener和NodeFaultDetectionListener分别是实现了MasterFaultDetection和NodesFaultDetection的内部类Listener。 如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 private class MasterNodeFailureListener implements MasterFaultDetection.Listener &#123; @Override public void onMasterFailure(DiscoveryNode masterNode, Throwable cause, String reason) &#123; handleMasterGone(masterNode, cause, reason); &#125; &#125;private class NodeFaultDetectionListener extends NodesFaultDetection.Listener &#123; private final AtomicInteger pingsWhileMaster = new AtomicInteger(0); @Override public void onNodeFailure(DiscoveryNode node, String reason) &#123; handleNodeFailure(node, reason); &#125; @Override public void onPingReceived(final NodesFaultDetection.PingRequest pingRequest) &#123; // if we are master, we don't expect any fault detection from another node. If we get it // means we potentially have two masters in the cluster. if (!localNodeMaster()) &#123; pingsWhileMaster.set(0); return; &#125; if (pingsWhileMaster.incrementAndGet() &lt; maxPingsFromAnotherMaster) &#123; logger.trace(\"got a ping from another master &#123;&#125;. current ping count: [&#123;&#125;]\", pingRequest.masterNode(), pingsWhileMaster.get()); return; &#125; logger.debug(\"got a ping from another master &#123;&#125;. resolving who should rejoin. current ping count: [&#123;&#125;]\", pingRequest.masterNode(), pingsWhileMaster.get()); clusterService.submitStateUpdateTask(\"ping from another master\", new LocalClusterUpdateTask(Priority.IMMEDIATE) &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) throws Exception &#123; if (currentState.nodes().isLocalNodeElectedMaster()) &#123; pingsWhileMaster.set(0); return handleAnotherMaster(currentState, pingRequest.masterNode(), pingRequest.clusterStateVersion(), \"node fd ping\"); &#125; else &#123; return unchanged(); &#125; &#125; @Override public void onFailure(String source, Exception e) &#123; logger.debug(\"unexpected error during cluster state update task after pings from another master\", e); &#125; &#125;); &#125; &#125; 对与这两个类，我们只需要看run方法就行了 MasterFaultDetection#run() 1234567...// check if the master node did not get switched on us..., if it did, we simply return with no reschedule if (masterToPing.equals(MasterFaultDetection.this.masterNode())) &#123; // we don't stop on disconnection from master, we keep pinging it threadPool.schedule(pingInterval, ThreadPool.Names.SAME, MasterPinger.this); &#125;... 和findMaster（）里面的不一样就是这里不再用temp连接而是在threadPool里面的长连接，这里对错误进行分类，如果是一些业务错误则不受尝试次数的限制，如请求的节点根本不是master节点，请求的master不是自己的cluster等等，会直接调用notifyMasterFailure回调，如果是常规错误，则记录尝试次数，当错误次数超过了阈值，则调用notifyMasterFailure回调。 接着看MasterNodeFailureListener的handleMasterGone()方法 123456789101112131415161718192021222324252627282930313233343536private void handleMasterGone(final DiscoveryNode masterNode, final Throwable cause, final String reason) &#123; // 状态不是已经开始，则直接返回 if (lifecycleState() != Lifecycle.State.STARTED) &#123; // not started, ignore a master failure return; &#125; // master是自己也直接返回 if (localNodeMaster()) &#123; // we might get this on both a master telling us shutting down, and then the disconnect failure return; &#125; logger.info((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"master_left [&#123;&#125;], reason [&#123;&#125;]\", masterNode, reason), cause); clusterService.submitStateUpdateTask(\"master_failed (\" + masterNode + \")\", new LocalClusterUpdateTask(Priority.IMMEDIATE) &#123; @Override public ClusterTasksResult&lt;LocalClusterUpdateTask&gt; execute(ClusterState currentState) &#123; if (!masterNode.equals(currentState.nodes().getMasterNode())) &#123; // master got switched on us, no need to send anything return unchanged(); &#125; // flush any pending cluster states from old master, so it will not be set as master again publishClusterState.pendingStatesQueue().failAllStatesAndClear(new ElasticsearchException(\"master left [&#123;&#125;]\", reason)); // 重新进入rejion流程 return rejoin(currentState, \"master left (reason = \" + reason + \")\"); &#125; @Override public void onFailure(String source, Exception e) &#123; logger.error((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(\"unexpected failure during [&#123;&#125;]\", source), e); &#125; &#125;); &#125; 在前面分析的Node初始化时，注册了clusterState的发布方法 1clusterService.setClusterStatePublisher(discovery::publish); 我们来看看publish()方法 12345678910111213141516171819202122public void publish(ClusterChangedEvent clusterChangedEvent, AckListener ackListener) &#123; if (!clusterChangedEvent.state().getNodes().isLocalNodeElectedMaster()) &#123; throw new IllegalStateException(\"Shouldn't publish state when not master\"); &#125; try &#123; // 调用PublishClusterStateAction的publish()方法 publishClusterState.publish(clusterChangedEvent, electMaster.minimumMasterNodes(), ackListener); &#125; catch (FailedToCommitClusterStateException t) &#123; // cluster service logs a WARN message logger.debug(\"failed to publish cluster state version [&#123;&#125;] (not enough nodes acknowledged, min master nodes [&#123;&#125;])\", clusterChangedEvent.state().version(), electMaster.minimumMasterNodes()); submitRejoin(\"zen-disco-failed-to-publish\"); throw t; &#125; // update the set of nodes to ping after the new cluster state has been published nodesFD.updateNodesAndPing(clusterChangedEvent.state()); // clean the pending cluster queue - we are currently master, so any pending cluster state should be failed // note that we also clean the queue on master failure (see handleMasterGone) but a delayed cluster state publish // from a stale master can still make it in the queue during the election (but not be committed) publishClusterState.pendingStatesQueue().failAllStatesAndClear(new ElasticsearchException(\"elected as master\")); &#125; 调用PublishClusterStateAction的publish()方法，在PublishClusterStateAction初始化的时候，会调用CommitClusterStateRequestHandler的构造方法，然后传递给TransportService的registerRequestHandler方法，最后会处理到一个pendingStatesQueue队列。 pendingStatesQueue会保存着每个待提交的state，并且也会提供最新的commit 的state给其他请求。 123456789101112131415161718192021222324252627282930313233343536private class CommitClusterStateRequestHandler implements TransportRequestHandler&lt;CommitClusterStateRequest&gt; &#123; @Override public void messageReceived(CommitClusterStateRequest request, final TransportChannel channel) throws Exception &#123; handleCommitRequest(request, channel); &#125; &#125; protected void handleCommitRequest(CommitClusterStateRequest request, final TransportChannel channel) &#123; final ClusterState state = pendingStatesQueue.markAsCommitted(request.stateUUID, new PendingClusterStatesQueue.StateProcessedListener() &#123; @Override public void onNewClusterStateProcessed() &#123; try &#123; // send a response to the master to indicate that this cluster state has been processed post committing it. channel.sendResponse(TransportResponse.Empty.INSTANCE); &#125; catch (Exception e) &#123; logger.debug(\"failed to send response on cluster state processed\", e); onNewClusterStateFailed(e); &#125; &#125; @Override public void onNewClusterStateFailed(Exception e) &#123; try &#123; channel.sendResponse(e); &#125; catch (Exception inner) &#123; inner.addSuppressed(e); logger.debug(\"failed to send response on cluster state processed\", inner); &#125; &#125; &#125;); if (state != null) &#123; newPendingClusterStatelistener.onNewClusterState(\"master \" + state.nodes().getMasterNode() + \" committed version [\" + state.version() + \"]\"); &#125; &#125; 而发布clusterChangedEvent则交给了PublishClusterStateAction主要逻辑在innerPublish方法 1234567891011121314151617181920212223242526272829303132333435363738394041private void innerPublish(final ClusterChangedEvent clusterChangedEvent, final Set&lt;DiscoveryNode&gt; nodesToPublishTo, final SendingController sendingController, final boolean sendFullVersion, final Map&lt;Version, BytesReference&gt; serializedStates, final Map&lt;Version, BytesReference&gt; serializedDiffs) &#123; final ClusterState clusterState = clusterChangedEvent.state(); final ClusterState previousState = clusterChangedEvent.previousState(); final TimeValue publishTimeout = discoverySettings.getPublishTimeout(); final long publishingStartInNanos = System.nanoTime(); for (final DiscoveryNode node : nodesToPublishTo) &#123; // try and serialize the cluster state once (or per version), so we don't serialize it // per node when we send it over the wire, compress it while we are at it... // we don't send full version if node didn't exist in the previous version of cluster state if (sendFullVersion || !previousState.nodes().nodeExists(node)) &#123; sendFullClusterState(clusterState, serializedStates, node, publishTimeout, sendingController); &#125; else &#123; // 是在ES2.x之后才有的，目的是为了减少网络带宽 sendClusterStateDiff(clusterState, serializedDiffs, serializedStates, node, publishTimeout, sendingController); &#125; &#125; sendingController.waitForCommit(discoverySettings.getCommitTimeout()); try &#123; long timeLeftInNanos = Math.max(0, publishTimeout.nanos() - (System.nanoTime() - publishingStartInNanos)); final BlockingClusterStatePublishResponseHandler publishResponseHandler = sendingController.getPublishResponseHandler(); sendingController.setPublishingTimedOut(!publishResponseHandler.awaitAllNodes(TimeValue.timeValueNanos(timeLeftInNanos))); if (sendingController.getPublishingTimedOut()) &#123; DiscoveryNode[] pendingNodes = publishResponseHandler.pendingNodes(); // everyone may have just responded if (pendingNodes.length &gt; 0) &#123; logger.warn(\"timed out waiting for all nodes to process published state [&#123;&#125;] (timeout [&#123;&#125;], pending nodes: &#123;&#125;)\", clusterState.version(), publishTimeout, pendingNodes); &#125; &#125; &#125; catch (InterruptedException e) &#123; // ignore &amp; restore interrupt Thread.currentThread().interrupt(); &#125; &#125; 在ES2.x 之后支持了发送临近版本的diff来同步状态，目的为了省网络带宽，点进去ClusterState类可以发现里面的状态信息量还是不少，不过diff 需要你的版本和目前的最新的版本只相差一个版本，如果你要从1跳到3需要发送full的状态。sendFullClusterState 和sendClusterStateDiff都会调用底层transportService来真正发送状态，而状态记录通过一个sendingController来维护，没接收到ack或者timeout都会让controller来check是否达到了minMasterNodes-1，达到则标记这次的状态推送commited，其余情况都会抛错。 这里一定需要注意，Publish状态分成两个阶段，首先是sendNotification 1234567891011121314151617181920212223242526272829private void sendClusterStateToNode(final ClusterState clusterState, BytesReference bytes, final DiscoveryNode node, final TimeValue publishTimeout, final SendingController sendingController, final boolean sendDiffs, final Map&lt;Version, BytesReference&gt; serializedStates) &#123; try &#123; // -&gt; no need to put a timeout on the options here, because we want the response to eventually be received // and not log an error if it arrives after the timeout // -&gt; no need to compress, we already compressed the bytes TransportRequestOptions options = TransportRequestOptions.builder() .withType(TransportRequestOptions.Type.STATE).withCompress(false).build(); transportService.sendRequest(node, SEND_ACTION_NAME, new BytesTransportRequest(bytes, node.getVersion()), options, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) &#123; @Override public void handleResponse(TransportResponse.Empty response) &#123; if (sendingController.getPublishingTimedOut()) &#123; logger.debug(\"node &#123;&#125; responded for cluster state [&#123;&#125;] (took longer than [&#123;&#125;])\", node, clusterState.version(), publishTimeout); &#125; sendingController.onNodeSendAck(node); &#125; .... &#125; &#125; 就是master先向所有节点发送这个状态，需要等minMasterNodes确认了这个通知，master节点才会把这个状态mark成commited，再sendCommitToNode() 告知所有节点把commited这个状态。 1234567891011121314public synchronized void onNodeSendAck(DiscoveryNode node) &#123; if (committed) &#123; assert sendAckedBeforeCommit.isEmpty(); sendCommitToNode(node, clusterState, this); &#125; else if (committedOrFailed()) &#123; logger.trace(\"ignoring ack from [&#123;&#125;] for cluster state version [&#123;&#125;]. already failed\", node, clusterState.version()); &#125; else &#123; // we're still waiting sendAckedBeforeCommit.add(node); if (node.isMasterNode()) &#123; checkForCommitOrFailIfNoPending(node); &#125; &#125; &#125; 其他Node消息处理也是在该类当中，大致流程就不再详细查看了，入口在 1234transportService.registerRequestHandler(SEND_ACTION_NAME, BytesTransportRequest::new, ThreadPool.Names.SAME, false, false, new SendClusterStateRequestHandler()); transportService.registerRequestHandler(COMMIT_ACTION_NAME, CommitClusterStateRequest::new, ThreadPool.Names.SAME, false, false, new CommitClusterStateRequestHandler()); 都会调用底层transportService来真正发送状态。 五、Discovery模块总结从上面的源码分析我们知道，Discovery模块是ES的核心模块，它对ES集群进行master选举、状态发布和更新。","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]},{"title":"Redis面试题整理","slug":"redis/redis","date":"2018-05-26T04:12:57.000Z","updated":"2018-06-20T01:13:19.831Z","comments":true,"path":"2018/05/26/redis/redis/","link":"","permalink":"http://yoursite.com/2018/05/26/redis/redis/","excerpt":"","text":"1、Redis为什么单线程的，能说说它的原理吗Redis使用了单线程架构和I/O多路复用模型来实现高性能的内存数据库服务。Redis使用了单线程架构，预防了多线程可能产生的竞争问题，但是也会引入另外的问题。Redis单线程架构导致无法充分利用CPU多核特性，通常的做法是在一台机器上部署多个Redis实例。 那么Redis使用单线程模型，为什么还那么快： 第一，纯内存访问，Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，这是Redis达到每秒万级别访问的重要基础。第二，非阻塞I/O，Redis使用epoll作为I/O多路复用技术的实现，再加上Redis自身的事件处理模型将epoll中的连接、读写、关闭都转换为事件，不在网络I/O上浪费过多的时间。 第三，单线程避免了线程切换和竞态产生的消耗。 2、mySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据Redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 3、缓存穿透可以介绍⼀一下么？你认为应该如何解决这个问题缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库中查询。 解决思路： 1，如果查询数据库也为空，直接设置一个默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。 2，根据缓存数据Key的规则。例如我们公司是做机顶盒的，缓存数据以Mac为Key，Mac是有规则，如果不符合规则就过滤掉，这样可以过滤一部分查询。在做缓存规划的时候，Key有一定规则的话，可以采取这种办法。这种办法只能缓解一部分的压力，过滤和系统无关的查询，但是无法根治。 3，采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的BitSet中，不存在的数据将会被拦截掉，从而避免了对底层存储系统的查询压力。关于布隆过滤器，详情查看：基于BitSet的布隆过滤器(Bloom Filter) 大并发的缓存穿透会导致缓存雪崩。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[]},{"title":"elasticsearch源码分析(二)--启动","slug":"Elasticsearch/elasticsearch源码分析(二)--启动","date":"2018-05-25T04:10:57.000Z","updated":"2018-06-26T08:44:42.593Z","comments":true,"path":"2018/05/25/Elasticsearch/elasticsearch源码分析(二)--启动/","link":"","permalink":"http://yoursite.com/2018/05/25/Elasticsearch/elasticsearch源码分析(二)--启动/","excerpt":"","text":"由于最近半年来一直在使用Elasticsearch来做全文检索和ELK统一日志工作，对于ES还是觉得需要细细研究，才能感受到它的魅力，才能有所提高。 我们先提出几个问题： 启动入口在哪个类？ 启动需要做哪些初始化工作？ 如何加载配置文件？ 一、怎么找启动入口在哪个类看源码最头疼的事情就是找入口，相信很多刚开始也是这样，面对那么多模块中的类，很难找到一个切入点，我刚开始看也是这样，对于这样的问题，其实还是自己的积累不够，多学习就是了。 我们先来看看启动的脚本elasticsearch.bat或者elasticsearch.sh 1234567@echo off忽略其他%JAVA% %ES_JAVA_OPTS% %ES_PARAMS% -cp \"%ES_CLASSPATH%\" \"org.elasticsearch.bootstrap.Elasticsearch\" !newparams!ENDLOCAL 看到了org.elasticsearch.bootstrap.Elasticsearch这个类，不用想就是它的启动类。 二、Elasticsearch类做了什么事情我们先来猜想一下，我们下载完Elasticsearch的安装包，一般有两种部署方式：单机部署和集群部署 2.1 单机部署一般我们会修改{Elasticsearch_home}\\config下的elasticsearch.yml文件和jvm.options 在elasticsearch.yml中配置集群名称、节点名称、日志存放路径、数据存放路径、网络IP、http端口（9200）、Netty端口（9300）等 同时还会去初始化一些module，如下图 2.2 集群部署我们会在单机部署的基础上，增加Discovery模块（集群发现）的配置、 有哪些节点参与到集群当中：discovery.zen.ping.unicast.hosts: [“host1”, “host2”] 需要有几个皇子在场才可以选举投票出master：discovery.zen.minimum_master_nodes: 3 2.3 启动流程猜想通过上述分析我们知道，ES集群启动会做一些初始化工作、加载配置文件，加载一下扩展插件，如果是集群启动，还会进行master选举，master选举需要有足够多的节点参与投票，这个参数是可以指定。 三、启动源码分析3.1 Elasticsearch类图 3.2 Elasticsearch#main()方法我们先来看看org.elasticsearch.bootstrap.Elasticsearch#main()方法 12345678910111213141516171819202122public static void main(final String[] args) throws Exception &#123; // we want the JVM to think there is a security manager installed so that if internal policy decisions that would be based on the // presence of a security manager or lack thereof act as if there is a security manager present (e.g., DNS cache policy) System.setSecurityManager(new SecurityManager() &#123; @Override public void checkPermission(Permission perm) &#123; // grant all permissions so that we can later set the security manager to the one that we want &#125; &#125;); LogConfigurator.registerErrorListener(); // 调用构造器 final Elasticsearch elasticsearch = new Elasticsearch(); // 调用main方法，执行完后返回一个状态 int status = main(args, elasticsearch, Terminal.DEFAULT); // 判断状态是否启动成功 if (status != ExitCodes.OK) &#123; exit(status); &#125; &#125; static int main(final String[] args, final Elasticsearch elasticsearch, final Terminal terminal) throws Exception &#123; return elasticsearch.main(args, terminal); &#125; 通过上面的类图关系，我们知道Elasticsearch是一个Command，就是一开始先设置了一个SecurityManager，做一些检查checkPermission(Permission perm)，因此主要还是增加一些启停的hook，配置日志输出，用意看注释吧，接着打印了一些基本参数后则进入init方法，在Command#execute(terminal, options)方法里会调用Bootstrap.init(!daemonize, pidFile, quiet, initialEnv); 12345678910111213141516171819202122public final int main(String[] args, Terminal terminal) throws Exception &#123; if (addShutdownHook()) &#123; shutdownHookThread.set(new Thread(() -&gt; &#123; try &#123; this.close(); &#125; catch (final IOException e) &#123; try ( StringWriter sw = new StringWriter(); PrintWriter pw = new PrintWriter(sw)) &#123; e.printStackTrace(pw); terminal.println(sw.toString()); &#125; catch (final IOException impossible) &#123; // StringWriter#close declares a checked IOException from the Closeable interface but the Javadocs for StringWriter // say that an exception here is impossible throw new AssertionError(impossible); &#125; &#125; &#125;)); // 当JVM关闭时，会执行系统中已经设置的所有通过方法addShutdownHook添加的钩子， // 当系统执行完这些钩子后，jvm才会关闭 Runtime.getRuntime().addShutdownHook(shutdownHookThread.get()); &#125; 配置日志输出Command#main()方法中 12345// 配置日志输出// initialize default for es.logger.level because we will not read the log4j2.propertiesfinal String loggerLevel = System.getProperty(&quot;es.logger.level&quot;, Level.INFO.name());final Settings settings = Settings.builder().put(&quot;logger.level&quot;, loggerLevel).build();LogConfigurator.configureWithoutConfig(settings); LogConfigurator#configureWithoutConfig()方法 123456public static void configureWithoutConfig(final Settings settings) &#123; Objects.requireNonNull(settings); // we initialize the status logger immediately otherwise Log4j will complain when we try to get the context configureStatusLogger(); configureLoggerLevels(settings); &#125; 在Command#mainWithoutErrorHandling(args, terminal)中执行Command，同时会抛出所有的异常给Command#main()方法，真正调用execute(terminal, options)方法执行操作，这是一个抽象方法，通过我们的类图,它的实现类应该是EnvironmentAwareCommand#execute() 123456789101112131415161718192021222324252627@Override protected void execute(Terminal terminal, OptionSet options) throws Exception &#123; // 将配置信息设置到HashMap中 final Map&lt;String, String&gt; settings = new HashMap&lt;&gt;(); for (final KeyValuePair kvp : settingOption.values(options)) &#123; if (kvp.value.isEmpty()) &#123; throw new UserException(ExitCodes.USAGE, \"setting [\" + kvp.key + \"] must not be empty\"); &#125; if (settings.containsKey(kvp.key)) &#123; final String message = String.format( Locale.ROOT, \"setting [%s] already set, saw [%s] and [%s]\", kvp.key, settings.get(kvp.key), kvp.value); throw new UserException(ExitCodes.USAGE, message); &#125; settings.put(kvp.key, kvp.value); &#125; // 检查了elasticsearch的三个环境参数： putSystemPropertyIfSettingIsMissing(settings, \"path.conf\", \"es.path.conf\"); putSystemPropertyIfSettingIsMissing(settings, \"path.data\", \"es.path.data\"); putSystemPropertyIfSettingIsMissing(settings, \"path.home\", \"es.path.home\"); putSystemPropertyIfSettingIsMissing(settings, \"path.logs\", \"es.path.logs\"); // 调用execute方法 execute(terminal, options, createEnv(terminal, settings)); &#125; 该方法也是一个抽象方法，它有很多实现类 在该方法中，会先调用createEnv(terminal, settings)设置环境参数，使用该方法来加载配置文件信息 1234/** Create an &#123;@link Environment&#125; for the command to use. Overrideable for tests. */ protected Environment createEnv(Terminal terminal, Map&lt;String, String&gt; settings) &#123; return InternalSettingsPreparer.prepareEnvironment(Settings.EMPTY, terminal, settings); &#125; 那么这些配置信息怎么跟节点信息关联呢？ 3.3 Elasticsearch#execute()方法直接来看Elasticsearch#execute()方法做了什么？ 12345678910111213141516171819202122232425262728protected void execute(Terminal terminal, OptionSet options, Environment env) throws UserException &#123; // 检查参数是否为空 if (options.nonOptionArguments().isEmpty() == false) &#123; throw new UserException(ExitCodes.USAGE, \"Positional arguments not allowed, found \" + options.nonOptionArguments()); &#125; if (options.has(versionOption)) &#123; if (options.has(daemonizeOption) || options.has(pidfileOption)) &#123; throw new UserException(ExitCodes.USAGE, \"Elasticsearch version option is mutually exclusive with any other option\"); &#125; terminal.println(\"Version: \" + org.elasticsearch.Version.CURRENT + \", Build: \" + Build.CURRENT.shortHash() + \"/\" + Build.CURRENT.date() + \", JVM: \" + JvmInfo.jvmInfo().version()); return; &#125; // 是否以守护线程启动（后台启动 -d） final boolean daemonize = options.has(daemonizeOption); // 进程文件 final Path pidFile = pidfileOption.value(options); // final boolean quiet = options.has(quietOption); try &#123; // 执行初始化方法 init(daemonize, pidFile, quiet, env); &#125; catch (NodeValidationException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125; &#125; 该方法主要是检查一些参数，然后调用Elasticsearch#init(daemonize, pidFile, quiet, env)方法，在方法里会调用Bootstrap.init(!daemonize, pidFile, quiet, initialEnv)，而这个方法才是Elasticsearch真正去启动ES。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * This method is invoked by &#123;@link Elasticsearch#main(String[])&#125; to startup elasticsearch. */ static void init( final boolean foreground, final Path pidFile, final boolean quiet, final Environment initialEnv) throws BootstrapException, NodeValidationException, UserException &#123; // Set the system property before anything has a chance to trigger its use initLoggerPrefix(); // force the class initializer for BootstrapInfo to run before // the security manager is installed BootstrapInfo.init(); INSTANCE = new Bootstrap(); final SecureSettings keystore = loadSecureSettings(initialEnv); Environment environment = createEnvironment(foreground, pidFile, keystore, initialEnv.settings()); try &#123; // 配置日志输出 LogConfigurator.configure(environment); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; // 检查自定义配置文件 checkForCustomConfFile(); // 检查是否配置错误 checkConfigExtension(environment.configExtension()); // 如果pidFile文件不为空，则创建pid文件，会在磁盘上持久化一个记录应用pid的文件 if (environment.pidFile() != null) &#123; try &#123; PidFile.create(environment.pidFile(), true); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; &#125; //通过参数foreground和quiet来控制日志输出 final boolean closeStandardStreams = (foreground == false) || quiet; try &#123; if (closeStandardStreams) &#123; final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; closeSystOut(); &#125; // fail if somebody replaced the lucene jars checkLucene(); // install the default uncaught exception handler; must be done before security is // initialized as we do not want to grant the runtime permission // setDefaultUncaughtExceptionHandler // 初始化节点信息 Thread.setDefaultUncaughtExceptionHandler( new ElasticsearchUncaughtExceptionHandler(() -&gt; Node.NODE_NAME_SETTING.get(environment.settings()))); // 调用Bootstrap的setup方法和start方法 INSTANCE.setup(true, environment); try &#123; // any secure settings must be read during node construction IOUtils.close(keystore); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; // 调用Bootstrap的start方法 INSTANCE.start(); if (closeStandardStreams) &#123; closeSysError(); &#125; ... 略 参数详解 foreground：标识elasticsearch是否是作为后台守护进程启动的， pidFile：通过parser解析args后得到，实际是解析了默认命令行参数（verbose，E,silent，version，help，quiet，daemonize，pidfile） quiet：同上 initialEnv：Environment实例化的环境参数对象，保存了一些类似于repoFile，configFile，pluginsFile，binFile，libFile等参数。 通过上述的源码阅读，我们发现在该方法中： 主要工作 首先会实例化一个Bootstrap对象 配置log输出器 创建pid文件，会在磁盘上持久化一个记录应用pid的文件 通过参数foreground和quiet来控制日志输出 调用Bootstrap的setup方法和start方法 3.5 Bootstrap#setup()方法1setup(boolean addShutdownHook, Environment environment)throws BootstrapException 该方法主要工作 通过environment生成本地插件控制器 12345678Settings settings = environment.settings(); try &#123; // Spawner类是一个Environment本地插件控制器 spawner.spawnNativePluginControllers(environment); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; 初始化本地资源 12345initializeNatives( environment.tmpFile(), BootstrapSettings.MEMORY_LOCK_SETTING.get(settings), BootstrapSettings.SYSTEM_CALL_FILTER_SETTING.get(settings), BootstrapSettings.CTRLHANDLER_SETTING.get(settings)); ​ 在安全管理器安装之前初始化探针 1initializeProbes(); ​ 添加关闭钩子 1234567891011121314if (addShutdownHook) &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; try &#123; IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to stop node\", ex); &#125; &#125; &#125;); &#125; ​ 检查jar重复 123456try &#123; // look for jar hell,检查jar重复 JarHell.checkJarHell(); &#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e); &#125; ​ 在安全管理器安装之前配置日志输出器 1234567// install SM after natives, shutdown hooks, etc. // 安装安全管理器 try &#123; Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); &#125; catch (IOException | NoSuchAlgorithmException e) &#123; throw new BootstrapException(e); &#125; ​ 安装安全管理器 1234567// install SM after natives, shutdown hooks, etc. // 安装安全管理器 try &#123; Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); &#125; catch (IOException | NoSuchAlgorithmException e) &#123; throw new BootstrapException(e); &#125; ​ 通过参数environment实例化Node 123456789// 通过参数environment实例化Node node = new Node(environment) &#123; @Override protected void validateNodeBeforeAcceptingRequests( final Settings settings, final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException &#123; BootstrapChecks.check(settings, boundTransportAddress, checks); &#125; &#125;; ​ 3.6 Bootstrap#start()方法1234private void start() throws NodeValidationException &#123; node.start(); keepAliveThread.start(); &#125; 主要工作 启动已经实例化的Node 启动keepAliveThread 线程，这个线程在Bootstrap初始化的时候就已经实例化了，该线程创建了一个计数为1的CountDownLatch，目的是在启动完成后能顺利添加关闭钩子，而这句： 1Runtime.getRuntime().addShutdownHook(new Thread()) 意思就是在jvm中增加一个关闭的钩子，当jvm关闭的时候，会执行系统中已经设置的所有通过方法addShutdownHook添加的钩子，当系统执行完这些钩子后，jvm才会关闭。所以这些钩子可以在jvm关闭的时候进行内存清理、对象销毁等操作。可以看到启动的重点在setup方法中，启动过后就是Node的事了。 keepAliveThhread线程 123456789101112131415161718192021222324private final CountDownLatch keepAliveLatch = new CountDownLatch(1);/** creates a new instance */ Bootstrap() &#123; // 在构造器中就创建keepAliveThread线程 keepAliveThread = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; keepAliveLatch.await(); &#125; catch (InterruptedException e) &#123; // bail out &#125; &#125; &#125;, \"elasticsearch[keepAlive/\" + Version.CURRENT + \"]\"); keepAliveThread.setDaemon(false); // keep this thread alive (non daemon thread) until we shutdown Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; // 这里的钩子执行完毕，才会执行完keepAliveThread线程的run()方法 keepAliveLatch.countDown(); &#125; &#125;); &#125; 3.4 Node类源码解读我们先不看源码，如果是你，会怎么去设计这个Node类？会怎么去加载配置文件信息？ 猜想，我们启动ES都是一个节点Node，如果是集群，会有多个Node，那么我们应该也是通过Node来加载配置文件，加载完配置文件构造一个Config对象，最后初始化一个Node对象。 继续猜想，Node应该是包含一些基本信息、全局环境配置Setting和Environment，节点环境NodeEnvironment、是否为master、是否可以参与投票等。 问题：这些信息设置完毕，如何启动、如何停止？如何加载插件？ 验证猜想，查看类的定义信息 3.4.1 Node初始化我们前面通过分析Bootstrap#setup()方法知道，Node的实例化是在该方法中调用 new Node(environment)进行的，节点的启动是在Bootstrap#start()方法中调用Node#start()方法进行启动的。 123456789// 通过参数environment实例化Node node = new Node(environment) &#123; @Override protected void validateNodeBeforeAcceptingRequests( final Settings settings, final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException &#123; BootstrapChecks.check(settings, boundTransportAddress, checks); &#125; &#125;; 使用google的注入框架Guice的Injector进行注入与获取实例。elasticsearch里面的组件都是用上面的方法进行模块化管理，elasticsearch对guice进行了封装，通过ModulesBuilder类构建elasticsearch的模块： 123456789101112131415161718ModulesBuilder modules = new ModulesBuilder(); // plugin modules must be added here, before others or we can get crazy injection errors... for (Module pluginModule : pluginsService.createGuiceModules()) &#123; modules.add(pluginModule); &#125; final MonitorService monitorService = new MonitorService(settings, nodeEnvironment, threadPool); modules.add(new NodeModule(this, monitorService)); ClusterModule clusterModule = new ClusterModule(settings, clusterService, pluginsService.filterPlugins(ClusterPlugin.class)); modules.add(clusterModule); IndicesModule indicesModule = new IndicesModule(pluginsService.filterPlugins(MapperPlugin.class)); modules.add(indicesModule); SearchModule searchModule = new SearchModule(settings, false, pluginsService.filterPlugins(SearchPlugin.class)); CircuitBreakerService circuitBreakerService = createCircuitBreakerService(settingsModule.getSettings(), settingsModule.getClusterSettings()); resourcesToClose.add(circuitBreakerService);... Node的实例化主要工作： 设置初始化信息：nodeEnvironment 123456789101112131415161718192021222324252627try &#123; Settings tmpSettings = Settings.builder().put(environment.settings()) .put(Client.CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE).build(); tmpSettings = TribeService.processSettings(tmpSettings); // create the node environment as soon as possible, to recover the node id and enable logging try &#123; nodeEnvironment = new NodeEnvironment(tmpSettings, environment); resourcesToClose.add(nodeEnvironment); &#125; catch (IOException ex) &#123; throw new IllegalStateException(\"Failed to create node environment\", ex); &#125; final boolean hadPredefinedNodeName = NODE_NAME_SETTING.exists(tmpSettings); Logger logger = Loggers.getLogger(Node.class, tmpSettings); final String nodeId = nodeEnvironment.nodeId(); tmpSettings = addNodeNameIfNeeded(tmpSettings, nodeId); if (DiscoveryNode.nodeRequiresLocalStorage(tmpSettings)) &#123; checkForIndexDataInDefaultPathData(tmpSettings, nodeEnvironment, logger); &#125; // this must be captured after the node name is possibly added to the settings final String nodeName = NODE_NAME_SETTING.get(tmpSettings); if (hadPredefinedNodeName == false) &#123; logger.info(\"node name [&#123;&#125;] derived from node ID [&#123;&#125;]; set [&#123;&#125;] to override\", nodeName, nodeId, NODE_NAME_SETTING.getKey()); &#125; else &#123; logger.info(\"node name [&#123;&#125;], node ID [&#123;&#125;]\", nodeName, nodeId); &#125; 打印JVM信息 ​ 初始化pluginsService类 1this.pluginsService = new PluginsService(tmpSettings, environment.modulesFile(), environment.pluginsFile(), classpathPlugins); environment(这里会加载配置文件) 12this.environment = new Environment(this.settings);Environment.assertEquivalent(environment, this.environment); Executors 和threadPool 1234567final List&lt;ExecutorBuilder&lt;?&gt;&gt; executorBuilders = pluginsService.getExecutorBuilders(settings);final ThreadPool threadPool = new ThreadPool(settings, executorBuilders.toArray(new ExecutorBuilder[0]));resourcesToClose.add(() -&gt; ThreadPool.terminate(threadPool, 10, TimeUnit.SECONDS));// adds the context to the DeprecationLogger so that it does not need to be injected everywhereDeprecationLogger.setThreadContext(threadPool.getThreadContext());resourcesToClose.add(() -&gt; DeprecationLogger.removeThreadContext(threadPool.getThreadContext())); 我们来看es线程池做了什么？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public ThreadPool(final Settings settings, final ExecutorBuilder&lt;?&gt;... customBuilders) &#123; super(settings); assert Node.NODE_NAME_SETTING.exists(settings); // 将构造好的线程池添加到HashMap中，key是线程池的名称，value是ExecutorBuilder // 每一个线程都是通过ExecutorBuilder来构造 final Map&lt;String, ExecutorBuilder&gt; builders = new HashMap&lt;&gt;(); final int availableProcessors = EsExecutors.boundedNumberOfProcessors(settings); final int halfProcMaxAt5 = halfNumberOfProcessorsMaxFive(availableProcessors); final int halfProcMaxAt10 = halfNumberOfProcessorsMaxTen(availableProcessors); final int genericThreadPoolMax = boundedBy(4 * availableProcessors, 128, 512); builders.put(Names.GENERIC, new ScalingExecutorBuilder(Names.GENERIC, 4, genericThreadPoolMax, TimeValue.timeValueSeconds(30))); builders.put(Names.INDEX, new FixedExecutorBuilder(settings, Names.INDEX, availableProcessors, 200)); builders.put(Names.BULK, new FixedExecutorBuilder(settings, Names.BULK, availableProcessors, 200)); // now that we reuse bulk for index/delete ops builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, availableProcessors, 1000)); builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(availableProcessors), 1000)); builders.put(Names.MANAGEMENT, new ScalingExecutorBuilder(Names.MANAGEMENT, 1, 5, TimeValue.timeValueMinutes(5))); // no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded // the assumption here is that the listeners should be very lightweight on the listeners side builders.put(Names.LISTENER, new FixedExecutorBuilder(settings, Names.LISTENER, halfProcMaxAt10, -1)); builders.put(Names.FLUSH, new ScalingExecutorBuilder(Names.FLUSH, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5))); builders.put(Names.REFRESH, new ScalingExecutorBuilder(Names.REFRESH, 1, halfProcMaxAt10, TimeValue.timeValueMinutes(5))); builders.put(Names.WARMER, new ScalingExecutorBuilder(Names.WARMER, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5))); builders.put(Names.SNAPSHOT, new ScalingExecutorBuilder(Names.SNAPSHOT, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5))); builders.put(Names.FETCH_SHARD_STARTED, new ScalingExecutorBuilder(Names.FETCH_SHARD_STARTED, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5))); builders.put(Names.FORCE_MERGE, new FixedExecutorBuilder(settings, Names.FORCE_MERGE, 1, -1)); builders.put(Names.FETCH_SHARD_STORE, new ScalingExecutorBuilder(Names.FETCH_SHARD_STORE, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5))); for (final ExecutorBuilder&lt;?&gt; builder : customBuilders) &#123; if (builders.containsKey(builder.name())) &#123; throw new IllegalArgumentException(\"builder with name [\" + builder.name() + \"] already exists\"); &#125; builders.put(builder.name(), builder); &#125; this.builders = Collections.unmodifiableMap(builders); threadContext = new ThreadContext(settings); final Map&lt;String, ExecutorHolder&gt; executors = new HashMap&lt;&gt;(); for (@SuppressWarnings(\"unchecked\") final Map.Entry&lt;String, ExecutorBuilder&gt; entry : builders.entrySet()) &#123; final ExecutorBuilder.ExecutorSettings executorSettings = entry.getValue().getSettings(settings); final ExecutorHolder executorHolder = entry.getValue().build(executorSettings, threadContext); if (executors.containsKey(executorHolder.info.getName())) &#123; throw new IllegalStateException(\"duplicate executors with name [\" + executorHolder.info.getName() + \"] registered\"); &#125; logger.debug(\"created thread pool: &#123;&#125;\", entry.getValue().formatInfo(executorHolder.info)); executors.put(entry.getKey(), executorHolder); &#125; executors.put(Names.SAME, new ExecutorHolder(DIRECT_EXECUTOR, new Info(Names.SAME, ThreadPoolType.DIRECT))); this.executors = unmodifiableMap(executors); // 最后创建一个1线程的scheduler来执行定时任务 this.scheduler = new ScheduledThreadPoolExecutor(1, EsExecutors.daemonThreadFactory(settings, \"scheduler\"), new EsAbortPolicy()); this.scheduler.setExecuteExistingDelayedTasksAfterShutdownPolicy(false); this.scheduler.setContinueExistingPeriodicTasksAfterShutdownPolicy(false); this.scheduler.setRemoveOnCancelPolicy(true); TimeValue estimatedTimeInterval = ESTIMATED_TIME_INTERVAL_SETTING.get(settings); // 最后创建一个执行timer的线程 this.cachedTimeThread = new CachedTimeThread(EsExecutors.threadName(settings, \"[timer]\"), estimatedTimeInterval.millis()); this.cachedTimeThread.start(); &#125; 原来在ES的threadPool中，根据不同的类型分别分配了不同线程数的一个线程池，而executor由一个executorBuilder来提供，所以submit task的时候也需要指定不同的Name。最后创建一个1线程的scheduler来执行定时任务。最后创建一个执行timer的线程。 再继续往下看Node的构造方法就会看到接下来会new 一堆的services和modules，这里就不一一过了，其共性就是都会绑定刚刚创建的threadPool，已经也会绑定必要的services，某些module本身具有后台线程的话，初始化完成需要调用.start()去启动这些后台线程。 问题： 32和Runtime.getRuntime().availableProcessors()中的最小值，也就是不能超过32，为什么会有这个限制呢？是因为在elasticsearch的github上有人提了个issues/3478，当使用core很多的机器的时候（比如48core），会创建太多的内存从而导致OOM，所以设置了32的上限来避免太多线程给系统产生压力。 初始化modules实例，通过Guice的Injector进行注入各个Module实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748ModulesBuilder modules = new ModulesBuilder();***modules.add(b -&gt; &#123; b.bind(NodeService.class).toInstance(nodeService); b.bind(NamedXContentRegistry.class).toInstance(xContentRegistry); b.bind(PluginsService.class).toInstance(pluginsService); b.bind(Client.class).toInstance(client); b.bind(NodeClient.class).toInstance(client); b.bind(Environment.class).toInstance(this.environment); b.bind(ThreadPool.class).toInstance(threadPool); b.bind(NodeEnvironment.class).toInstance(nodeEnvironment); b.bind(TribeService.class).toInstance(tribeService); b.bind(ResourceWatcherService.class).toInstance(resourceWatcherService); b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService); b.bind(BigArrays.class).toInstance(bigArrays); b.bind(ScriptService.class).toInstance(scriptModule.getScriptService()); b.bind(AnalysisRegistry.class).toInstance(analysisModule.getAnalysisRegistry()); b.bind(IngestService.class).toInstance(ingestService); b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry); b.bind(MetaDataUpgrader.class).toInstance(metaDataUpgrader); b.bind(MetaStateService.class).toInstance(metaStateService); b.bind(IndicesService.class).toInstance(indicesService); b.bind(SearchService.class).toInstance(newSearchService(clusterService, indicesService, threadPool, scriptModule.getScriptService(), bigArrays, searchModule.getFetchPhase())); b.bind(SearchTransportService.class).toInstance(searchTransportService); b.bind(SearchPhaseController.class).toInstance(new SearchPhaseController(settings, bigArrays, scriptModule.getScriptService())); b.bind(Transport.class).toInstance(transport); b.bind(TransportService.class).toInstance(transportService); b.bind(NetworkService.class).toInstance(networkService); b.bind(UpdateHelper.class).toInstance(new UpdateHelper(settings, scriptModule.getScriptService())); b.bind(MetaDataIndexUpgradeService.class).toInstance(new MetaDataIndexUpgradeService(settings, xContentRegistry, indicesModule.getMapperRegistry(), settingsModule.getIndexScopedSettings(), indexMetaDataUpgraders)); b.bind(ClusterInfoService.class).toInstance(clusterInfoService); b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery()); &#123; RecoverySettings recoverySettings = new RecoverySettings(settings, settingsModule.getClusterSettings()); processRecoverySettings(settingsModule.getClusterSettings(), recoverySettings); b.bind(PeerRecoverySourceService.class).toInstance(new PeerRecoverySourceService(settings, transportService, indicesService, recoverySettings, clusterService)); b.bind(PeerRecoveryTargetService.class).toInstance(new PeerRecoveryTargetService(settings, threadPool, transportService, recoverySettings, clusterService)); &#125; httpBind.accept(b); pluginComponents.stream().forEach(p -&gt; b.bind((Class) p.getClass()).toInstance(p)); &#125; ); injector = modules.createInjector(); 这里面会注入Discovery，ClusterService，Transport Service，还创建了NodeClient用来接收全部其他节点请求。这些都会在往后重点剖析。 3.4.2 启动Node通过在Bootstrap#start()方法中调用Node.start()来启动节点 我们知道，在Node的初始化方法中，Model组件会被添加到绑定的线程当中，那么启动这些只需要调用相应组件的.start()方法即可完成组件的加载 1234567891011121314151617181920public Node start() throws NodeValidationException &#123; if (!lifecycle.moveToStarted()) &#123; return this; &#125; Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings)); logger.info(\"starting ...\"); // hack around dependency injection problem (for now...) injector.getInstance(Discovery.class).setAllocationService(injector.getInstance(AllocationService.class)); pluginLifecycleComponents.forEach(LifecycleComponent::start); injector.getInstance(MappingUpdatedAction.class).setClient(client); injector.getInstance(IndicesService.class).start(); injector.getInstance(IndicesClusterStateService.class).start(); injector.getInstance(IndicesTTLService.class).start(); injector.getInstance(SnapshotsService.class).start(); injector.getInstance(SnapshotShardsService.class).start(); injector.getInstance(RoutingService.class).start(); injector.getInstance(SearchService.class).start(); injector.getInstance(MonitorService.class).start(); 3.4.3 Node节点停止 该方法跟node启动差不多，也是调用相关组件的stop方法即可，这里就不再分析了 3.4.4 加载配置文件信息 入口 通过Node的构造方法 123public Node(Settings preparedSettings) &#123; this(InternalSettingsPreparer.prepareEnvironment(preparedSettings, null));&#125; 这就是加载配置文件的入口，它有三个方法 12345678910public static Settings prepareSettings(Settings input) &#123; Settings.Builder output = Settings.builder(); initializeSettings(output, input, Collections.emptyMap()); finalizeSettings(output, null); return output.build(); &#125; public static Environment prepareEnvironment(Settings input, Terminal terminal) &#123; return prepareEnvironment(input, terminal, Collections.emptyMap()); &#125;public static Environment prepareEnvironment(Settings input, Terminal terminal, Map&lt;String, String&gt; properties) &#123;&#125; 在InternalSettingsPreparer类的prepareEnvironment(org.elasticsearch.common.settings.Settings, org.elasticsearch.cli.Terminal, java.util.Map&lt;java.lang.String,java.lang.String&gt;, java.nio.file.Path)方法中进行了配置文件的加载。 加载配置文件的方法 1234567891011121314151617181920212223242526272829303132333435363738394041public static Environment prepareEnvironment(Settings input, Terminal terminal, Map&lt;String, String&gt; properties) &#123; // just create enough settings to build the environment, to get the config dir Settings.Builder output = Settings.builder(); // 初始化输入输出流信息 initializeSettings(output, input, properties); // 构造Environment实例 Environment environment = new Environment(output.build()); // 这个很关键，保证elasticsearch.yml文件中配置的日志路径path.logs生效 output = Settings.builder(); // start with a fresh output boolean settingsFileFound = false; Set&lt;String&gt; foundSuffixes = new HashSet&lt;&gt;(); for (String allowedSuffix : ALLOWED_SUFFIXES) &#123; Path path = environment.configFile().resolve(\"elasticsearch\" + allowedSuffix); if (Files.exists(path)) &#123; if (!settingsFileFound) &#123; try &#123; output.loadFromPath(path); &#125; catch (IOException e) &#123; throw new SettingsException(\"Failed to load settings from \" + path.toString(), e); &#125; &#125; settingsFileFound = true; foundSuffixes.add(allowedSuffix); &#125; &#125; if (foundSuffixes.size() &gt; 1) &#123; throw new SettingsException(\"multiple settings files found with suffixes: \" + Strings.collectionToDelimitedString(foundSuffixes, \",\")); &#125; // re-initialize settings now that the config file has been loaded initializeSettings(output, input, properties); finalizeSettings(output, terminal); // 再次获取Environment实例 environment = new Environment(output.build()); // we put back the path.logs so we can use it in the logging configuration file output.put(Environment.PATH_LOGS_SETTING.getKey(), cleanPath(environment.logsFile().toAbsolutePath().toString())); String configExtension = foundSuffixes.isEmpty() ? null : foundSuffixes.iterator().next(); // 返回Environment实例 return new Environment(output.build(), configExtension); 构建一个默认的Settings的实例 然后用构造出来的新的Settings来加载给定或默认路径下的elasticsearch.yml 然后将方法接受的参数Settings实例也加载到这个新的Settings中。 最后才将日志文件的路径加载进Settings中，这样就保证了elasticsearch.yml文件中配置的日志路径path.logs生效（覆盖该方法参数中的配置）。 最后返回一个Environment的实例，使得Node开始构建 四、总结：通过上述的源码分析，我们知道Elasticsearch节点启动的入口是Elasticsearch#main()方法，在该方法中会进行一些安全管理的设置，去调用Command的main()方法，整个方法执行没有任何异常，则返回ok状态。 Command#main()：会去添加一些钩子、配置日志输出、调用mainWithoutErrorHandling()去执行EnvironmentAwareCommand#execute(terminal, options)方法。 EnvironmentAwareCommand#execute(terminal, options)方法：只是将配置信息设置到HashMap中，检查了elasticsearch的参数path.conf、path.data、path.home、path.logs，最后调用Elasticsearch#execute()方法，execute(terminal, options, createEnv(terminal, settings))会先调用EnvironmentAwareCommand# createEnv(terminal, settings) Elasticsearch#execute()方法：主要是处理参数，调用init(daemonize, pidFile, quiet, env)，真正执行启动的是Bootstrap.init(!daemonize, pidFile, quiet, initialEnv)方法。 Bootstrap.init(!daemonize, pidFile, quiet, initialEnv)：主要是调用setup()方法和start()方法，在setup()方法中主要通过environment生成本地插件控制器spawner、添加钩子、添加安全管理器、检查jar包、创建Node节点。而start()通过启动初始化好的Node和keepAliveThread线程，这个keepAliveThread使用了CountdownLatch计数器为1来保证钩子一定能够关闭。 Node类的初始化：通过设置好的environment来初始化节点，设置nodEnvironment、Environment、设置Node_name、设置线程池（其实是一个HashMap&lt;String,ExecutorBuilder&gt;） ，根据不同的类型分别分配了不同线程数的一个线程池。将创建好的module绑定到创建的ThreadPool。 大致的时序图如下： 现在还遗留着几个问题： master选举是在什么模块进行的 怎么进行master选举 怎么进行节点监控、维护的 留到下一篇再进行分析。。。","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]},{"title":"elasticsearch源码分析(一)--整体架构","slug":"Elasticsearch/elasticsearch源码分析(一)--整体架构","date":"2018-05-24T02:12:57.000Z","updated":"2018-06-25T00:07:21.049Z","comments":true,"path":"2018/05/24/Elasticsearch/elasticsearch源码分析(一)--整体架构/","link":"","permalink":"http://yoursite.com/2018/05/24/Elasticsearch/elasticsearch源码分析(一)--整体架构/","excerpt":"","text":"一、源码主要模块我下载的Elasticsearch的源码版本为5.6.4 从上图来看：Elasticsearch主要包含以下几个模块 distribution：elasticsearch的打包发行相关，将elasticsearch打成各种发行包（zip，deb，rpm，tar）的模块。具体用法如是，在相应的发行版本模块下执行publishToMavenLocal这个Task，如果执行成功的话就会在路径build/distributions下生成对应的发行包，这种打好的包就能在生产服务器上运行。如果自己修改了源码，打包时就需要用到该模块了。 core：核心包，elasticsearch的源码主要在这个里面，Elasticsearch索引管理、集群管理、服务发现、查询、对Lucene操作的封装等都位于该模块 buildSrc：elasticsearch的构建相关的代码，gradle相关依赖配置都在改模块下 client：作为连接elasticsearch的客户端相关代码，它提供了Rest方式（基于Http）、transport （Java Netty内部的通信方式）等方式。 modules：作为elasticsearch除核心外的必备模块相关代码,比如对Netty的封装、父子类查询、重建索引 plugins：作为elasticsearch必备的插件的相关代码，丰富ES的相关功能，比如IK分词器插件、mapper-attachments/ingest-attachment文件处理插件。 二、Elasticsearch整体架构图 服务发现以及选主 ZenDiscovery 恢复以及容灾 搜索引擎 Search ClusterState 网络层 Rest 和 RPC 线程池","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]},{"title":"Elasticsearch源码阅读环境搭建","slug":"Elasticsearch/elasticsearch源码环境搭建","date":"2018-05-23T04:12:57.000Z","updated":"2018-06-25T00:07:07.275Z","comments":true,"path":"2018/05/23/Elasticsearch/elasticsearch源码环境搭建/","link":"","permalink":"http://yoursite.com/2018/05/23/Elasticsearch/elasticsearch源码环境搭建/","excerpt":"","text":"","categories":[{"name":"Elasticsearch源码分析专题","slug":"Elasticsearch源码分析专题","permalink":"http://yoursite.com/categories/Elasticsearch源码分析专题/"}],"tags":[]}]}